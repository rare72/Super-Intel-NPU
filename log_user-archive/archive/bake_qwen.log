2026-01-15 22:06:24,944 - INFO - >>> [Bake] Starting Qwen3 Process...
2026-01-15 22:06:24,944 - INFO -     > Seq Len: 4096
2026-01-15 22:06:24,944 - INFO -     > Staging: ./model_staging_qwen
2026-01-15 22:06:24,944 - INFO - >>> [Bake] Cleaning up stale NPU cache...
2026-01-15 22:06:24,945 - INFO - >>> [Bake] Stage 1: Exporting + NNCF INT4...
2026-01-15 22:06:40,703 - DEBUG - Inlined graph:
graph(%self.1 : __torch__.transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM,
      %input_ids : Long(2, 16, strides=[16, 1], requires_grad=0, device=cpu),
      %attention_mask.1 : Long(2, 16, strides=[16, 1], requires_grad=0, device=cpu),
      %position_ids : Long(2, 16, strides=[16, 1], requires_grad=0, device=cpu)):
  %lm_head : __torch__.torch.nn.modules.linear.___torch_mangle_463.Linear = prim::GetAttr[name="lm_head"](%self.1)
  %model : __torch__.transformers.models.qwen2.modeling_qwen2.Qwen2Model = prim::GetAttr[name="model"](%self.1)
  %24 : Double(requires_grad=0, device=cpu) = prim::Constant[value={1e-06}](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %25 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %26 : float = prim::Constant[value=0.088388347648318447](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %27 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %28 : Long(requires_grad=0, device=cpu) = prim::Constant[value={4}](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %29 : Long(requires_grad=0, device=cpu) = prim::Constant[value={2}](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %30 : int = prim::Constant[value=128](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %31 : Double(requires_grad=0, device=cpu) = prim::Constant[value={1}](), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:303:0
  %32 : Float(requires_grad=0, device=cpu) = prim::Constant[value={-65504}](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:208:0
  %33 : int = prim::Constant[value=6](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:207:0
  %34 : Float(requires_grad=0, device=cpu) = prim::Constant[value={0}](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:207:0
  %35 : int = prim::Constant[value=4](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:407:0
  %36 : int = prim::Constant[value=3](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:405:0
  %37 : int = prim::Constant[value=9223372036854775807](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:405:0
  %38 : int = prim::Constant[value=2](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:405:0
  %39 : int = prim::Constant[value=11](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:723:0
  %40 : Device = prim::Constant[value="cpu"](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:352:0
  %41 : NoneType = prim::Constant(), scope: __module.model
  %42 : int = prim::Constant[value=0](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:352:0
  %43 : Long(requires_grad=0, device=cpu) = prim::Constant[value={0}](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:353:0
  %44 : int = prim::Constant[value=1](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:353:0
  %45 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.embed_tokens # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2542:0
  %46 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.embed_tokens # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2542:0
  %norm : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_462.Qwen2RMSNorm = prim::GetAttr[name="norm"](%model)
  %layers : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_35 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_461.Qwen2DecoderLayer = prim::GetAttr[name="35"](%layers)
  %layers.69 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_34 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_448.Qwen2DecoderLayer = prim::GetAttr[name="34"](%layers.69)
  %layers.67 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_33 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_435.Qwen2DecoderLayer = prim::GetAttr[name="33"](%layers.67)
  %layers.65 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_32 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_422.Qwen2DecoderLayer = prim::GetAttr[name="32"](%layers.65)
  %layers.63 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_31 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_409.Qwen2DecoderLayer = prim::GetAttr[name="31"](%layers.63)
  %layers.61 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_30 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_396.Qwen2DecoderLayer = prim::GetAttr[name="30"](%layers.61)
  %layers.59 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_29 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_383.Qwen2DecoderLayer = prim::GetAttr[name="29"](%layers.59)
  %layers.57 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_28 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_370.Qwen2DecoderLayer = prim::GetAttr[name="28"](%layers.57)
  %layers.55 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_27 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_357.Qwen2DecoderLayer = prim::GetAttr[name="27"](%layers.55)
  %layers.53 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_26 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_344.Qwen2DecoderLayer = prim::GetAttr[name="26"](%layers.53)
  %layers.51 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_25 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_331.Qwen2DecoderLayer = prim::GetAttr[name="25"](%layers.51)
  %layers.49 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_24 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_318.Qwen2DecoderLayer = prim::GetAttr[name="24"](%layers.49)
  %layers.47 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_23 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_305.Qwen2DecoderLayer = prim::GetAttr[name="23"](%layers.47)
  %layers.45 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_22 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_292.Qwen2DecoderLayer = prim::GetAttr[name="22"](%layers.45)
  %layers.43 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_21 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_279.Qwen2DecoderLayer = prim::GetAttr[name="21"](%layers.43)
  %layers.41 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_20 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_266.Qwen2DecoderLayer = prim::GetAttr[name="20"](%layers.41)
  %layers.39 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_19 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_253.Qwen2DecoderLayer = prim::GetAttr[name="19"](%layers.39)
  %layers.37 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_18 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_240.Qwen2DecoderLayer = prim::GetAttr[name="18"](%layers.37)
  %layers.35 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_17 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_227.Qwen2DecoderLayer = prim::GetAttr[name="17"](%layers.35)
  %layers.33 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_16 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_214.Qwen2DecoderLayer = prim::GetAttr[name="16"](%layers.33)
  %layers.31 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_15 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_201.Qwen2DecoderLayer = prim::GetAttr[name="15"](%layers.31)
  %layers.29 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_14 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_188.Qwen2DecoderLayer = prim::GetAttr[name="14"](%layers.29)
  %layers.27 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_13 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_175.Qwen2DecoderLayer = prim::GetAttr[name="13"](%layers.27)
  %layers.25 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_12 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_162.Qwen2DecoderLayer = prim::GetAttr[name="12"](%layers.25)
  %layers.23 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_11 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_149.Qwen2DecoderLayer = prim::GetAttr[name="11"](%layers.23)
  %layers.21 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_10 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_136.Qwen2DecoderLayer = prim::GetAttr[name="10"](%layers.21)
  %layers.19 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_9 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_123.Qwen2DecoderLayer = prim::GetAttr[name="9"](%layers.19)
  %layers.17 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_8 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_110.Qwen2DecoderLayer = prim::GetAttr[name="8"](%layers.17)
  %layers.15 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_7 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_97.Qwen2DecoderLayer = prim::GetAttr[name="7"](%layers.15)
  %layers.13 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_6 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_84.Qwen2DecoderLayer = prim::GetAttr[name="6"](%layers.13)
  %layers.11 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_5 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_71.Qwen2DecoderLayer = prim::GetAttr[name="5"](%layers.11)
  %layers.9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_4 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_58.Qwen2DecoderLayer = prim::GetAttr[name="4"](%layers.9)
  %layers.7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_3 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_45.Qwen2DecoderLayer = prim::GetAttr[name="3"](%layers.7)
  %layers.5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_2 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_32.Qwen2DecoderLayer = prim::GetAttr[name="2"](%layers.5)
  %layers.3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_1 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_19.Qwen2DecoderLayer = prim::GetAttr[name="1"](%layers.3)
  %layers.1 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_0 : __torch__.transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer = prim::GetAttr[name="0"](%layers.1)
  %rotary_emb : __torch__.transformers.models.qwen2.modeling_qwen2.Qwen2RotaryEmbedding = prim::GetAttr[name="rotary_emb"](%model)
  %embed_tokens : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="embed_tokens"](%model)
  %weight.3 : Tensor = prim::GetAttr[name="weight"](%embed_tokens)
  %inputs_embeds : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::embedding(%weight.3, %input_ids, %45, %46, %46), scope: __module.model/__module.model.embed_tokens # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2542:0
  %124 : int = aten::size(%inputs_embeds, %44), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:353:0
  %125 : Long(device=cpu) = prim::NumToTensor(%124), scope: __module.model
  %126 : Long(requires_grad=0, device=cpu) = aten::add(%125, %43, %44), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:353:0
  %127 : Scalar = aten::ScalarImplicit(%126), scope: __module.model
  %cache_position : Long(16, strides=[1], requires_grad=0, device=cpu) = aten::arange(%42, %127, %41, %41, %40, %46), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:352:0
  %attention_mask.3 : Bool(2, 16, strides=[16, 1], requires_grad=0, device=cpu) = aten::to(%attention_mask.1, %40, %39, %46, %46, %41), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:723:0
  %130 : int = aten::size(%inputs_embeds, %44), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:730:0
  %131 : int = aten::size(%inputs_embeds, %42), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:794:0
  %132 : int = aten::size(%cache_position, %42), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:391:0
  %133 : Long(1, 16, strides=[16, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%cache_position, %42), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:405:0
  %134 : Long(1, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%133, %44), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:405:0
  %135 : Long(1, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::slice(%134, %38, %42, %37, %44), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:405:0
  %q_idx : Long(1, 1, 16, 1, strides=[16, 16, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%135, %36), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:405:0
  %137 : Long(2, strides=[1], requires_grad=0, device=cpu) = aten::arange(%131, %35, %41, %40, %46), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:407:0
  %138 : Long(2, strides=[1], requires_grad=0, device=cpu) = aten::slice(%137, %42, %42, %37, %44), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:407:0
  %139 : Long(2, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%138, %44), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:407:0
  %140 : Long(2, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%139, %38), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:407:0
  %batch_idx : Long(2, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%140, %36), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:407:0
  %142 : Long(16, strides=[1], requires_grad=0, device=cpu) = aten::arange(%130, %35, %41, %40, %46), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:408:0
  %143 : Long(1, 16, strides=[16, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%142, %42), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:408:0
  %144 : Long(1, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%143, %44), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:408:0
  %145 : Long(1, 1, 1, 16, strides=[16, 16, 16, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%144, %38), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:408:0
  %146 : Long(1, 1, 1, 16, strides=[16, 16, 16, 1], requires_grad=0, device=cpu) = aten::slice(%145, %36, %42, %37, %44), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:408:0
  %kv_idx : Long(1, 1, 1, 16, strides=[16, 16, 16, 1], requires_grad=0, device=cpu) = aten::add(%146, %43, %44), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:408:0
  %148 : int[] = prim::ListConstruct(), scope: __module.model
  %result.1 : Bool(requires_grad=0, device=cpu) = aten::new_ones(%q_idx, %148, %39, %41, %41, %46), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:52:0
  %150 : Bool(1, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::le(%kv_idx, %q_idx), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:78:0
  %151 : Bool(1, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::to(%150, %39, %42, %40, %41, %46, %46, %41), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:54:0
  %result : Bool(1, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::__and__(%result.1, %151), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:54:0
  %153 : Tensor?[] = prim::ListConstruct(%batch_idx, %kv_idx), scope: __module.model
  %154 : Bool(2, 1, 1, 16, strides=[16, 16, 16, 1], requires_grad=0, device=cpu) = aten::index(%attention_mask.3, %153), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:142:0
  %155 : Bool(2, 1, 1, 16, strides=[16, 16, 16, 1], requires_grad=0, device=cpu) = aten::to(%154, %39, %42, %40, %41, %46, %46, %41), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:54:0
  %causal_mask : Bool(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::__and__(%result, %155), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:54:0
  %157 : int[] = prim::ListConstruct(%131, %45, %132, %130), scope: __module.model
  %mask : Bool(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::expand(%causal_mask, %157, %46), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:412:0
  %159 : Float(requires_grad=0, device=cpu) = aten::to(%34, %40, %33, %46, %46, %41), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:207:0
  %160 : Float(requires_grad=0, device=cpu) = aten::detach(%159), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:207:0
  %161 : Float(requires_grad=0, device=cpu) = aten::to(%32, %40, %33, %46, %46, %41), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:208:0
  %162 : Float(requires_grad=0, device=cpu) = aten::detach(%161), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:208:0
  %attention_mask.5 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::where(%mask, %160, %162), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:205:0
  %inv_freq : Tensor = prim::GetAttr[name="inv_freq"](%rotary_emb)
  %165 : Float(1, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%inv_freq, %42), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:296:0
  %166 : Float(1, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%165, %44, %42, %37, %44), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:296:0
  %167 : Float(1, 64, 1, strides=[64, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%166, %38), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:296:0
  %168 : Float(1, 64, 1, strides=[64, 1, 1], requires_grad=0, device=cpu) = aten::to(%167, %33, %46, %46, %41), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:296:0
  %169 : int = aten::size(%position_ids, %42), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:296:0
  %170 : int[] = prim::ListConstruct(%169, %45, %44), scope: __module.model/__module.model.rotary_emb
  %171 : Float(2, 64, 1, strides=[0, 1, 1], requires_grad=0, device=cpu) = aten::expand(%168, %170, %46), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:296:0
  %inv_freq_expanded.1 : Float(2, 64, 1, strides=[0, 1, 1], requires_grad=0, device=cpu) = aten::to(%171, %33, %42, %40, %41, %46, %46, %41), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:296:0
  %173 : Long(2, 16, strides=[16, 1], requires_grad=0, device=cpu) = aten::slice(%position_ids, %42, %42, %37, %44), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:297:0
  %174 : Long(2, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%173, %44), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:297:0
  %175 : Long(2, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::slice(%174, %38, %42, %37, %44), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:297:0
  %position_ids_expanded.1 : Float(2, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::to(%175, %33, %46, %46, %41), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:297:0
  %inv_freq_expanded : Float(2, 64, 1, strides=[0, 1, 1], requires_grad=0, device=cpu) = aten::to(%inv_freq_expanded.1, %33, %46, %46, %41), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:301:0
  %position_ids_expanded : Float(2, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::to(%position_ids_expanded.1, %33, %46, %46, %41), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:301:0
  %179 : Float(2, 64, 16, strides=[1024, 16, 1], requires_grad=0, device=cpu) = aten::matmul(%inv_freq_expanded, %position_ids_expanded), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:301:0
  %freqs : Float(2, 16, 64, strides=[1024, 1, 16], requires_grad=0, device=cpu) = aten::transpose(%179, %44, %38), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:301:0
  %181 : Tensor[] = prim::ListConstruct(%freqs, %freqs), scope: __module.model/__module.model.rotary_emb
  %emb : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%181, %45), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:302:0
  %183 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::cos(%emb), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:303:0
  %cos.1 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%183, %31), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:303:0
  %185 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::sin(%emb), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:304:0
  %sin.1 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%185, %31), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:304:0
  %cos.3 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::to(%cos.1, %33, %46, %46, %41), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:306:0
  %sin.3 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::to(%sin.1, %33, %46, %46, %41), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:306:0
  %189 : (Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu), Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%cos.3, %sin.3)
  %190 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu), %191 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%189)
  %mlp.1 : __torch__.transformers.models.qwen2.modeling_qwen2.Qwen2MLP = prim::GetAttr[name="mlp"](%_0)
  %post_attention_layernorm.1 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_6.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_0)
  %self_attn.1 : __torch__.transformers.models.qwen2.modeling_qwen2.Qwen2Attention = prim::GetAttr[name="self_attn"](%_0)
  %input_layernorm.1 : __torch__.transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_0)
  %weight.5 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.1)
  %hidden_states.1 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%inputs_embeds, %33, %46, %46, %41), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %198 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.1, %38), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %199 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm
  %variance.1 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%198, %199, %25, %41), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %201 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.1, %24, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %202 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%201), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.3 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.1, %202), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.5 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.3, %33, %46, %46, %41), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.7 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.5, %hidden_states.5), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %206 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.7, %hidden_states.1)
  %207 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %208 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%206)
  %o_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_2.Linear = prim::GetAttr[name="o_proj"](%self_attn.1)
  %v_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_1.Linear = prim::GetAttr[name="v_proj"](%self_attn.1)
  %k_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_0.Linear = prim::GetAttr[name="k_proj"](%self_attn.1)
  %q_proj.1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self_attn.1)
  %213 : int = aten::size(%207, %42), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %214 : int = aten::size(%207, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.1 : Tensor = prim::GetAttr[name="bias"](%q_proj.1)
  %weight.7 : Tensor = prim::GetAttr[name="weight"](%q_proj.1)
  %217 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%207, %weight.7, %bias.1), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn/__module.model.layers.0.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %218 : int[] = prim::ListConstruct(%213, %214, %45, %30), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %219 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%217, %218), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.1 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%219, %44, %38), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.3 : Tensor = prim::GetAttr[name="bias"](%k_proj.1)
  %weight.9 : Tensor = prim::GetAttr[name="weight"](%k_proj.1)
  %223 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%207, %weight.9, %bias.3), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn/__module.model.layers.0.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %224 : int[] = prim::ListConstruct(%213, %214, %45, %30), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %225 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%223, %224), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.1 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%225, %44, %38), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.5 : Tensor = prim::GetAttr[name="bias"](%v_proj.1)
  %weight.11 : Tensor = prim::GetAttr[name="weight"](%v_proj.1)
  %229 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%207, %weight.11, %bias.5), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn/__module.model.layers.0.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %230 : int[] = prim::ListConstruct(%213, %214, %45, %30), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %231 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%229, %230), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.13 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%231, %44, %38), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.5 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.5 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %235 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.1, %cos.5), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %236 : int = aten::size(%q.1, %36), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %237 : Long(device=cpu) = prim::NumToTensor(%236), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %238 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%237, %29), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %239 : int = aten::Int(%238), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %x1.1 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.1, %36, %42, %239, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %241 : int = aten::size(%q.1, %36), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %242 : Long(device=cpu) = prim::NumToTensor(%241), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %243 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%242, %29), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %244 : int = aten::Int(%243), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %x2.1 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.1, %36, %244, %37, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %246 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.1), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %247 : Tensor[] = prim::ListConstruct(%246, %x1.1), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %248 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%247, %45), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %249 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%248, %sin.5), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.1 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%235, %249, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %251 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.1, %cos.5), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %252 : int = aten::size(%k.1, %36), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %253 : Long(device=cpu) = prim::NumToTensor(%252), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %254 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%253, %29), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %255 : int = aten::Int(%254), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %x1.3 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.1, %36, %42, %255, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %257 : int = aten::size(%k.1, %36), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %258 : Long(device=cpu) = prim::NumToTensor(%257), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %259 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%258, %29), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %260 : int = aten::Int(%259), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %x2.3 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.1, %36, %260, %37, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %262 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.3), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %263 : Tensor[] = prim::ListConstruct(%262, %x1.3), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %264 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%263, %45), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %265 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%264, %sin.5), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.9 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%251, %265, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %267 : int = aten::size(%hidden_states.9, %42), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %268 : int = aten::size(%hidden_states.9, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.1 : Long(device=cpu) = prim::NumToTensor(%268), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %270 : int = aten::size(%hidden_states.9, %38), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %271 : int = aten::size(%hidden_states.9, %36), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %272 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.9, %42, %42, %37, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %273 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%272, %44, %42, %37, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %274 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%273, %38), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %275 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%274, %36, %42, %37, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %276 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%275, %35, %42, %37, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %277 : int[] = prim::ListConstruct(%267, %268, %35, %270, %271), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %hidden_states.11 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%276, %277, %46), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %279 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.1, %28), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %280 : int = aten::Int(%279), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %281 : int[] = prim::ListConstruct(%267, %280, %270, %271), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %key.1 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.11, %281), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %283 : int = aten::size(%hidden_states.13, %42), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %284 : int = aten::size(%hidden_states.13, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.3 : Long(device=cpu) = prim::NumToTensor(%284), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %286 : int = aten::size(%hidden_states.13, %38), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %287 : int = aten::size(%hidden_states.13, %36), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %288 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.13, %42, %42, %37, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %289 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%288, %44, %42, %37, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %290 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%289, %38), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %291 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%290, %36, %42, %37, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %292 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%291, %35, %42, %37, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %293 : int[] = prim::ListConstruct(%283, %284, %35, %286, %287), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %hidden_states.15 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%292, %293, %46), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %295 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.3, %28), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %296 : int = aten::Int(%295), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %297 : int[] = prim::ListConstruct(%283, %296, %286, %287), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %value.1 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.15, %297), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %299 : int = aten::size(%key.1, %38), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %300 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %301 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%300, %44, %42, %37, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %302 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%301, %38, %42, %37, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.7 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%302, %36, %42, %299, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.1 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.1, %key.1, %value.1, %attention_mask.7, %27, %46, %26, %46), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %305 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.1, %44, %38), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.3 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%305, %42), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %307 : int[] = prim::ListConstruct(%213, %214, %45), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %308 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.3, %307), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.1 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%308, %42), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.13 : Tensor = prim::GetAttr[name="weight"](%o_proj.1)
  %hidden_states.17 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.1, %weight.13, %41), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn/__module.model.layers.0.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.19 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%208, %hidden_states.17, %44), scope: __module.model/__module.model.layers.0 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.15 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.1)
  %hidden_states.21 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.19, %33, %46, %46, %41), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %315 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.21, %38), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %316 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm
  %variance.3 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%315, %316, %25, %41), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %318 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.3, %24, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %319 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%318), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.23 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.21, %319), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.25 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.23, %33, %46, %46, %41), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.3 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.15, %hidden_states.25), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %323 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.3, %hidden_states.21)
  %324 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %325 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%323)
  %down_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name="down_proj"](%mlp.1)
  %up_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name="up_proj"](%mlp.1)
  %gate_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name="gate_proj"](%mlp.1)
  %weight.17 : Tensor = prim::GetAttr[name="weight"](%gate_proj.1)
  %input.5 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%324, %weight.17, %41), scope: __module.model/__module.model.layers.0/__module.model.layers.0.mlp/__module.model.layers.0.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %331 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.5), scope: __module.model/__module.model.layers.0/__module.model.layers.0.mlp/__module.model.layers.0.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.19 : Tensor = prim::GetAttr[name="weight"](%up_proj.1)
  %333 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%324, %weight.19, %41), scope: __module.model/__module.model.layers.0/__module.model.layers.0.mlp/__module.model.layers.0.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.7 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%331, %333), scope: __module.model/__module.model.layers.0/__module.model.layers.0.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.21 : Tensor = prim::GetAttr[name="weight"](%down_proj.1)
  %hidden_states.27 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.7, %weight.21, %41), scope: __module.model/__module.model.layers.0/__module.model.layers.0.mlp/__module.model.layers.0.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.29 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%325, %hidden_states.27, %44), scope: __module.model/__module.model.layers.0 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.3 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_16.Qwen2MLP = prim::GetAttr[name="mlp"](%_1)
  %post_attention_layernorm.3 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_18.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_1)
  %self_attn.3 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_11.Qwen2Attention = prim::GetAttr[name="self_attn"](%_1)
  %input_layernorm.3 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_17.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_1)
  %weight.23 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.3)
  %hidden_states.31 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.29, %33, %46, %46, %41), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %344 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.31, %38), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %345 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm
  %variance.5 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%344, %345, %25, %41), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %347 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.5, %24, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %348 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%347), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.33 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.31, %348), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.35 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.33, %33, %46, %46, %41), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.37 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.23, %hidden_states.35), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %352 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.37, %hidden_states.31)
  %353 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %354 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%352)
  %o_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name="o_proj"](%self_attn.3)
  %v_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_9.Linear = prim::GetAttr[name="v_proj"](%self_attn.3)
  %k_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name="k_proj"](%self_attn.3)
  %q_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_7.Linear = prim::GetAttr[name="q_proj"](%self_attn.3)
  %359 : int = aten::size(%353, %42), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %360 : int = aten::size(%353, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.7 : Tensor = prim::GetAttr[name="bias"](%q_proj.3)
  %weight.25 : Tensor = prim::GetAttr[name="weight"](%q_proj.3)
  %363 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%353, %weight.25, %bias.7), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn/__module.model.layers.1.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %364 : int[] = prim::ListConstruct(%359, %360, %45, %30), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %365 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%363, %364), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.3 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%365, %44, %38), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.9 : Tensor = prim::GetAttr[name="bias"](%k_proj.3)
  %weight.27 : Tensor = prim::GetAttr[name="weight"](%k_proj.3)
  %369 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%353, %weight.27, %bias.9), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn/__module.model.layers.1.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %370 : int[] = prim::ListConstruct(%359, %360, %45, %30), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %371 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%369, %370), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.3 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%371, %44, %38), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.11 : Tensor = prim::GetAttr[name="bias"](%v_proj.3)
  %weight.29 : Tensor = prim::GetAttr[name="weight"](%v_proj.3)
  %375 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%353, %weight.29, %bias.11), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn/__module.model.layers.1.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %376 : int[] = prim::ListConstruct(%359, %360, %45, %30), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %377 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%375, %376), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.43 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%377, %44, %38), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.7 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.7 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %381 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.3, %cos.7), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %382 : int = aten::size(%q.3, %36), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %383 : Long(device=cpu) = prim::NumToTensor(%382), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %384 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%383, %29), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %385 : int = aten::Int(%384), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %x1.5 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.3, %36, %42, %385, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %387 : int = aten::size(%q.3, %36), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %388 : Long(device=cpu) = prim::NumToTensor(%387), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %389 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%388, %29), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %390 : int = aten::Int(%389), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %x2.5 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.3, %36, %390, %37, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %392 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.5), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %393 : Tensor[] = prim::ListConstruct(%392, %x1.5), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %394 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%393, %45), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %395 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%394, %sin.7), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.3 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%381, %395, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %397 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.3, %cos.7), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %398 : int = aten::size(%k.3, %36), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %399 : Long(device=cpu) = prim::NumToTensor(%398), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %400 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%399, %29), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %401 : int = aten::Int(%400), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %x1.7 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.3, %36, %42, %401, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %403 : int = aten::size(%k.3, %36), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %404 : Long(device=cpu) = prim::NumToTensor(%403), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %405 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%404, %29), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %406 : int = aten::Int(%405), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %x2.7 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.3, %36, %406, %37, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %408 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.7), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %409 : Tensor[] = prim::ListConstruct(%408, %x1.7), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %410 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%409, %45), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %411 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%410, %sin.7), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.39 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%397, %411, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %413 : int = aten::size(%hidden_states.39, %42), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %414 : int = aten::size(%hidden_states.39, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.5 : Long(device=cpu) = prim::NumToTensor(%414), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %416 : int = aten::size(%hidden_states.39, %38), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %417 : int = aten::size(%hidden_states.39, %36), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %418 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.39, %42, %42, %37, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %419 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%418, %44, %42, %37, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %420 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%419, %38), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %421 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%420, %36, %42, %37, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %422 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%421, %35, %42, %37, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %423 : int[] = prim::ListConstruct(%413, %414, %35, %416, %417), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %hidden_states.41 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%422, %423, %46), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %425 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.5, %28), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %426 : int = aten::Int(%425), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %427 : int[] = prim::ListConstruct(%413, %426, %416, %417), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %key.3 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.41, %427), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %429 : int = aten::size(%hidden_states.43, %42), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %430 : int = aten::size(%hidden_states.43, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.7 : Long(device=cpu) = prim::NumToTensor(%430), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %432 : int = aten::size(%hidden_states.43, %38), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %433 : int = aten::size(%hidden_states.43, %36), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %434 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.43, %42, %42, %37, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %435 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%434, %44, %42, %37, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %436 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%435, %38), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %437 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%436, %36, %42, %37, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %438 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%437, %35, %42, %37, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %439 : int[] = prim::ListConstruct(%429, %430, %35, %432, %433), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %hidden_states.45 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%438, %439, %46), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %441 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.7, %28), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %442 : int = aten::Int(%441), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %443 : int[] = prim::ListConstruct(%429, %442, %432, %433), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %value.3 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.45, %443), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %445 : int = aten::size(%key.3, %38), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %446 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %447 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%446, %44, %42, %37, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %448 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%447, %38, %42, %37, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.9 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%448, %36, %42, %445, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.5 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.3, %key.3, %value.3, %attention_mask.9, %27, %46, %26, %46), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %451 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.5, %44, %38), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.7 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%451, %42), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %453 : int[] = prim::ListConstruct(%359, %360, %45), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %454 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.7, %453), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.9 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%454, %42), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.31 : Tensor = prim::GetAttr[name="weight"](%o_proj.3)
  %hidden_states.47 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.9, %weight.31, %41), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn/__module.model.layers.1.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.49 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%354, %hidden_states.47, %44), scope: __module.model/__module.model.layers.1 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.33 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.3)
  %hidden_states.51 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.49, %33, %46, %46, %41), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %461 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.51, %38), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %462 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm
  %variance.7 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%461, %462, %25, %41), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %464 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.7, %24, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %465 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%464), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.53 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.51, %465), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.55 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.53, %33, %46, %46, %41), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.11 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.33, %hidden_states.55), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %469 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.11, %hidden_states.51)
  %470 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %471 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%469)
  %down_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_14.Linear = prim::GetAttr[name="down_proj"](%mlp.3)
  %up_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_13.Linear = prim::GetAttr[name="up_proj"](%mlp.3)
  %gate_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_12.Linear = prim::GetAttr[name="gate_proj"](%mlp.3)
  %weight.35 : Tensor = prim::GetAttr[name="weight"](%gate_proj.3)
  %input.13 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%470, %weight.35, %41), scope: __module.model/__module.model.layers.1/__module.model.layers.1.mlp/__module.model.layers.1.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %477 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.13), scope: __module.model/__module.model.layers.1/__module.model.layers.1.mlp/__module.model.layers.1.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.37 : Tensor = prim::GetAttr[name="weight"](%up_proj.3)
  %479 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%470, %weight.37, %41), scope: __module.model/__module.model.layers.1/__module.model.layers.1.mlp/__module.model.layers.1.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.15 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%477, %479), scope: __module.model/__module.model.layers.1/__module.model.layers.1.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.39 : Tensor = prim::GetAttr[name="weight"](%down_proj.3)
  %hidden_states.57 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.15, %weight.39, %41), scope: __module.model/__module.model.layers.1/__module.model.layers.1.mlp/__module.model.layers.1.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.59 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%471, %hidden_states.57, %44), scope: __module.model/__module.model.layers.1 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.5 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_29.Qwen2MLP = prim::GetAttr[name="mlp"](%_2)
  %post_attention_layernorm.5 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_31.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_2)
  %self_attn.5 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_24.Qwen2Attention = prim::GetAttr[name="self_attn"](%_2)
  %input_layernorm.5 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_30.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_2)
  %weight.41 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.5)
  %hidden_states.61 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.59, %33, %46, %46, %41), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %490 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.61, %38), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %491 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm
  %variance.9 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%490, %491, %25, %41), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %493 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.9, %24, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %494 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%493), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.63 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.61, %494), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.65 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.63, %33, %46, %46, %41), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.67 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.41, %hidden_states.65), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %498 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.67, %hidden_states.61)
  %499 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %500 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%498)
  %o_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_23.Linear = prim::GetAttr[name="o_proj"](%self_attn.5)
  %v_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_22.Linear = prim::GetAttr[name="v_proj"](%self_attn.5)
  %k_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_21.Linear = prim::GetAttr[name="k_proj"](%self_attn.5)
  %q_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_20.Linear = prim::GetAttr[name="q_proj"](%self_attn.5)
  %505 : int = aten::size(%499, %42), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %506 : int = aten::size(%499, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.13 : Tensor = prim::GetAttr[name="bias"](%q_proj.5)
  %weight.43 : Tensor = prim::GetAttr[name="weight"](%q_proj.5)
  %509 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%499, %weight.43, %bias.13), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn/__module.model.layers.2.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %510 : int[] = prim::ListConstruct(%505, %506, %45, %30), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %511 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%509, %510), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.5 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%511, %44, %38), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.15 : Tensor = prim::GetAttr[name="bias"](%k_proj.5)
  %weight.45 : Tensor = prim::GetAttr[name="weight"](%k_proj.5)
  %515 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%499, %weight.45, %bias.15), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn/__module.model.layers.2.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %516 : int[] = prim::ListConstruct(%505, %506, %45, %30), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %517 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%515, %516), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.5 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%517, %44, %38), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.17 : Tensor = prim::GetAttr[name="bias"](%v_proj.5)
  %weight.47 : Tensor = prim::GetAttr[name="weight"](%v_proj.5)
  %521 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%499, %weight.47, %bias.17), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn/__module.model.layers.2.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %522 : int[] = prim::ListConstruct(%505, %506, %45, %30), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %523 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%521, %522), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.73 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%523, %44, %38), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.9 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.9 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %527 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.5, %cos.9), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %528 : int = aten::size(%q.5, %36), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %529 : Long(device=cpu) = prim::NumToTensor(%528), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %530 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%529, %29), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %531 : int = aten::Int(%530), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %x1.9 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.5, %36, %42, %531, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %533 : int = aten::size(%q.5, %36), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %534 : Long(device=cpu) = prim::NumToTensor(%533), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %535 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%534, %29), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %536 : int = aten::Int(%535), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %x2.9 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.5, %36, %536, %37, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %538 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.9), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %539 : Tensor[] = prim::ListConstruct(%538, %x1.9), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %540 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%539, %45), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %541 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%540, %sin.9), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.5 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%527, %541, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %543 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.5, %cos.9), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %544 : int = aten::size(%k.5, %36), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %545 : Long(device=cpu) = prim::NumToTensor(%544), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %546 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%545, %29), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %547 : int = aten::Int(%546), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %x1.11 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.5, %36, %42, %547, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %549 : int = aten::size(%k.5, %36), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %550 : Long(device=cpu) = prim::NumToTensor(%549), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %551 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%550, %29), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %552 : int = aten::Int(%551), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %x2.11 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.5, %36, %552, %37, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %554 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.11), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %555 : Tensor[] = prim::ListConstruct(%554, %x1.11), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %556 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%555, %45), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %557 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%556, %sin.9), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.69 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%543, %557, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %559 : int = aten::size(%hidden_states.69, %42), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %560 : int = aten::size(%hidden_states.69, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.9 : Long(device=cpu) = prim::NumToTensor(%560), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %562 : int = aten::size(%hidden_states.69, %38), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %563 : int = aten::size(%hidden_states.69, %36), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %564 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.69, %42, %42, %37, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %565 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%564, %44, %42, %37, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %566 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%565, %38), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %567 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%566, %36, %42, %37, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %568 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%567, %35, %42, %37, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %569 : int[] = prim::ListConstruct(%559, %560, %35, %562, %563), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %hidden_states.71 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%568, %569, %46), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %571 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.9, %28), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %572 : int = aten::Int(%571), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %573 : int[] = prim::ListConstruct(%559, %572, %562, %563), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %key.5 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.71, %573), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %575 : int = aten::size(%hidden_states.73, %42), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %576 : int = aten::size(%hidden_states.73, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.11 : Long(device=cpu) = prim::NumToTensor(%576), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %578 : int = aten::size(%hidden_states.73, %38), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %579 : int = aten::size(%hidden_states.73, %36), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %580 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.73, %42, %42, %37, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %581 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%580, %44, %42, %37, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %582 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%581, %38), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %583 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%582, %36, %42, %37, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %584 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%583, %35, %42, %37, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %585 : int[] = prim::ListConstruct(%575, %576, %35, %578, %579), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %hidden_states.75 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%584, %585, %46), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %587 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.11, %28), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %588 : int = aten::Int(%587), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %589 : int[] = prim::ListConstruct(%575, %588, %578, %579), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %value.5 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.75, %589), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %591 : int = aten::size(%key.5, %38), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %592 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %593 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%592, %44, %42, %37, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %594 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%593, %38, %42, %37, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.11 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%594, %36, %42, %591, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.9 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.5, %key.5, %value.5, %attention_mask.11, %27, %46, %26, %46), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %597 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.9, %44, %38), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.11 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%597, %42), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %599 : int[] = prim::ListConstruct(%505, %506, %45), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %600 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.11, %599), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.17 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%600, %42), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.49 : Tensor = prim::GetAttr[name="weight"](%o_proj.5)
  %hidden_states.77 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.17, %weight.49, %41), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn/__module.model.layers.2.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.79 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%500, %hidden_states.77, %44), scope: __module.model/__module.model.layers.2 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.51 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.5)
  %hidden_states.81 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.79, %33, %46, %46, %41), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %607 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.81, %38), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %608 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm
  %variance.11 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%607, %608, %25, %41), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %610 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.11, %24, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %611 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%610), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.83 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.81, %611), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.85 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.83, %33, %46, %46, %41), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.19 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.51, %hidden_states.85), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %615 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.19, %hidden_states.81)
  %616 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %617 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%615)
  %down_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_27.Linear = prim::GetAttr[name="down_proj"](%mlp.5)
  %up_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_26.Linear = prim::GetAttr[name="up_proj"](%mlp.5)
  %gate_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_25.Linear = prim::GetAttr[name="gate_proj"](%mlp.5)
  %weight.53 : Tensor = prim::GetAttr[name="weight"](%gate_proj.5)
  %input.21 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%616, %weight.53, %41), scope: __module.model/__module.model.layers.2/__module.model.layers.2.mlp/__module.model.layers.2.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %623 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.21), scope: __module.model/__module.model.layers.2/__module.model.layers.2.mlp/__module.model.layers.2.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.55 : Tensor = prim::GetAttr[name="weight"](%up_proj.5)
  %625 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%616, %weight.55, %41), scope: __module.model/__module.model.layers.2/__module.model.layers.2.mlp/__module.model.layers.2.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.23 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%623, %625), scope: __module.model/__module.model.layers.2/__module.model.layers.2.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.57 : Tensor = prim::GetAttr[name="weight"](%down_proj.5)
  %hidden_states.87 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.23, %weight.57, %41), scope: __module.model/__module.model.layers.2/__module.model.layers.2.mlp/__module.model.layers.2.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.89 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%617, %hidden_states.87, %44), scope: __module.model/__module.model.layers.2 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.7 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_42.Qwen2MLP = prim::GetAttr[name="mlp"](%_3)
  %post_attention_layernorm.7 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_44.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_3)
  %self_attn.7 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_37.Qwen2Attention = prim::GetAttr[name="self_attn"](%_3)
  %input_layernorm.7 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_43.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_3)
  %weight.59 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.7)
  %hidden_states.91 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.89, %33, %46, %46, %41), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %636 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.91, %38), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %637 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm
  %variance.13 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%636, %637, %25, %41), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %639 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.13, %24, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %640 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%639), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.93 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.91, %640), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.95 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.93, %33, %46, %46, %41), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.97 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.59, %hidden_states.95), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %644 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.97, %hidden_states.91)
  %645 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %646 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%644)
  %o_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_36.Linear = prim::GetAttr[name="o_proj"](%self_attn.7)
  %v_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_35.Linear = prim::GetAttr[name="v_proj"](%self_attn.7)
  %k_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_34.Linear = prim::GetAttr[name="k_proj"](%self_attn.7)
  %q_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_33.Linear = prim::GetAttr[name="q_proj"](%self_attn.7)
  %651 : int = aten::size(%645, %42), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %652 : int = aten::size(%645, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.19 : Tensor = prim::GetAttr[name="bias"](%q_proj.7)
  %weight.61 : Tensor = prim::GetAttr[name="weight"](%q_proj.7)
  %655 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%645, %weight.61, %bias.19), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn/__module.model.layers.3.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %656 : int[] = prim::ListConstruct(%651, %652, %45, %30), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %657 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%655, %656), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.7 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%657, %44, %38), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.21 : Tensor = prim::GetAttr[name="bias"](%k_proj.7)
  %weight.63 : Tensor = prim::GetAttr[name="weight"](%k_proj.7)
  %661 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%645, %weight.63, %bias.21), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn/__module.model.layers.3.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %662 : int[] = prim::ListConstruct(%651, %652, %45, %30), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %663 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%661, %662), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.7 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%663, %44, %38), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.23 : Tensor = prim::GetAttr[name="bias"](%v_proj.7)
  %weight.65 : Tensor = prim::GetAttr[name="weight"](%v_proj.7)
  %667 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%645, %weight.65, %bias.23), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn/__module.model.layers.3.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %668 : int[] = prim::ListConstruct(%651, %652, %45, %30), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %669 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%667, %668), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.103 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%669, %44, %38), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.11 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.11 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %673 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.7, %cos.11), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %674 : int = aten::size(%q.7, %36), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %675 : Long(device=cpu) = prim::NumToTensor(%674), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %676 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%675, %29), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %677 : int = aten::Int(%676), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %x1.13 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.7, %36, %42, %677, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %679 : int = aten::size(%q.7, %36), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %680 : Long(device=cpu) = prim::NumToTensor(%679), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %681 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%680, %29), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %682 : int = aten::Int(%681), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %x2.13 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.7, %36, %682, %37, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %684 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.13), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %685 : Tensor[] = prim::ListConstruct(%684, %x1.13), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %686 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%685, %45), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %687 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%686, %sin.11), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.7 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%673, %687, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %689 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.7, %cos.11), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %690 : int = aten::size(%k.7, %36), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %691 : Long(device=cpu) = prim::NumToTensor(%690), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %692 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%691, %29), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %693 : int = aten::Int(%692), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %x1.15 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.7, %36, %42, %693, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %695 : int = aten::size(%k.7, %36), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %696 : Long(device=cpu) = prim::NumToTensor(%695), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %697 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%696, %29), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %698 : int = aten::Int(%697), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %x2.15 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.7, %36, %698, %37, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %700 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.15), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %701 : Tensor[] = prim::ListConstruct(%700, %x1.15), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %702 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%701, %45), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %703 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%702, %sin.11), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.99 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%689, %703, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %705 : int = aten::size(%hidden_states.99, %42), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %706 : int = aten::size(%hidden_states.99, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.13 : Long(device=cpu) = prim::NumToTensor(%706), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %708 : int = aten::size(%hidden_states.99, %38), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %709 : int = aten::size(%hidden_states.99, %36), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %710 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.99, %42, %42, %37, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %711 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%710, %44, %42, %37, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %712 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%711, %38), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %713 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%712, %36, %42, %37, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %714 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%713, %35, %42, %37, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %715 : int[] = prim::ListConstruct(%705, %706, %35, %708, %709), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %hidden_states.101 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%714, %715, %46), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %717 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.13, %28), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %718 : int = aten::Int(%717), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %719 : int[] = prim::ListConstruct(%705, %718, %708, %709), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %key.7 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.101, %719), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %721 : int = aten::size(%hidden_states.103, %42), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %722 : int = aten::size(%hidden_states.103, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.15 : Long(device=cpu) = prim::NumToTensor(%722), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %724 : int = aten::size(%hidden_states.103, %38), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %725 : int = aten::size(%hidden_states.103, %36), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %726 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.103, %42, %42, %37, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %727 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%726, %44, %42, %37, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %728 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%727, %38), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %729 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%728, %36, %42, %37, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %730 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%729, %35, %42, %37, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %731 : int[] = prim::ListConstruct(%721, %722, %35, %724, %725), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %hidden_states.105 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%730, %731, %46), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %733 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.15, %28), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %734 : int = aten::Int(%733), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %735 : int[] = prim::ListConstruct(%721, %734, %724, %725), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %value.7 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.105, %735), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %737 : int = aten::size(%key.7, %38), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %738 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %739 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%738, %44, %42, %37, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %740 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%739, %38, %42, %37, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.13 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%740, %36, %42, %737, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.13 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.7, %key.7, %value.7, %attention_mask.13, %27, %46, %26, %46), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %743 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.13, %44, %38), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.15 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%743, %42), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %745 : int[] = prim::ListConstruct(%651, %652, %45), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %746 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.15, %745), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.25 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%746, %42), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.67 : Tensor = prim::GetAttr[name="weight"](%o_proj.7)
  %hidden_states.107 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.25, %weight.67, %41), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn/__module.model.layers.3.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.109 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%646, %hidden_states.107, %44), scope: __module.model/__module.model.layers.3 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.69 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.7)
  %hidden_states.111 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.109, %33, %46, %46, %41), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %753 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.111, %38), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %754 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm
  %variance.15 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%753, %754, %25, %41), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %756 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.15, %24, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %757 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%756), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.113 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.111, %757), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.115 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.113, %33, %46, %46, %41), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.27 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.69, %hidden_states.115), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %761 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.27, %hidden_states.111)
  %762 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %763 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%761)
  %down_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_40.Linear = prim::GetAttr[name="down_proj"](%mlp.7)
  %up_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_39.Linear = prim::GetAttr[name="up_proj"](%mlp.7)
  %gate_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_38.Linear = prim::GetAttr[name="gate_proj"](%mlp.7)
  %weight.71 : Tensor = prim::GetAttr[name="weight"](%gate_proj.7)
  %input.29 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%762, %weight.71, %41), scope: __module.model/__module.model.layers.3/__module.model.layers.3.mlp/__module.model.layers.3.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %769 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.29), scope: __module.model/__module.model.layers.3/__module.model.layers.3.mlp/__module.model.layers.3.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.73 : Tensor = prim::GetAttr[name="weight"](%up_proj.7)
  %771 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%762, %weight.73, %41), scope: __module.model/__module.model.layers.3/__module.model.layers.3.mlp/__module.model.layers.3.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.31 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%769, %771), scope: __module.model/__module.model.layers.3/__module.model.layers.3.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.75 : Tensor = prim::GetAttr[name="weight"](%down_proj.7)
  %hidden_states.117 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.31, %weight.75, %41), scope: __module.model/__module.model.layers.3/__module.model.layers.3.mlp/__module.model.layers.3.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.119 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%763, %hidden_states.117, %44), scope: __module.model/__module.model.layers.3 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.9 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_55.Qwen2MLP = prim::GetAttr[name="mlp"](%_4)
  %post_attention_layernorm.9 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_57.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_4)
  %self_attn.9 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_50.Qwen2Attention = prim::GetAttr[name="self_attn"](%_4)
  %input_layernorm.9 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_56.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_4)
  %weight.77 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.9)
  %hidden_states.121 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.119, %33, %46, %46, %41), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %782 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.121, %38), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %783 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm
  %variance.17 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%782, %783, %25, %41), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %785 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.17, %24, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %786 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%785), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.123 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.121, %786), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.125 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.123, %33, %46, %46, %41), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.127 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.77, %hidden_states.125), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %790 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.127, %hidden_states.121)
  %791 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %792 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%790)
  %o_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_49.Linear = prim::GetAttr[name="o_proj"](%self_attn.9)
  %v_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_48.Linear = prim::GetAttr[name="v_proj"](%self_attn.9)
  %k_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_47.Linear = prim::GetAttr[name="k_proj"](%self_attn.9)
  %q_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_46.Linear = prim::GetAttr[name="q_proj"](%self_attn.9)
  %797 : int = aten::size(%791, %42), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %798 : int = aten::size(%791, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.25 : Tensor = prim::GetAttr[name="bias"](%q_proj.9)
  %weight.79 : Tensor = prim::GetAttr[name="weight"](%q_proj.9)
  %801 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%791, %weight.79, %bias.25), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn/__module.model.layers.4.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %802 : int[] = prim::ListConstruct(%797, %798, %45, %30), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %803 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%801, %802), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.9 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%803, %44, %38), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.27 : Tensor = prim::GetAttr[name="bias"](%k_proj.9)
  %weight.81 : Tensor = prim::GetAttr[name="weight"](%k_proj.9)
  %807 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%791, %weight.81, %bias.27), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn/__module.model.layers.4.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %808 : int[] = prim::ListConstruct(%797, %798, %45, %30), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %809 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%807, %808), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.9 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%809, %44, %38), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.29 : Tensor = prim::GetAttr[name="bias"](%v_proj.9)
  %weight.83 : Tensor = prim::GetAttr[name="weight"](%v_proj.9)
  %813 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%791, %weight.83, %bias.29), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn/__module.model.layers.4.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %814 : int[] = prim::ListConstruct(%797, %798, %45, %30), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %815 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%813, %814), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.133 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%815, %44, %38), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.13 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.13 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %819 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.9, %cos.13), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %820 : int = aten::size(%q.9, %36), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %821 : Long(device=cpu) = prim::NumToTensor(%820), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %822 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%821, %29), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %823 : int = aten::Int(%822), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %x1.17 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.9, %36, %42, %823, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %825 : int = aten::size(%q.9, %36), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %826 : Long(device=cpu) = prim::NumToTensor(%825), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %827 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%826, %29), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %828 : int = aten::Int(%827), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %x2.17 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.9, %36, %828, %37, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %830 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.17), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %831 : Tensor[] = prim::ListConstruct(%830, %x1.17), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %832 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%831, %45), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %833 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%832, %sin.13), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.9 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%819, %833, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %835 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.9, %cos.13), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %836 : int = aten::size(%k.9, %36), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %837 : Long(device=cpu) = prim::NumToTensor(%836), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %838 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%837, %29), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %839 : int = aten::Int(%838), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %x1.19 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.9, %36, %42, %839, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %841 : int = aten::size(%k.9, %36), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %842 : Long(device=cpu) = prim::NumToTensor(%841), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %843 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%842, %29), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %844 : int = aten::Int(%843), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %x2.19 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.9, %36, %844, %37, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %846 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.19), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %847 : Tensor[] = prim::ListConstruct(%846, %x1.19), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %848 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%847, %45), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %849 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%848, %sin.13), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.129 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%835, %849, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %851 : int = aten::size(%hidden_states.129, %42), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %852 : int = aten::size(%hidden_states.129, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.17 : Long(device=cpu) = prim::NumToTensor(%852), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %854 : int = aten::size(%hidden_states.129, %38), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %855 : int = aten::size(%hidden_states.129, %36), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %856 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.129, %42, %42, %37, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %857 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%856, %44, %42, %37, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %858 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%857, %38), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %859 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%858, %36, %42, %37, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %860 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%859, %35, %42, %37, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %861 : int[] = prim::ListConstruct(%851, %852, %35, %854, %855), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %hidden_states.131 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%860, %861, %46), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %863 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.17, %28), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %864 : int = aten::Int(%863), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %865 : int[] = prim::ListConstruct(%851, %864, %854, %855), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %key.9 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.131, %865), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %867 : int = aten::size(%hidden_states.133, %42), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %868 : int = aten::size(%hidden_states.133, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.19 : Long(device=cpu) = prim::NumToTensor(%868), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %870 : int = aten::size(%hidden_states.133, %38), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %871 : int = aten::size(%hidden_states.133, %36), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %872 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.133, %42, %42, %37, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %873 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%872, %44, %42, %37, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %874 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%873, %38), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %875 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%874, %36, %42, %37, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %876 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%875, %35, %42, %37, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %877 : int[] = prim::ListConstruct(%867, %868, %35, %870, %871), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %hidden_states.135 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%876, %877, %46), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %879 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.19, %28), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %880 : int = aten::Int(%879), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %881 : int[] = prim::ListConstruct(%867, %880, %870, %871), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %value.9 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.135, %881), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %883 : int = aten::size(%key.9, %38), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %884 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %885 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%884, %44, %42, %37, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %886 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%885, %38, %42, %37, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.15 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%886, %36, %42, %883, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.17 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.9, %key.9, %value.9, %attention_mask.15, %27, %46, %26, %46), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %889 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.17, %44, %38), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.19 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%889, %42), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %891 : int[] = prim::ListConstruct(%797, %798, %45), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %892 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.19, %891), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.33 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%892, %42), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.85 : Tensor = prim::GetAttr[name="weight"](%o_proj.9)
  %hidden_states.137 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.33, %weight.85, %41), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn/__module.model.layers.4.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.139 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%792, %hidden_states.137, %44), scope: __module.model/__module.model.layers.4 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.87 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.9)
  %hidden_states.141 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.139, %33, %46, %46, %41), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %899 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.141, %38), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %900 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm
  %variance.19 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%899, %900, %25, %41), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %902 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.19, %24, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %903 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%902), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.143 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.141, %903), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.145 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.143, %33, %46, %46, %41), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.35 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.87, %hidden_states.145), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %907 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.35, %hidden_states.141)
  %908 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %909 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%907)
  %down_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_53.Linear = prim::GetAttr[name="down_proj"](%mlp.9)
  %up_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_52.Linear = prim::GetAttr[name="up_proj"](%mlp.9)
  %gate_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_51.Linear = prim::GetAttr[name="gate_proj"](%mlp.9)
  %weight.89 : Tensor = prim::GetAttr[name="weight"](%gate_proj.9)
  %input.37 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%908, %weight.89, %41), scope: __module.model/__module.model.layers.4/__module.model.layers.4.mlp/__module.model.layers.4.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %915 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.37), scope: __module.model/__module.model.layers.4/__module.model.layers.4.mlp/__module.model.layers.4.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.91 : Tensor = prim::GetAttr[name="weight"](%up_proj.9)
  %917 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%908, %weight.91, %41), scope: __module.model/__module.model.layers.4/__module.model.layers.4.mlp/__module.model.layers.4.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.39 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%915, %917), scope: __module.model/__module.model.layers.4/__module.model.layers.4.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.93 : Tensor = prim::GetAttr[name="weight"](%down_proj.9)
  %hidden_states.147 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.39, %weight.93, %41), scope: __module.model/__module.model.layers.4/__module.model.layers.4.mlp/__module.model.layers.4.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.149 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%909, %hidden_states.147, %44), scope: __module.model/__module.model.layers.4 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.11 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_68.Qwen2MLP = prim::GetAttr[name="mlp"](%_5)
  %post_attention_layernorm.11 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_70.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_5)
  %self_attn.11 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_63.Qwen2Attention = prim::GetAttr[name="self_attn"](%_5)
  %input_layernorm.11 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_69.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_5)
  %weight.95 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.11)
  %hidden_states.151 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.149, %33, %46, %46, %41), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %928 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.151, %38), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %929 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm
  %variance.21 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%928, %929, %25, %41), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %931 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.21, %24, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %932 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%931), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.153 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.151, %932), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.155 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.153, %33, %46, %46, %41), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.157 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.95, %hidden_states.155), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %936 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.157, %hidden_states.151)
  %937 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %938 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%936)
  %o_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_62.Linear = prim::GetAttr[name="o_proj"](%self_attn.11)
  %v_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_61.Linear = prim::GetAttr[name="v_proj"](%self_attn.11)
  %k_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_60.Linear = prim::GetAttr[name="k_proj"](%self_attn.11)
  %q_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_59.Linear = prim::GetAttr[name="q_proj"](%self_attn.11)
  %943 : int = aten::size(%937, %42), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %944 : int = aten::size(%937, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.31 : Tensor = prim::GetAttr[name="bias"](%q_proj.11)
  %weight.97 : Tensor = prim::GetAttr[name="weight"](%q_proj.11)
  %947 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%937, %weight.97, %bias.31), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn/__module.model.layers.5.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %948 : int[] = prim::ListConstruct(%943, %944, %45, %30), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %949 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%947, %948), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.11 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%949, %44, %38), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.33 : Tensor = prim::GetAttr[name="bias"](%k_proj.11)
  %weight.99 : Tensor = prim::GetAttr[name="weight"](%k_proj.11)
  %953 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%937, %weight.99, %bias.33), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn/__module.model.layers.5.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %954 : int[] = prim::ListConstruct(%943, %944, %45, %30), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %955 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%953, %954), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.11 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%955, %44, %38), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.35 : Tensor = prim::GetAttr[name="bias"](%v_proj.11)
  %weight.101 : Tensor = prim::GetAttr[name="weight"](%v_proj.11)
  %959 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%937, %weight.101, %bias.35), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn/__module.model.layers.5.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %960 : int[] = prim::ListConstruct(%943, %944, %45, %30), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %961 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%959, %960), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.163 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%961, %44, %38), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.15 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.15 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %965 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.11, %cos.15), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %966 : int = aten::size(%q.11, %36), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %967 : Long(device=cpu) = prim::NumToTensor(%966), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %968 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%967, %29), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %969 : int = aten::Int(%968), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %x1.21 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.11, %36, %42, %969, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %971 : int = aten::size(%q.11, %36), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %972 : Long(device=cpu) = prim::NumToTensor(%971), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %973 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%972, %29), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %974 : int = aten::Int(%973), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %x2.21 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.11, %36, %974, %37, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %976 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.21), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %977 : Tensor[] = prim::ListConstruct(%976, %x1.21), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %978 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%977, %45), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %979 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%978, %sin.15), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.11 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%965, %979, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %981 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.11, %cos.15), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %982 : int = aten::size(%k.11, %36), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %983 : Long(device=cpu) = prim::NumToTensor(%982), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %984 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%983, %29), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %985 : int = aten::Int(%984), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %x1.23 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.11, %36, %42, %985, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %987 : int = aten::size(%k.11, %36), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %988 : Long(device=cpu) = prim::NumToTensor(%987), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %989 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%988, %29), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %990 : int = aten::Int(%989), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %x2.23 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.11, %36, %990, %37, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %992 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.23), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %993 : Tensor[] = prim::ListConstruct(%992, %x1.23), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %994 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%993, %45), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %995 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%994, %sin.15), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.159 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%981, %995, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %997 : int = aten::size(%hidden_states.159, %42), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %998 : int = aten::size(%hidden_states.159, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.21 : Long(device=cpu) = prim::NumToTensor(%998), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %1000 : int = aten::size(%hidden_states.159, %38), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1001 : int = aten::size(%hidden_states.159, %36), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1002 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.159, %42, %42, %37, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1003 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1002, %44, %42, %37, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1004 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1003, %38), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1005 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1004, %36, %42, %37, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1006 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1005, %35, %42, %37, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1007 : int[] = prim::ListConstruct(%997, %998, %35, %1000, %1001), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %hidden_states.161 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1006, %1007, %46), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1009 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.21, %28), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1010 : int = aten::Int(%1009), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %1011 : int[] = prim::ListConstruct(%997, %1010, %1000, %1001), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %key.11 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.161, %1011), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1013 : int = aten::size(%hidden_states.163, %42), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1014 : int = aten::size(%hidden_states.163, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.23 : Long(device=cpu) = prim::NumToTensor(%1014), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %1016 : int = aten::size(%hidden_states.163, %38), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1017 : int = aten::size(%hidden_states.163, %36), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1018 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.163, %42, %42, %37, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1019 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1018, %44, %42, %37, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1020 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1019, %38), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1021 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1020, %36, %42, %37, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1022 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1021, %35, %42, %37, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1023 : int[] = prim::ListConstruct(%1013, %1014, %35, %1016, %1017), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %hidden_states.165 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1022, %1023, %46), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1025 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.23, %28), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1026 : int = aten::Int(%1025), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %1027 : int[] = prim::ListConstruct(%1013, %1026, %1016, %1017), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %value.11 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.165, %1027), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1029 : int = aten::size(%key.11, %38), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1030 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1031 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1030, %44, %42, %37, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1032 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1031, %38, %42, %37, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.17 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1032, %36, %42, %1029, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.21 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.11, %key.11, %value.11, %attention_mask.17, %27, %46, %26, %46), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %1035 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.21, %44, %38), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.23 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1035, %42), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %1037 : int[] = prim::ListConstruct(%943, %944, %45), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %1038 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.23, %1037), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.41 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%1038, %42), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.103 : Tensor = prim::GetAttr[name="weight"](%o_proj.11)
  %hidden_states.167 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.41, %weight.103, %41), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn/__module.model.layers.5.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.169 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%938, %hidden_states.167, %44), scope: __module.model/__module.model.layers.5 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.105 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.11)
  %hidden_states.171 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.169, %33, %46, %46, %41), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %1045 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.171, %38), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1046 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm
  %variance.23 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1045, %1046, %25, %41), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1048 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.23, %24, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %1049 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1048), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.173 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.171, %1049), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.175 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.173, %33, %46, %46, %41), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.43 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.105, %hidden_states.175), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %1053 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.43, %hidden_states.171)
  %1054 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1055 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1053)
  %down_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_66.Linear = prim::GetAttr[name="down_proj"](%mlp.11)
  %up_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_65.Linear = prim::GetAttr[name="up_proj"](%mlp.11)
  %gate_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_64.Linear = prim::GetAttr[name="gate_proj"](%mlp.11)
  %weight.107 : Tensor = prim::GetAttr[name="weight"](%gate_proj.11)
  %input.45 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%1054, %weight.107, %41), scope: __module.model/__module.model.layers.5/__module.model.layers.5.mlp/__module.model.layers.5.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1061 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.45), scope: __module.model/__module.model.layers.5/__module.model.layers.5.mlp/__module.model.layers.5.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.109 : Tensor = prim::GetAttr[name="weight"](%up_proj.11)
  %1063 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%1054, %weight.109, %41), scope: __module.model/__module.model.layers.5/__module.model.layers.5.mlp/__module.model.layers.5.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.47 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%1061, %1063), scope: __module.model/__module.model.layers.5/__module.model.layers.5.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.111 : Tensor = prim::GetAttr[name="weight"](%down_proj.11)
  %hidden_states.177 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.47, %weight.111, %41), scope: __module.model/__module.model.layers.5/__module.model.layers.5.mlp/__module.model.layers.5.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.179 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1055, %hidden_states.177, %44), scope: __module.model/__module.model.layers.5 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.13 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_81.Qwen2MLP = prim::GetAttr[name="mlp"](%_6)
  %post_attention_layernorm.13 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_83.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_6)
  %self_attn.13 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_76.Qwen2Attention = prim::GetAttr[name="self_attn"](%_6)
  %input_layernorm.13 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_82.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_6)
  %weight.113 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.13)
  %hidden_states.181 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.179, %33, %46, %46, %41), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %1074 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.181, %38), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1075 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm
  %variance.25 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1074, %1075, %25, %41), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1077 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.25, %24, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %1078 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1077), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.183 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.181, %1078), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.185 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.183, %33, %46, %46, %41), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.187 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.113, %hidden_states.185), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %1082 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.187, %hidden_states.181)
  %1083 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1084 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1082)
  %o_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_75.Linear = prim::GetAttr[name="o_proj"](%self_attn.13)
  %v_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_74.Linear = prim::GetAttr[name="v_proj"](%self_attn.13)
  %k_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_73.Linear = prim::GetAttr[name="k_proj"](%self_attn.13)
  %q_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_72.Linear = prim::GetAttr[name="q_proj"](%self_attn.13)
  %1089 : int = aten::size(%1083, %42), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %1090 : int = aten::size(%1083, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.37 : Tensor = prim::GetAttr[name="bias"](%q_proj.13)
  %weight.115 : Tensor = prim::GetAttr[name="weight"](%q_proj.13)
  %1093 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1083, %weight.115, %bias.37), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn/__module.model.layers.6.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1094 : int[] = prim::ListConstruct(%1089, %1090, %45, %30), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1095 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%1093, %1094), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.13 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1095, %44, %38), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.39 : Tensor = prim::GetAttr[name="bias"](%k_proj.13)
  %weight.117 : Tensor = prim::GetAttr[name="weight"](%k_proj.13)
  %1099 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1083, %weight.117, %bias.39), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn/__module.model.layers.6.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1100 : int[] = prim::ListConstruct(%1089, %1090, %45, %30), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1101 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1099, %1100), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.13 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1101, %44, %38), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.41 : Tensor = prim::GetAttr[name="bias"](%v_proj.13)
  %weight.119 : Tensor = prim::GetAttr[name="weight"](%v_proj.13)
  %1105 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1083, %weight.119, %bias.41), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn/__module.model.layers.6.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1106 : int[] = prim::ListConstruct(%1089, %1090, %45, %30), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1107 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1105, %1106), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.193 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1107, %44, %38), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.17 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.17 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %1111 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.13, %cos.17), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %1112 : int = aten::size(%q.13, %36), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1113 : Long(device=cpu) = prim::NumToTensor(%1112), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1114 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1113, %29), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1115 : int = aten::Int(%1114), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %x1.25 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.13, %36, %42, %1115, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1117 : int = aten::size(%q.13, %36), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1118 : Long(device=cpu) = prim::NumToTensor(%1117), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1119 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1118, %29), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1120 : int = aten::Int(%1119), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %x2.25 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.13, %36, %1120, %37, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1122 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.25), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1123 : Tensor[] = prim::ListConstruct(%1122, %x1.25), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1124 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1123, %45), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1125 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1124, %sin.17), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.13 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1111, %1125, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %1127 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.13, %cos.17), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %1128 : int = aten::size(%k.13, %36), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1129 : Long(device=cpu) = prim::NumToTensor(%1128), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1130 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1129, %29), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1131 : int = aten::Int(%1130), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %x1.27 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.13, %36, %42, %1131, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1133 : int = aten::size(%k.13, %36), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1134 : Long(device=cpu) = prim::NumToTensor(%1133), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1135 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1134, %29), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1136 : int = aten::Int(%1135), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %x2.27 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.13, %36, %1136, %37, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1138 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.27), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1139 : Tensor[] = prim::ListConstruct(%1138, %x1.27), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1140 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1139, %45), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1141 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1140, %sin.17), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.189 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1127, %1141, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %1143 : int = aten::size(%hidden_states.189, %42), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1144 : int = aten::size(%hidden_states.189, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.25 : Long(device=cpu) = prim::NumToTensor(%1144), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1146 : int = aten::size(%hidden_states.189, %38), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1147 : int = aten::size(%hidden_states.189, %36), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1148 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.189, %42, %42, %37, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1149 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1148, %44, %42, %37, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1150 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1149, %38), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1151 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1150, %36, %42, %37, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1152 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1151, %35, %42, %37, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1153 : int[] = prim::ListConstruct(%1143, %1144, %35, %1146, %1147), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %hidden_states.191 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1152, %1153, %46), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1155 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.25, %28), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1156 : int = aten::Int(%1155), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1157 : int[] = prim::ListConstruct(%1143, %1156, %1146, %1147), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %key.13 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.191, %1157), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1159 : int = aten::size(%hidden_states.193, %42), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1160 : int = aten::size(%hidden_states.193, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.27 : Long(device=cpu) = prim::NumToTensor(%1160), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1162 : int = aten::size(%hidden_states.193, %38), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1163 : int = aten::size(%hidden_states.193, %36), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1164 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.193, %42, %42, %37, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1165 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1164, %44, %42, %37, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1166 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1165, %38), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1167 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1166, %36, %42, %37, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1168 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1167, %35, %42, %37, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1169 : int[] = prim::ListConstruct(%1159, %1160, %35, %1162, %1163), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %hidden_states.195 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1168, %1169, %46), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1171 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.27, %28), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1172 : int = aten::Int(%1171), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1173 : int[] = prim::ListConstruct(%1159, %1172, %1162, %1163), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %value.13 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.195, %1173), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1175 : int = aten::size(%key.13, %38), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1176 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1177 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1176, %44, %42, %37, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1178 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1177, %38, %42, %37, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.19 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1178, %36, %42, %1175, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.25 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.13, %key.13, %value.13, %attention_mask.19, %27, %46, %26, %46), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %1181 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.25, %44, %38), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.27 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1181, %42), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %1183 : int[] = prim::ListConstruct(%1089, %1090, %45), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1184 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.27, %1183), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.49 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%1184, %42), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.121 : Tensor = prim::GetAttr[name="weight"](%o_proj.13)
  %hidden_states.197 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.49, %weight.121, %41), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn/__module.model.layers.6.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.199 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1084, %hidden_states.197, %44), scope: __module.model/__module.model.layers.6 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.123 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.13)
  %hidden_states.201 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.199, %33, %46, %46, %41), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %1191 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.201, %38), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1192 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm
  %variance.27 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1191, %1192, %25, %41), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1194 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.27, %24, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %1195 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1194), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.203 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.201, %1195), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.205 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.203, %33, %46, %46, %41), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.51 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.123, %hidden_states.205), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %1199 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.51, %hidden_states.201)
  %1200 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1201 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1199)
  %down_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_79.Linear = prim::GetAttr[name="down_proj"](%mlp.13)
  %up_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_78.Linear = prim::GetAttr[name="up_proj"](%mlp.13)
  %gate_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_77.Linear = prim::GetAttr[name="gate_proj"](%mlp.13)
  %weight.125 : Tensor = prim::GetAttr[name="weight"](%gate_proj.13)
  %input.53 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%1200, %weight.125, %41), scope: __module.model/__module.model.layers.6/__module.model.layers.6.mlp/__module.model.layers.6.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1207 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.53), scope: __module.model/__module.model.layers.6/__module.model.layers.6.mlp/__module.model.layers.6.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.127 : Tensor = prim::GetAttr[name="weight"](%up_proj.13)
  %1209 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%1200, %weight.127, %41), scope: __module.model/__module.model.layers.6/__module.model.layers.6.mlp/__module.model.layers.6.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.55 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%1207, %1209), scope: __module.model/__module.model.layers.6/__module.model.layers.6.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.129 : Tensor = prim::GetAttr[name="weight"](%down_proj.13)
  %hidden_states.207 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.55, %weight.129, %41), scope: __module.model/__module.model.layers.6/__module.model.layers.6.mlp/__module.model.layers.6.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.209 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1201, %hidden_states.207, %44), scope: __module.model/__module.model.layers.6 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.15 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_94.Qwen2MLP = prim::GetAttr[name="mlp"](%_7)
  %post_attention_layernorm.15 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_96.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_7)
  %self_attn.15 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_89.Qwen2Attention = prim::GetAttr[name="self_attn"](%_7)
  %input_layernorm.15 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_95.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_7)
  %weight.131 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.15)
  %hidden_states.211 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.209, %33, %46, %46, %41), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %1220 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.211, %38), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1221 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm
  %variance.29 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1220, %1221, %25, %41), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1223 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.29, %24, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %1224 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1223), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.213 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.211, %1224), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.215 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.213, %33, %46, %46, %41), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.217 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.131, %hidden_states.215), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %1228 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.217, %hidden_states.211)
  %1229 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1230 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1228)
  %o_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_88.Linear = prim::GetAttr[name="o_proj"](%self_attn.15)
  %v_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_87.Linear = prim::GetAttr[name="v_proj"](%self_attn.15)
  %k_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_86.Linear = prim::GetAttr[name="k_proj"](%self_attn.15)
  %q_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_85.Linear = prim::GetAttr[name="q_proj"](%self_attn.15)
  %1235 : int = aten::size(%1229, %42), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %1236 : int = aten::size(%1229, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.43 : Tensor = prim::GetAttr[name="bias"](%q_proj.15)
  %weight.133 : Tensor = prim::GetAttr[name="weight"](%q_proj.15)
  %1239 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1229, %weight.133, %bias.43), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn/__module.model.layers.7.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1240 : int[] = prim::ListConstruct(%1235, %1236, %45, %30), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1241 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%1239, %1240), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.15 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1241, %44, %38), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.45 : Tensor = prim::GetAttr[name="bias"](%k_proj.15)
  %weight.135 : Tensor = prim::GetAttr[name="weight"](%k_proj.15)
  %1245 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1229, %weight.135, %bias.45), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn/__module.model.layers.7.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1246 : int[] = prim::ListConstruct(%1235, %1236, %45, %30), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1247 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1245, %1246), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.15 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1247, %44, %38), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.47 : Tensor = prim::GetAttr[name="bias"](%v_proj.15)
  %weight.137 : Tensor = prim::GetAttr[name="weight"](%v_proj.15)
  %1251 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1229, %weight.137, %bias.47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn/__module.model.layers.7.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1252 : int[] = prim::ListConstruct(%1235, %1236, %45, %30), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1253 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1251, %1252), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.223 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1253, %44, %38), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.19 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.19 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %1257 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.15, %cos.19), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %1258 : int = aten::size(%q.15, %36), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1259 : Long(device=cpu) = prim::NumToTensor(%1258), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1260 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1259, %29), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1261 : int = aten::Int(%1260), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %x1.29 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.15, %36, %42, %1261, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1263 : int = aten::size(%q.15, %36), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1264 : Long(device=cpu) = prim::NumToTensor(%1263), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1265 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1264, %29), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1266 : int = aten::Int(%1265), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %x2.29 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.15, %36, %1266, %37, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1268 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.29), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1269 : Tensor[] = prim::ListConstruct(%1268, %x1.29), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1270 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1269, %45), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1271 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1270, %sin.19), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.15 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1257, %1271, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %1273 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.15, %cos.19), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %1274 : int = aten::size(%k.15, %36), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1275 : Long(device=cpu) = prim::NumToTensor(%1274), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1276 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1275, %29), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1277 : int = aten::Int(%1276), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %x1.31 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.15, %36, %42, %1277, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1279 : int = aten::size(%k.15, %36), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1280 : Long(device=cpu) = prim::NumToTensor(%1279), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1281 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1280, %29), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1282 : int = aten::Int(%1281), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %x2.31 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.15, %36, %1282, %37, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1284 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.31), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1285 : Tensor[] = prim::ListConstruct(%1284, %x1.31), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1286 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1285, %45), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1287 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1286, %sin.19), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.219 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1273, %1287, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %1289 : int = aten::size(%hidden_states.219, %42), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1290 : int = aten::size(%hidden_states.219, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.29 : Long(device=cpu) = prim::NumToTensor(%1290), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1292 : int = aten::size(%hidden_states.219, %38), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1293 : int = aten::size(%hidden_states.219, %36), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1294 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.219, %42, %42, %37, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1295 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1294, %44, %42, %37, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1296 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1295, %38), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1297 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1296, %36, %42, %37, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1298 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1297, %35, %42, %37, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1299 : int[] = prim::ListConstruct(%1289, %1290, %35, %1292, %1293), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %hidden_states.221 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1298, %1299, %46), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1301 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.29, %28), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1302 : int = aten::Int(%1301), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1303 : int[] = prim::ListConstruct(%1289, %1302, %1292, %1293), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %key.15 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.221, %1303), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1305 : int = aten::size(%hidden_states.223, %42), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1306 : int = aten::size(%hidden_states.223, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.31 : Long(device=cpu) = prim::NumToTensor(%1306), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1308 : int = aten::size(%hidden_states.223, %38), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1309 : int = aten::size(%hidden_states.223, %36), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1310 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.223, %42, %42, %37, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1311 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1310, %44, %42, %37, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1312 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1311, %38), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1313 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1312, %36, %42, %37, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1314 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1313, %35, %42, %37, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1315 : int[] = prim::ListConstruct(%1305, %1306, %35, %1308, %1309), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %hidden_states.225 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1314, %1315, %46), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1317 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.31, %28), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1318 : int = aten::Int(%1317), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1319 : int[] = prim::ListConstruct(%1305, %1318, %1308, %1309), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %value.15 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.225, %1319), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1321 : int = aten::size(%key.15, %38), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1322 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1323 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1322, %44, %42, %37, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1324 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1323, %38, %42, %37, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.21 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1324, %36, %42, %1321, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.29 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.15, %key.15, %value.15, %attention_mask.21, %27, %46, %26, %46), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %1327 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.29, %44, %38), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.31 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1327, %42), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %1329 : int[] = prim::ListConstruct(%1235, %1236, %45), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1330 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.31, %1329), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.57 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%1330, %42), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.139 : Tensor = prim::GetAttr[name="weight"](%o_proj.15)
  %hidden_states.227 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.57, %weight.139, %41), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn/__module.model.layers.7.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.229 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1230, %hidden_states.227, %44), scope: __module.model/__module.model.layers.7 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.141 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.15)
  %hidden_states.231 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.229, %33, %46, %46, %41), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %1337 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.231, %38), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1338 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm
  %variance.31 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1337, %1338, %25, %41), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1340 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.31, %24, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %1341 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1340), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.233 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.231, %1341), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.235 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.233, %33, %46, %46, %41), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.59 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.141, %hidden_states.235), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %1345 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.59, %hidden_states.231)
  %1346 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1347 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1345)
  %down_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_92.Linear = prim::GetAttr[name="down_proj"](%mlp.15)
  %up_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_91.Linear = prim::GetAttr[name="up_proj"](%mlp.15)
  %gate_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_90.Linear = prim::GetAttr[name="gate_proj"](%mlp.15)
  %weight.143 : Tensor = prim::GetAttr[name="weight"](%gate_proj.15)
  %input.61 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%1346, %weight.143, %41), scope: __module.model/__module.model.layers.7/__module.model.layers.7.mlp/__module.model.layers.7.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1353 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.61), scope: __module.model/__module.model.layers.7/__module.model.layers.7.mlp/__module.model.layers.7.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.145 : Tensor = prim::GetAttr[name="weight"](%up_proj.15)
  %1355 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%1346, %weight.145, %41), scope: __module.model/__module.model.layers.7/__module.model.layers.7.mlp/__module.model.layers.7.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.63 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%1353, %1355), scope: __module.model/__module.model.layers.7/__module.model.layers.7.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.147 : Tensor = prim::GetAttr[name="weight"](%down_proj.15)
  %hidden_states.237 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.63, %weight.147, %41), scope: __module.model/__module.model.layers.7/__module.model.layers.7.mlp/__module.model.layers.7.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.239 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1347, %hidden_states.237, %44), scope: __module.model/__module.model.layers.7 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.17 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_107.Qwen2MLP = prim::GetAttr[name="mlp"](%_8)
  %post_attention_layernorm.17 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_109.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_8)
  %self_attn.17 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_102.Qwen2Attention = prim::GetAttr[name="self_attn"](%_8)
  %input_layernorm.17 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_108.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_8)
  %weight.149 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.17)
  %hidden_states.241 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.239, %33, %46, %46, %41), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %1366 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.241, %38), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1367 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm
  %variance.33 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1366, %1367, %25, %41), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1369 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.33, %24, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %1370 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1369), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.243 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.241, %1370), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.245 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.243, %33, %46, %46, %41), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.247 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.149, %hidden_states.245), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %1374 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.247, %hidden_states.241)
  %1375 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1376 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1374)
  %o_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_101.Linear = prim::GetAttr[name="o_proj"](%self_attn.17)
  %v_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_100.Linear = prim::GetAttr[name="v_proj"](%self_attn.17)
  %k_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_99.Linear = prim::GetAttr[name="k_proj"](%self_attn.17)
  %q_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_98.Linear = prim::GetAttr[name="q_proj"](%self_attn.17)
  %1381 : int = aten::size(%1375, %42), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %1382 : int = aten::size(%1375, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.49 : Tensor = prim::GetAttr[name="bias"](%q_proj.17)
  %weight.151 : Tensor = prim::GetAttr[name="weight"](%q_proj.17)
  %1385 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1375, %weight.151, %bias.49), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn/__module.model.layers.8.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1386 : int[] = prim::ListConstruct(%1381, %1382, %45, %30), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1387 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%1385, %1386), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.17 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1387, %44, %38), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.51 : Tensor = prim::GetAttr[name="bias"](%k_proj.17)
  %weight.153 : Tensor = prim::GetAttr[name="weight"](%k_proj.17)
  %1391 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1375, %weight.153, %bias.51), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn/__module.model.layers.8.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1392 : int[] = prim::ListConstruct(%1381, %1382, %45, %30), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1393 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1391, %1392), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.17 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1393, %44, %38), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.53 : Tensor = prim::GetAttr[name="bias"](%v_proj.17)
  %weight.155 : Tensor = prim::GetAttr[name="weight"](%v_proj.17)
  %1397 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1375, %weight.155, %bias.53), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn/__module.model.layers.8.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1398 : int[] = prim::ListConstruct(%1381, %1382, %45, %30), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1399 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1397, %1398), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.253 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1399, %44, %38), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.21 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.21 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %1403 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.17, %cos.21), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %1404 : int = aten::size(%q.17, %36), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1405 : Long(device=cpu) = prim::NumToTensor(%1404), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1406 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1405, %29), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1407 : int = aten::Int(%1406), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %x1.33 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.17, %36, %42, %1407, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1409 : int = aten::size(%q.17, %36), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1410 : Long(device=cpu) = prim::NumToTensor(%1409), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1411 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1410, %29), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1412 : int = aten::Int(%1411), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %x2.33 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.17, %36, %1412, %37, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1414 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.33), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1415 : Tensor[] = prim::ListConstruct(%1414, %x1.33), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1416 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1415, %45), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1417 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1416, %sin.21), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.17 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1403, %1417, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %1419 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.17, %cos.21), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %1420 : int = aten::size(%k.17, %36), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1421 : Long(device=cpu) = prim::NumToTensor(%1420), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1422 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1421, %29), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1423 : int = aten::Int(%1422), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %x1.35 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.17, %36, %42, %1423, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1425 : int = aten::size(%k.17, %36), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1426 : Long(device=cpu) = prim::NumToTensor(%1425), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1427 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1426, %29), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1428 : int = aten::Int(%1427), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %x2.35 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.17, %36, %1428, %37, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1430 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.35), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1431 : Tensor[] = prim::ListConstruct(%1430, %x1.35), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1432 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1431, %45), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1433 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1432, %sin.21), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.249 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1419, %1433, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %1435 : int = aten::size(%hidden_states.249, %42), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1436 : int = aten::size(%hidden_states.249, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.33 : Long(device=cpu) = prim::NumToTensor(%1436), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1438 : int = aten::size(%hidden_states.249, %38), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1439 : int = aten::size(%hidden_states.249, %36), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1440 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.249, %42, %42, %37, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1441 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1440, %44, %42, %37, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1442 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1441, %38), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1443 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1442, %36, %42, %37, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1444 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1443, %35, %42, %37, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1445 : int[] = prim::ListConstruct(%1435, %1436, %35, %1438, %1439), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %hidden_states.251 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1444, %1445, %46), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1447 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.33, %28), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1448 : int = aten::Int(%1447), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1449 : int[] = prim::ListConstruct(%1435, %1448, %1438, %1439), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %key.17 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.251, %1449), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1451 : int = aten::size(%hidden_states.253, %42), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1452 : int = aten::size(%hidden_states.253, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.35 : Long(device=cpu) = prim::NumToTensor(%1452), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1454 : int = aten::size(%hidden_states.253, %38), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1455 : int = aten::size(%hidden_states.253, %36), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1456 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.253, %42, %42, %37, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1457 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1456, %44, %42, %37, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1458 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1457, %38), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1459 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1458, %36, %42, %37, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1460 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1459, %35, %42, %37, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1461 : int[] = prim::ListConstruct(%1451, %1452, %35, %1454, %1455), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %hidden_states.255 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1460, %1461, %46), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1463 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.35, %28), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1464 : int = aten::Int(%1463), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1465 : int[] = prim::ListConstruct(%1451, %1464, %1454, %1455), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %value.17 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.255, %1465), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1467 : int = aten::size(%key.17, %38), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1468 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1469 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1468, %44, %42, %37, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1470 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1469, %38, %42, %37, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.23 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1470, %36, %42, %1467, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.33 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.17, %key.17, %value.17, %attention_mask.23, %27, %46, %26, %46), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %1473 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.33, %44, %38), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.35 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1473, %42), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %1475 : int[] = prim::ListConstruct(%1381, %1382, %45), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1476 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.35, %1475), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.65 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%1476, %42), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.157 : Tensor = prim::GetAttr[name="weight"](%o_proj.17)
  %hidden_states.257 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.65, %weight.157, %41), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn/__module.model.layers.8.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.259 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1376, %hidden_states.257, %44), scope: __module.model/__module.model.layers.8 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.159 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.17)
  %hidden_states.261 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.259, %33, %46, %46, %41), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %1483 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.261, %38), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1484 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm
  %variance.35 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1483, %1484, %25, %41), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1486 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.35, %24, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %1487 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1486), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.263 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.261, %1487), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.265 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.263, %33, %46, %46, %41), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.67 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.159, %hidden_states.265), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %1491 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.67, %hidden_states.261)
  %1492 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1493 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1491)
  %down_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_105.Linear = prim::GetAttr[name="down_proj"](%mlp.17)
  %up_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_104.Linear = prim::GetAttr[name="up_proj"](%mlp.17)
  %gate_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_103.Linear = prim::GetAttr[name="gate_proj"](%mlp.17)
  %weight.161 : Tensor = prim::GetAttr[name="weight"](%gate_proj.17)
  %input.69 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%1492, %weight.161, %41), scope: __module.model/__module.model.layers.8/__module.model.layers.8.mlp/__module.model.layers.8.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1499 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.69), scope: __module.model/__module.model.layers.8/__module.model.layers.8.mlp/__module.model.layers.8.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.163 : Tensor = prim::GetAttr[name="weight"](%up_proj.17)
  %1501 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%1492, %weight.163, %41), scope: __module.model/__module.model.layers.8/__module.model.layers.8.mlp/__module.model.layers.8.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.71 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%1499, %1501), scope: __module.model/__module.model.layers.8/__module.model.layers.8.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.165 : Tensor = prim::GetAttr[name="weight"](%down_proj.17)
  %hidden_states.267 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.71, %weight.165, %41), scope: __module.model/__module.model.layers.8/__module.model.layers.8.mlp/__module.model.layers.8.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.269 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1493, %hidden_states.267, %44), scope: __module.model/__module.model.layers.8 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.19 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_120.Qwen2MLP = prim::GetAttr[name="mlp"](%_9)
  %post_attention_layernorm.19 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_122.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_9)
  %self_attn.19 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_115.Qwen2Attention = prim::GetAttr[name="self_attn"](%_9)
  %input_layernorm.19 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_121.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_9)
  %weight.167 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.19)
  %hidden_states.271 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.269, %33, %46, %46, %41), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %1512 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.271, %38), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1513 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm
  %variance.37 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1512, %1513, %25, %41), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1515 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.37, %24, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %1516 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1515), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.273 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.271, %1516), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.275 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.273, %33, %46, %46, %41), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.277 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.167, %hidden_states.275), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %1520 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.277, %hidden_states.271)
  %1521 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1522 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1520)
  %o_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_114.Linear = prim::GetAttr[name="o_proj"](%self_attn.19)
  %v_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_113.Linear = prim::GetAttr[name="v_proj"](%self_attn.19)
  %k_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_112.Linear = prim::GetAttr[name="k_proj"](%self_attn.19)
  %q_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_111.Linear = prim::GetAttr[name="q_proj"](%self_attn.19)
  %1527 : int = aten::size(%1521, %42), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %1528 : int = aten::size(%1521, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.55 : Tensor = prim::GetAttr[name="bias"](%q_proj.19)
  %weight.169 : Tensor = prim::GetAttr[name="weight"](%q_proj.19)
  %1531 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1521, %weight.169, %bias.55), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn/__module.model.layers.9.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1532 : int[] = prim::ListConstruct(%1527, %1528, %45, %30), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1533 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%1531, %1532), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.19 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1533, %44, %38), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.57 : Tensor = prim::GetAttr[name="bias"](%k_proj.19)
  %weight.171 : Tensor = prim::GetAttr[name="weight"](%k_proj.19)
  %1537 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1521, %weight.171, %bias.57), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn/__module.model.layers.9.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1538 : int[] = prim::ListConstruct(%1527, %1528, %45, %30), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1539 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1537, %1538), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.19 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1539, %44, %38), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.59 : Tensor = prim::GetAttr[name="bias"](%v_proj.19)
  %weight.173 : Tensor = prim::GetAttr[name="weight"](%v_proj.19)
  %1543 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1521, %weight.173, %bias.59), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn/__module.model.layers.9.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1544 : int[] = prim::ListConstruct(%1527, %1528, %45, %30), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1545 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1543, %1544), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.283 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1545, %44, %38), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.23 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.23 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %1549 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.19, %cos.23), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %1550 : int = aten::size(%q.19, %36), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1551 : Long(device=cpu) = prim::NumToTensor(%1550), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1552 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1551, %29), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1553 : int = aten::Int(%1552), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %x1.37 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.19, %36, %42, %1553, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1555 : int = aten::size(%q.19, %36), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1556 : Long(device=cpu) = prim::NumToTensor(%1555), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1557 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1556, %29), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1558 : int = aten::Int(%1557), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %x2.37 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.19, %36, %1558, %37, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1560 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.37), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1561 : Tensor[] = prim::ListConstruct(%1560, %x1.37), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1562 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1561, %45), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1563 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1562, %sin.23), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.19 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1549, %1563, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %1565 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.19, %cos.23), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %1566 : int = aten::size(%k.19, %36), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1567 : Long(device=cpu) = prim::NumToTensor(%1566), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1568 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1567, %29), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1569 : int = aten::Int(%1568), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %x1.39 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.19, %36, %42, %1569, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1571 : int = aten::size(%k.19, %36), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1572 : Long(device=cpu) = prim::NumToTensor(%1571), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1573 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1572, %29), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1574 : int = aten::Int(%1573), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %x2.39 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.19, %36, %1574, %37, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1576 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.39), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1577 : Tensor[] = prim::ListConstruct(%1576, %x1.39), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1578 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1577, %45), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1579 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1578, %sin.23), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.279 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1565, %1579, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %1581 : int = aten::size(%hidden_states.279, %42), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1582 : int = aten::size(%hidden_states.279, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.37 : Long(device=cpu) = prim::NumToTensor(%1582), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1584 : int = aten::size(%hidden_states.279, %38), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1585 : int = aten::size(%hidden_states.279, %36), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1586 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.279, %42, %42, %37, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1587 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1586, %44, %42, %37, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1588 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1587, %38), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1589 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1588, %36, %42, %37, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1590 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1589, %35, %42, %37, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1591 : int[] = prim::ListConstruct(%1581, %1582, %35, %1584, %1585), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %hidden_states.281 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1590, %1591, %46), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1593 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.37, %28), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1594 : int = aten::Int(%1593), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1595 : int[] = prim::ListConstruct(%1581, %1594, %1584, %1585), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %key.19 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.281, %1595), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1597 : int = aten::size(%hidden_states.283, %42), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1598 : int = aten::size(%hidden_states.283, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.39 : Long(device=cpu) = prim::NumToTensor(%1598), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1600 : int = aten::size(%hidden_states.283, %38), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1601 : int = aten::size(%hidden_states.283, %36), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1602 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.283, %42, %42, %37, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1603 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1602, %44, %42, %37, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1604 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1603, %38), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1605 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1604, %36, %42, %37, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1606 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1605, %35, %42, %37, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1607 : int[] = prim::ListConstruct(%1597, %1598, %35, %1600, %1601), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %hidden_states.285 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1606, %1607, %46), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1609 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.39, %28), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1610 : int = aten::Int(%1609), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1611 : int[] = prim::ListConstruct(%1597, %1610, %1600, %1601), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %value.19 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.285, %1611), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1613 : int = aten::size(%key.19, %38), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1614 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1615 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1614, %44, %42, %37, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1616 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1615, %38, %42, %37, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.25 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1616, %36, %42, %1613, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.37 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.19, %key.19, %value.19, %attention_mask.25, %27, %46, %26, %46), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %1619 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.37, %44, %38), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.39 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1619, %42), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %1621 : int[] = prim::ListConstruct(%1527, %1528, %45), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1622 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.39, %1621), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.73 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%1622, %42), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.175 : Tensor = prim::GetAttr[name="weight"](%o_proj.19)
  %hidden_states.287 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.73, %weight.175, %41), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn/__module.model.layers.9.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.289 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1522, %hidden_states.287, %44), scope: __module.model/__module.model.layers.9 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.177 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.19)
  %hidden_states.291 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.289, %33, %46, %46, %41), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %1629 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.291, %38), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1630 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm
  %variance.39 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1629, %1630, %25, %41), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1632 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.39, %24, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %1633 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1632), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.293 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.291, %1633), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.295 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.293, %33, %46, %46, %41), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.75 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.177, %hidden_states.295), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %1637 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.75, %hidden_states.291)
  %1638 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1639 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1637)
  %down_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_118.Linear = prim::GetAttr[name="down_proj"](%mlp.19)
  %up_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_117.Linear = prim::GetAttr[name="up_proj"](%mlp.19)
  %gate_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_116.Linear = prim::GetAttr[name="gate_proj"](%mlp.19)
  %weight.179 : Tensor = prim::GetAttr[name="weight"](%gate_proj.19)
  %input.77 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%1638, %weight.179, %41), scope: __module.model/__module.model.layers.9/__module.model.layers.9.mlp/__module.model.layers.9.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1645 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.77), scope: __module.model/__module.model.layers.9/__module.model.layers.9.mlp/__module.model.layers.9.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.181 : Tensor = prim::GetAttr[name="weight"](%up_proj.19)
  %1647 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%1638, %weight.181, %41), scope: __module.model/__module.model.layers.9/__module.model.layers.9.mlp/__module.model.layers.9.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.79 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%1645, %1647), scope: __module.model/__module.model.layers.9/__module.model.layers.9.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.183 : Tensor = prim::GetAttr[name="weight"](%down_proj.19)
  %hidden_states.297 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.79, %weight.183, %41), scope: __module.model/__module.model.layers.9/__module.model.layers.9.mlp/__module.model.layers.9.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.299 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1639, %hidden_states.297, %44), scope: __module.model/__module.model.layers.9 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.21 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_133.Qwen2MLP = prim::GetAttr[name="mlp"](%_10)
  %post_attention_layernorm.21 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_135.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_10)
  %self_attn.21 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_128.Qwen2Attention = prim::GetAttr[name="self_attn"](%_10)
  %input_layernorm.21 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_134.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_10)
  %weight.185 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.21)
  %hidden_states.301 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.299, %33, %46, %46, %41), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %1658 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.301, %38), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1659 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm
  %variance.41 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1658, %1659, %25, %41), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1661 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.41, %24, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %1662 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1661), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.303 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.301, %1662), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.305 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.303, %33, %46, %46, %41), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.307 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.185, %hidden_states.305), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %1666 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.307, %hidden_states.301)
  %1667 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1668 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1666)
  %o_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_127.Linear = prim::GetAttr[name="o_proj"](%self_attn.21)
  %v_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_126.Linear = prim::GetAttr[name="v_proj"](%self_attn.21)
  %k_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_125.Linear = prim::GetAttr[name="k_proj"](%self_attn.21)
  %q_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_124.Linear = prim::GetAttr[name="q_proj"](%self_attn.21)
  %1673 : int = aten::size(%1667, %42), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %1674 : int = aten::size(%1667, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.61 : Tensor = prim::GetAttr[name="bias"](%q_proj.21)
  %weight.187 : Tensor = prim::GetAttr[name="weight"](%q_proj.21)
  %1677 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1667, %weight.187, %bias.61), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn/__module.model.layers.10.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1678 : int[] = prim::ListConstruct(%1673, %1674, %45, %30), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1679 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%1677, %1678), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.21 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1679, %44, %38), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.63 : Tensor = prim::GetAttr[name="bias"](%k_proj.21)
  %weight.189 : Tensor = prim::GetAttr[name="weight"](%k_proj.21)
  %1683 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1667, %weight.189, %bias.63), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn/__module.model.layers.10.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1684 : int[] = prim::ListConstruct(%1673, %1674, %45, %30), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1685 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1683, %1684), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.21 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1685, %44, %38), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.65 : Tensor = prim::GetAttr[name="bias"](%v_proj.21)
  %weight.191 : Tensor = prim::GetAttr[name="weight"](%v_proj.21)
  %1689 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1667, %weight.191, %bias.65), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn/__module.model.layers.10.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1690 : int[] = prim::ListConstruct(%1673, %1674, %45, %30), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1691 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1689, %1690), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.313 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1691, %44, %38), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.25 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.25 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %1695 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.21, %cos.25), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %1696 : int = aten::size(%q.21, %36), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1697 : Long(device=cpu) = prim::NumToTensor(%1696), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1698 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1697, %29), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1699 : int = aten::Int(%1698), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %x1.41 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.21, %36, %42, %1699, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1701 : int = aten::size(%q.21, %36), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1702 : Long(device=cpu) = prim::NumToTensor(%1701), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1703 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1702, %29), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1704 : int = aten::Int(%1703), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %x2.41 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.21, %36, %1704, %37, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1706 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.41), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1707 : Tensor[] = prim::ListConstruct(%1706, %x1.41), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1708 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1707, %45), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1709 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1708, %sin.25), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.21 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1695, %1709, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %1711 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.21, %cos.25), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %1712 : int = aten::size(%k.21, %36), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1713 : Long(device=cpu) = prim::NumToTensor(%1712), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1714 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1713, %29), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1715 : int = aten::Int(%1714), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %x1.43 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.21, %36, %42, %1715, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1717 : int = aten::size(%k.21, %36), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1718 : Long(device=cpu) = prim::NumToTensor(%1717), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1719 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1718, %29), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1720 : int = aten::Int(%1719), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %x2.43 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.21, %36, %1720, %37, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1722 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.43), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1723 : Tensor[] = prim::ListConstruct(%1722, %x1.43), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1724 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1723, %45), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1725 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1724, %sin.25), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.309 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1711, %1725, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %1727 : int = aten::size(%hidden_states.309, %42), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1728 : int = aten::size(%hidden_states.309, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.41 : Long(device=cpu) = prim::NumToTensor(%1728), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1730 : int = aten::size(%hidden_states.309, %38), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1731 : int = aten::size(%hidden_states.309, %36), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1732 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.309, %42, %42, %37, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1733 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1732, %44, %42, %37, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1734 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1733, %38), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1735 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1734, %36, %42, %37, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1736 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1735, %35, %42, %37, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1737 : int[] = prim::ListConstruct(%1727, %1728, %35, %1730, %1731), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %hidden_states.311 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1736, %1737, %46), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1739 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.41, %28), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1740 : int = aten::Int(%1739), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1741 : int[] = prim::ListConstruct(%1727, %1740, %1730, %1731), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %key.21 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.311, %1741), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1743 : int = aten::size(%hidden_states.313, %42), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1744 : int = aten::size(%hidden_states.313, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.43 : Long(device=cpu) = prim::NumToTensor(%1744), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1746 : int = aten::size(%hidden_states.313, %38), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1747 : int = aten::size(%hidden_states.313, %36), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1748 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.313, %42, %42, %37, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1749 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1748, %44, %42, %37, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1750 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1749, %38), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1751 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1750, %36, %42, %37, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1752 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1751, %35, %42, %37, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1753 : int[] = prim::ListConstruct(%1743, %1744, %35, %1746, %1747), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %hidden_states.315 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1752, %1753, %46), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1755 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.43, %28), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1756 : int = aten::Int(%1755), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1757 : int[] = prim::ListConstruct(%1743, %1756, %1746, %1747), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %value.21 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.315, %1757), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1759 : int = aten::size(%key.21, %38), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1760 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1761 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1760, %44, %42, %37, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1762 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1761, %38, %42, %37, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.27 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1762, %36, %42, %1759, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.41 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.21, %key.21, %value.21, %attention_mask.27, %27, %46, %26, %46), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %1765 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.41, %44, %38), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.43 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1765, %42), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %1767 : int[] = prim::ListConstruct(%1673, %1674, %45), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1768 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.43, %1767), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.81 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%1768, %42), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.193 : Tensor = prim::GetAttr[name="weight"](%o_proj.21)
  %hidden_states.317 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.81, %weight.193, %41), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn/__module.model.layers.10.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.319 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1668, %hidden_states.317, %44), scope: __module.model/__module.model.layers.10 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.195 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.21)
  %hidden_states.321 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.319, %33, %46, %46, %41), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %1775 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.321, %38), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1776 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm
  %variance.43 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1775, %1776, %25, %41), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1778 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.43, %24, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %1779 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1778), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.323 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.321, %1779), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.325 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.323, %33, %46, %46, %41), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.83 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.195, %hidden_states.325), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %1783 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.83, %hidden_states.321)
  %1784 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1785 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1783)
  %down_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_131.Linear = prim::GetAttr[name="down_proj"](%mlp.21)
  %up_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_130.Linear = prim::GetAttr[name="up_proj"](%mlp.21)
  %gate_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_129.Linear = prim::GetAttr[name="gate_proj"](%mlp.21)
  %weight.197 : Tensor = prim::GetAttr[name="weight"](%gate_proj.21)
  %input.85 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%1784, %weight.197, %41), scope: __module.model/__module.model.layers.10/__module.model.layers.10.mlp/__module.model.layers.10.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1791 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.85), scope: __module.model/__module.model.layers.10/__module.model.layers.10.mlp/__module.model.layers.10.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.199 : Tensor = prim::GetAttr[name="weight"](%up_proj.21)
  %1793 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%1784, %weight.199, %41), scope: __module.model/__module.model.layers.10/__module.model.layers.10.mlp/__module.model.layers.10.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.87 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%1791, %1793), scope: __module.model/__module.model.layers.10/__module.model.layers.10.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.201 : Tensor = prim::GetAttr[name="weight"](%down_proj.21)
  %hidden_states.327 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.87, %weight.201, %41), scope: __module.model/__module.model.layers.10/__module.model.layers.10.mlp/__module.model.layers.10.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.329 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1785, %hidden_states.327, %44), scope: __module.model/__module.model.layers.10 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.23 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_146.Qwen2MLP = prim::GetAttr[name="mlp"](%_11)
  %post_attention_layernorm.23 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_148.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_11)
  %self_attn.23 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_141.Qwen2Attention = prim::GetAttr[name="self_attn"](%_11)
  %input_layernorm.23 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_147.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_11)
  %weight.203 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.23)
  %hidden_states.331 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.329, %33, %46, %46, %41), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %1804 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.331, %38), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1805 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm
  %variance.45 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1804, %1805, %25, %41), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1807 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.45, %24, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %1808 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1807), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.333 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.331, %1808), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.335 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.333, %33, %46, %46, %41), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.337 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.203, %hidden_states.335), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %1812 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.337, %hidden_states.331)
  %1813 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1814 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1812)
  %o_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_140.Linear = prim::GetAttr[name="o_proj"](%self_attn.23)
  %v_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_139.Linear = prim::GetAttr[name="v_proj"](%self_attn.23)
  %k_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_138.Linear = prim::GetAttr[name="k_proj"](%self_attn.23)
  %q_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_137.Linear = prim::GetAttr[name="q_proj"](%self_attn.23)
  %1819 : int = aten::size(%1813, %42), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %1820 : int = aten::size(%1813, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.67 : Tensor = prim::GetAttr[name="bias"](%q_proj.23)
  %weight.205 : Tensor = prim::GetAttr[name="weight"](%q_proj.23)
  %1823 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1813, %weight.205, %bias.67), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn/__module.model.layers.11.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1824 : int[] = prim::ListConstruct(%1819, %1820, %45, %30), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1825 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%1823, %1824), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.23 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1825, %44, %38), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.69 : Tensor = prim::GetAttr[name="bias"](%k_proj.23)
  %weight.207 : Tensor = prim::GetAttr[name="weight"](%k_proj.23)
  %1829 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1813, %weight.207, %bias.69), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn/__module.model.layers.11.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1830 : int[] = prim::ListConstruct(%1819, %1820, %45, %30), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1831 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1829, %1830), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.23 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1831, %44, %38), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.71 : Tensor = prim::GetAttr[name="bias"](%v_proj.23)
  %weight.209 : Tensor = prim::GetAttr[name="weight"](%v_proj.23)
  %1835 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1813, %weight.209, %bias.71), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn/__module.model.layers.11.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1836 : int[] = prim::ListConstruct(%1819, %1820, %45, %30), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1837 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1835, %1836), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.343 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1837, %44, %38), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.27 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.27 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %1841 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.23, %cos.27), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %1842 : int = aten::size(%q.23, %36), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1843 : Long(device=cpu) = prim::NumToTensor(%1842), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1844 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1843, %29), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1845 : int = aten::Int(%1844), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %x1.45 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.23, %36, %42, %1845, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1847 : int = aten::size(%q.23, %36), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1848 : Long(device=cpu) = prim::NumToTensor(%1847), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1849 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1848, %29), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1850 : int = aten::Int(%1849), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %x2.45 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.23, %36, %1850, %37, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1852 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.45), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1853 : Tensor[] = prim::ListConstruct(%1852, %x1.45), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1854 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1853, %45), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1855 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1854, %sin.27), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.23 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1841, %1855, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %1857 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.23, %cos.27), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %1858 : int = aten::size(%k.23, %36), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1859 : Long(device=cpu) = prim::NumToTensor(%1858), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1860 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1859, %29), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1861 : int = aten::Int(%1860), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %x1.47 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.23, %36, %42, %1861, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1863 : int = aten::size(%k.23, %36), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1864 : Long(device=cpu) = prim::NumToTensor(%1863), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1865 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1864, %29), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1866 : int = aten::Int(%1865), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %x2.47 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.23, %36, %1866, %37, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1868 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1869 : Tensor[] = prim::ListConstruct(%1868, %x1.47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1870 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1869, %45), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1871 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1870, %sin.27), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.339 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1857, %1871, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %1873 : int = aten::size(%hidden_states.339, %42), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1874 : int = aten::size(%hidden_states.339, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.45 : Long(device=cpu) = prim::NumToTensor(%1874), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1876 : int = aten::size(%hidden_states.339, %38), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1877 : int = aten::size(%hidden_states.339, %36), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1878 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.339, %42, %42, %37, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1879 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1878, %44, %42, %37, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1880 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1879, %38), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1881 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1880, %36, %42, %37, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1882 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1881, %35, %42, %37, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1883 : int[] = prim::ListConstruct(%1873, %1874, %35, %1876, %1877), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %hidden_states.341 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1882, %1883, %46), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1885 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.45, %28), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1886 : int = aten::Int(%1885), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1887 : int[] = prim::ListConstruct(%1873, %1886, %1876, %1877), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %key.23 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.341, %1887), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1889 : int = aten::size(%hidden_states.343, %42), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1890 : int = aten::size(%hidden_states.343, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.47 : Long(device=cpu) = prim::NumToTensor(%1890), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1892 : int = aten::size(%hidden_states.343, %38), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1893 : int = aten::size(%hidden_states.343, %36), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1894 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.343, %42, %42, %37, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1895 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1894, %44, %42, %37, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1896 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1895, %38), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1897 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1896, %36, %42, %37, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1898 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1897, %35, %42, %37, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1899 : int[] = prim::ListConstruct(%1889, %1890, %35, %1892, %1893), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %hidden_states.345 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1898, %1899, %46), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1901 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.47, %28), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1902 : int = aten::Int(%1901), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1903 : int[] = prim::ListConstruct(%1889, %1902, %1892, %1893), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %value.23 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.345, %1903), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1905 : int = aten::size(%key.23, %38), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1906 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1907 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1906, %44, %42, %37, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1908 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1907, %38, %42, %37, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.29 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1908, %36, %42, %1905, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.45 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.23, %key.23, %value.23, %attention_mask.29, %27, %46, %26, %46), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %1911 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.45, %44, %38), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.47 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1911, %42), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %1913 : int[] = prim::ListConstruct(%1819, %1820, %45), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1914 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.47, %1913), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.89 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%1914, %42), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.211 : Tensor = prim::GetAttr[name="weight"](%o_proj.23)
  %hidden_states.347 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.89, %weight.211, %41), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn/__module.model.layers.11.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.349 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1814, %hidden_states.347, %44), scope: __module.model/__module.model.layers.11 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.213 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.23)
  %hidden_states.351 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.349, %33, %46, %46, %41), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %1921 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.351, %38), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1922 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm
  %variance.47 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1921, %1922, %25, %41), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1924 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.47, %24, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %1925 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1924), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.353 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.351, %1925), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.355 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.353, %33, %46, %46, %41), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.91 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.213, %hidden_states.355), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %1929 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.91, %hidden_states.351)
  %1930 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1931 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1929)
  %down_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_144.Linear = prim::GetAttr[name="down_proj"](%mlp.23)
  %up_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_143.Linear = prim::GetAttr[name="up_proj"](%mlp.23)
  %gate_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_142.Linear = prim::GetAttr[name="gate_proj"](%mlp.23)
  %weight.215 : Tensor = prim::GetAttr[name="weight"](%gate_proj.23)
  %input.93 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%1930, %weight.215, %41), scope: __module.model/__module.model.layers.11/__module.model.layers.11.mlp/__module.model.layers.11.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1937 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.93), scope: __module.model/__module.model.layers.11/__module.model.layers.11.mlp/__module.model.layers.11.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.217 : Tensor = prim::GetAttr[name="weight"](%up_proj.23)
  %1939 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%1930, %weight.217, %41), scope: __module.model/__module.model.layers.11/__module.model.layers.11.mlp/__module.model.layers.11.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.95 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%1937, %1939), scope: __module.model/__module.model.layers.11/__module.model.layers.11.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.219 : Tensor = prim::GetAttr[name="weight"](%down_proj.23)
  %hidden_states.357 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.95, %weight.219, %41), scope: __module.model/__module.model.layers.11/__module.model.layers.11.mlp/__module.model.layers.11.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.359 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1931, %hidden_states.357, %44), scope: __module.model/__module.model.layers.11 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.25 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_159.Qwen2MLP = prim::GetAttr[name="mlp"](%_12)
  %post_attention_layernorm.25 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_161.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_12)
  %self_attn.25 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_154.Qwen2Attention = prim::GetAttr[name="self_attn"](%_12)
  %input_layernorm.25 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_160.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_12)
  %weight.221 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.25)
  %hidden_states.361 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.359, %33, %46, %46, %41), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %1950 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.361, %38), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1951 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm
  %variance.49 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1950, %1951, %25, %41), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %1953 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.49, %24, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %1954 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1953), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.363 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.361, %1954), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.365 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.363, %33, %46, %46, %41), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.367 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.221, %hidden_states.365), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %1958 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.367, %hidden_states.361)
  %1959 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1960 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1958)
  %o_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_153.Linear = prim::GetAttr[name="o_proj"](%self_attn.25)
  %v_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_152.Linear = prim::GetAttr[name="v_proj"](%self_attn.25)
  %k_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_151.Linear = prim::GetAttr[name="k_proj"](%self_attn.25)
  %q_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_150.Linear = prim::GetAttr[name="q_proj"](%self_attn.25)
  %1965 : int = aten::size(%1959, %42), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %1966 : int = aten::size(%1959, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.73 : Tensor = prim::GetAttr[name="bias"](%q_proj.25)
  %weight.223 : Tensor = prim::GetAttr[name="weight"](%q_proj.25)
  %1969 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1959, %weight.223, %bias.73), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn/__module.model.layers.12.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1970 : int[] = prim::ListConstruct(%1965, %1966, %45, %30), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %1971 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%1969, %1970), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.25 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1971, %44, %38), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.75 : Tensor = prim::GetAttr[name="bias"](%k_proj.25)
  %weight.225 : Tensor = prim::GetAttr[name="weight"](%k_proj.25)
  %1975 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1959, %weight.225, %bias.75), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn/__module.model.layers.12.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1976 : int[] = prim::ListConstruct(%1965, %1966, %45, %30), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %1977 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1975, %1976), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.25 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1977, %44, %38), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.77 : Tensor = prim::GetAttr[name="bias"](%v_proj.25)
  %weight.227 : Tensor = prim::GetAttr[name="weight"](%v_proj.25)
  %1981 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1959, %weight.227, %bias.77), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn/__module.model.layers.12.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %1982 : int[] = prim::ListConstruct(%1965, %1966, %45, %30), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %1983 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1981, %1982), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.373 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1983, %44, %38), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.29 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.29 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %1987 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.25, %cos.29), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %1988 : int = aten::size(%q.25, %36), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1989 : Long(device=cpu) = prim::NumToTensor(%1988), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %1990 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1989, %29), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1991 : int = aten::Int(%1990), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %x1.49 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.25, %36, %42, %1991, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %1993 : int = aten::size(%q.25, %36), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1994 : Long(device=cpu) = prim::NumToTensor(%1993), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %1995 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1994, %29), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1996 : int = aten::Int(%1995), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %x2.49 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.25, %36, %1996, %37, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %1998 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.49), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %1999 : Tensor[] = prim::ListConstruct(%1998, %x1.49), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2000 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1999, %45), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2001 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2000, %sin.29), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.25 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1987, %2001, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %2003 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.25, %cos.29), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %2004 : int = aten::size(%k.25, %36), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2005 : Long(device=cpu) = prim::NumToTensor(%2004), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2006 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2005, %29), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2007 : int = aten::Int(%2006), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %x1.51 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.25, %36, %42, %2007, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2009 : int = aten::size(%k.25, %36), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2010 : Long(device=cpu) = prim::NumToTensor(%2009), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2011 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2010, %29), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2012 : int = aten::Int(%2011), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %x2.51 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.25, %36, %2012, %37, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2014 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.51), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2015 : Tensor[] = prim::ListConstruct(%2014, %x1.51), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2016 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2015, %45), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2017 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2016, %sin.29), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.369 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2003, %2017, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %2019 : int = aten::size(%hidden_states.369, %42), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2020 : int = aten::size(%hidden_states.369, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.49 : Long(device=cpu) = prim::NumToTensor(%2020), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2022 : int = aten::size(%hidden_states.369, %38), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2023 : int = aten::size(%hidden_states.369, %36), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2024 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.369, %42, %42, %37, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2025 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2024, %44, %42, %37, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2026 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2025, %38), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2027 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2026, %36, %42, %37, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2028 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2027, %35, %42, %37, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2029 : int[] = prim::ListConstruct(%2019, %2020, %35, %2022, %2023), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %hidden_states.371 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2028, %2029, %46), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2031 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.49, %28), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2032 : int = aten::Int(%2031), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2033 : int[] = prim::ListConstruct(%2019, %2032, %2022, %2023), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %key.25 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.371, %2033), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2035 : int = aten::size(%hidden_states.373, %42), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2036 : int = aten::size(%hidden_states.373, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.51 : Long(device=cpu) = prim::NumToTensor(%2036), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2038 : int = aten::size(%hidden_states.373, %38), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2039 : int = aten::size(%hidden_states.373, %36), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2040 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.373, %42, %42, %37, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2041 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2040, %44, %42, %37, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2042 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2041, %38), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2043 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2042, %36, %42, %37, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2044 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2043, %35, %42, %37, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2045 : int[] = prim::ListConstruct(%2035, %2036, %35, %2038, %2039), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %hidden_states.375 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2044, %2045, %46), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2047 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.51, %28), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2048 : int = aten::Int(%2047), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2049 : int[] = prim::ListConstruct(%2035, %2048, %2038, %2039), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %value.25 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.375, %2049), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2051 : int = aten::size(%key.25, %38), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2052 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2053 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2052, %44, %42, %37, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2054 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2053, %38, %42, %37, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.31 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2054, %36, %42, %2051, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.49 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.25, %key.25, %value.25, %attention_mask.31, %27, %46, %26, %46), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %2057 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.49, %44, %38), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.51 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2057, %42), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %2059 : int[] = prim::ListConstruct(%1965, %1966, %45), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2060 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.51, %2059), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.97 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%2060, %42), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.229 : Tensor = prim::GetAttr[name="weight"](%o_proj.25)
  %hidden_states.377 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.97, %weight.229, %41), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn/__module.model.layers.12.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.379 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1960, %hidden_states.377, %44), scope: __module.model/__module.model.layers.12 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.231 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.25)
  %hidden_states.381 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.379, %33, %46, %46, %41), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %2067 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.381, %38), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2068 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm
  %variance.51 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2067, %2068, %25, %41), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2070 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.51, %24, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %2071 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2070), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.383 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.381, %2071), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.385 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.383, %33, %46, %46, %41), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.99 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.231, %hidden_states.385), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %2075 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.99, %hidden_states.381)
  %2076 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2077 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2075)
  %down_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_157.Linear = prim::GetAttr[name="down_proj"](%mlp.25)
  %up_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_156.Linear = prim::GetAttr[name="up_proj"](%mlp.25)
  %gate_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_155.Linear = prim::GetAttr[name="gate_proj"](%mlp.25)
  %weight.233 : Tensor = prim::GetAttr[name="weight"](%gate_proj.25)
  %input.101 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%2076, %weight.233, %41), scope: __module.model/__module.model.layers.12/__module.model.layers.12.mlp/__module.model.layers.12.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2083 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.101), scope: __module.model/__module.model.layers.12/__module.model.layers.12.mlp/__module.model.layers.12.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.235 : Tensor = prim::GetAttr[name="weight"](%up_proj.25)
  %2085 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%2076, %weight.235, %41), scope: __module.model/__module.model.layers.12/__module.model.layers.12.mlp/__module.model.layers.12.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.103 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%2083, %2085), scope: __module.model/__module.model.layers.12/__module.model.layers.12.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.237 : Tensor = prim::GetAttr[name="weight"](%down_proj.25)
  %hidden_states.387 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.103, %weight.237, %41), scope: __module.model/__module.model.layers.12/__module.model.layers.12.mlp/__module.model.layers.12.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.389 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2077, %hidden_states.387, %44), scope: __module.model/__module.model.layers.12 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.27 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_172.Qwen2MLP = prim::GetAttr[name="mlp"](%_13)
  %post_attention_layernorm.27 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_174.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_13)
  %self_attn.27 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_167.Qwen2Attention = prim::GetAttr[name="self_attn"](%_13)
  %input_layernorm.27 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_173.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_13)
  %weight.239 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.27)
  %hidden_states.391 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.389, %33, %46, %46, %41), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %2096 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.391, %38), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2097 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm
  %variance.53 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2096, %2097, %25, %41), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2099 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.53, %24, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %2100 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2099), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.393 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.391, %2100), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.395 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.393, %33, %46, %46, %41), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.397 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.239, %hidden_states.395), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %2104 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.397, %hidden_states.391)
  %2105 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2106 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2104)
  %o_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_166.Linear = prim::GetAttr[name="o_proj"](%self_attn.27)
  %v_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_165.Linear = prim::GetAttr[name="v_proj"](%self_attn.27)
  %k_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_164.Linear = prim::GetAttr[name="k_proj"](%self_attn.27)
  %q_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_163.Linear = prim::GetAttr[name="q_proj"](%self_attn.27)
  %2111 : int = aten::size(%2105, %42), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %2112 : int = aten::size(%2105, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.79 : Tensor = prim::GetAttr[name="bias"](%q_proj.27)
  %weight.241 : Tensor = prim::GetAttr[name="weight"](%q_proj.27)
  %2115 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2105, %weight.241, %bias.79), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn/__module.model.layers.13.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2116 : int[] = prim::ListConstruct(%2111, %2112, %45, %30), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2117 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%2115, %2116), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.27 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2117, %44, %38), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.81 : Tensor = prim::GetAttr[name="bias"](%k_proj.27)
  %weight.243 : Tensor = prim::GetAttr[name="weight"](%k_proj.27)
  %2121 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2105, %weight.243, %bias.81), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn/__module.model.layers.13.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2122 : int[] = prim::ListConstruct(%2111, %2112, %45, %30), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2123 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2121, %2122), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.27 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2123, %44, %38), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.83 : Tensor = prim::GetAttr[name="bias"](%v_proj.27)
  %weight.245 : Tensor = prim::GetAttr[name="weight"](%v_proj.27)
  %2127 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2105, %weight.245, %bias.83), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn/__module.model.layers.13.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2128 : int[] = prim::ListConstruct(%2111, %2112, %45, %30), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2129 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2127, %2128), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.403 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2129, %44, %38), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.31 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.31 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %2133 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.27, %cos.31), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %2134 : int = aten::size(%q.27, %36), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2135 : Long(device=cpu) = prim::NumToTensor(%2134), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2136 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2135, %29), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2137 : int = aten::Int(%2136), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %x1.53 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.27, %36, %42, %2137, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2139 : int = aten::size(%q.27, %36), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2140 : Long(device=cpu) = prim::NumToTensor(%2139), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2141 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2140, %29), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2142 : int = aten::Int(%2141), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %x2.53 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.27, %36, %2142, %37, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2144 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.53), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2145 : Tensor[] = prim::ListConstruct(%2144, %x1.53), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2146 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2145, %45), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2147 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2146, %sin.31), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.27 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2133, %2147, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %2149 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.27, %cos.31), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %2150 : int = aten::size(%k.27, %36), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2151 : Long(device=cpu) = prim::NumToTensor(%2150), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2152 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2151, %29), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2153 : int = aten::Int(%2152), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %x1.55 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.27, %36, %42, %2153, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2155 : int = aten::size(%k.27, %36), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2156 : Long(device=cpu) = prim::NumToTensor(%2155), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2157 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2156, %29), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2158 : int = aten::Int(%2157), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %x2.55 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.27, %36, %2158, %37, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2160 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.55), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2161 : Tensor[] = prim::ListConstruct(%2160, %x1.55), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2162 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2161, %45), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2163 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2162, %sin.31), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.399 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2149, %2163, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %2165 : int = aten::size(%hidden_states.399, %42), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2166 : int = aten::size(%hidden_states.399, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.53 : Long(device=cpu) = prim::NumToTensor(%2166), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2168 : int = aten::size(%hidden_states.399, %38), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2169 : int = aten::size(%hidden_states.399, %36), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2170 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.399, %42, %42, %37, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2171 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2170, %44, %42, %37, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2172 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2171, %38), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2173 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2172, %36, %42, %37, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2174 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2173, %35, %42, %37, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2175 : int[] = prim::ListConstruct(%2165, %2166, %35, %2168, %2169), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %hidden_states.401 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2174, %2175, %46), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2177 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.53, %28), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2178 : int = aten::Int(%2177), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2179 : int[] = prim::ListConstruct(%2165, %2178, %2168, %2169), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %key.27 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.401, %2179), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2181 : int = aten::size(%hidden_states.403, %42), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2182 : int = aten::size(%hidden_states.403, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.55 : Long(device=cpu) = prim::NumToTensor(%2182), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2184 : int = aten::size(%hidden_states.403, %38), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2185 : int = aten::size(%hidden_states.403, %36), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2186 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.403, %42, %42, %37, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2187 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2186, %44, %42, %37, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2188 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2187, %38), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2189 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2188, %36, %42, %37, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2190 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2189, %35, %42, %37, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2191 : int[] = prim::ListConstruct(%2181, %2182, %35, %2184, %2185), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %hidden_states.405 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2190, %2191, %46), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2193 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.55, %28), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2194 : int = aten::Int(%2193), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2195 : int[] = prim::ListConstruct(%2181, %2194, %2184, %2185), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %value.27 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.405, %2195), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2197 : int = aten::size(%key.27, %38), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2198 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2199 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2198, %44, %42, %37, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2200 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2199, %38, %42, %37, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.33 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2200, %36, %42, %2197, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.53 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.27, %key.27, %value.27, %attention_mask.33, %27, %46, %26, %46), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %2203 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.53, %44, %38), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.55 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2203, %42), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %2205 : int[] = prim::ListConstruct(%2111, %2112, %45), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2206 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.55, %2205), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.105 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%2206, %42), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.247 : Tensor = prim::GetAttr[name="weight"](%o_proj.27)
  %hidden_states.407 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.105, %weight.247, %41), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn/__module.model.layers.13.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.409 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2106, %hidden_states.407, %44), scope: __module.model/__module.model.layers.13 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.249 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.27)
  %hidden_states.411 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.409, %33, %46, %46, %41), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %2213 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.411, %38), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2214 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm
  %variance.55 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2213, %2214, %25, %41), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2216 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.55, %24, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %2217 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2216), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.413 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.411, %2217), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.415 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.413, %33, %46, %46, %41), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.107 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.249, %hidden_states.415), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %2221 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.107, %hidden_states.411)
  %2222 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2223 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2221)
  %down_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_170.Linear = prim::GetAttr[name="down_proj"](%mlp.27)
  %up_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_169.Linear = prim::GetAttr[name="up_proj"](%mlp.27)
  %gate_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_168.Linear = prim::GetAttr[name="gate_proj"](%mlp.27)
  %weight.251 : Tensor = prim::GetAttr[name="weight"](%gate_proj.27)
  %input.109 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%2222, %weight.251, %41), scope: __module.model/__module.model.layers.13/__module.model.layers.13.mlp/__module.model.layers.13.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2229 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.109), scope: __module.model/__module.model.layers.13/__module.model.layers.13.mlp/__module.model.layers.13.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.253 : Tensor = prim::GetAttr[name="weight"](%up_proj.27)
  %2231 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%2222, %weight.253, %41), scope: __module.model/__module.model.layers.13/__module.model.layers.13.mlp/__module.model.layers.13.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.111 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%2229, %2231), scope: __module.model/__module.model.layers.13/__module.model.layers.13.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.255 : Tensor = prim::GetAttr[name="weight"](%down_proj.27)
  %hidden_states.417 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.111, %weight.255, %41), scope: __module.model/__module.model.layers.13/__module.model.layers.13.mlp/__module.model.layers.13.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.419 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2223, %hidden_states.417, %44), scope: __module.model/__module.model.layers.13 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.29 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_185.Qwen2MLP = prim::GetAttr[name="mlp"](%_14)
  %post_attention_layernorm.29 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_187.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_14)
  %self_attn.29 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_180.Qwen2Attention = prim::GetAttr[name="self_attn"](%_14)
  %input_layernorm.29 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_186.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_14)
  %weight.257 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.29)
  %hidden_states.421 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.419, %33, %46, %46, %41), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %2242 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.421, %38), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2243 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm
  %variance.57 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2242, %2243, %25, %41), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2245 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.57, %24, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %2246 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2245), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.423 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.421, %2246), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.425 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.423, %33, %46, %46, %41), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.427 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.257, %hidden_states.425), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %2250 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.427, %hidden_states.421)
  %2251 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2252 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2250)
  %o_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_179.Linear = prim::GetAttr[name="o_proj"](%self_attn.29)
  %v_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_178.Linear = prim::GetAttr[name="v_proj"](%self_attn.29)
  %k_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_177.Linear = prim::GetAttr[name="k_proj"](%self_attn.29)
  %q_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_176.Linear = prim::GetAttr[name="q_proj"](%self_attn.29)
  %2257 : int = aten::size(%2251, %42), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %2258 : int = aten::size(%2251, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.85 : Tensor = prim::GetAttr[name="bias"](%q_proj.29)
  %weight.259 : Tensor = prim::GetAttr[name="weight"](%q_proj.29)
  %2261 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2251, %weight.259, %bias.85), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn/__module.model.layers.14.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2262 : int[] = prim::ListConstruct(%2257, %2258, %45, %30), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2263 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%2261, %2262), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.29 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2263, %44, %38), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.87 : Tensor = prim::GetAttr[name="bias"](%k_proj.29)
  %weight.261 : Tensor = prim::GetAttr[name="weight"](%k_proj.29)
  %2267 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2251, %weight.261, %bias.87), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn/__module.model.layers.14.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2268 : int[] = prim::ListConstruct(%2257, %2258, %45, %30), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2269 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2267, %2268), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.29 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2269, %44, %38), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.89 : Tensor = prim::GetAttr[name="bias"](%v_proj.29)
  %weight.263 : Tensor = prim::GetAttr[name="weight"](%v_proj.29)
  %2273 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2251, %weight.263, %bias.89), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn/__module.model.layers.14.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2274 : int[] = prim::ListConstruct(%2257, %2258, %45, %30), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2275 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2273, %2274), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.433 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2275, %44, %38), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.33 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.33 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %2279 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.29, %cos.33), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %2280 : int = aten::size(%q.29, %36), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2281 : Long(device=cpu) = prim::NumToTensor(%2280), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2282 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2281, %29), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2283 : int = aten::Int(%2282), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %x1.57 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.29, %36, %42, %2283, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2285 : int = aten::size(%q.29, %36), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2286 : Long(device=cpu) = prim::NumToTensor(%2285), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2287 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2286, %29), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2288 : int = aten::Int(%2287), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %x2.57 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.29, %36, %2288, %37, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2290 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.57), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2291 : Tensor[] = prim::ListConstruct(%2290, %x1.57), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2292 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2291, %45), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2293 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2292, %sin.33), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.29 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2279, %2293, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %2295 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.29, %cos.33), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %2296 : int = aten::size(%k.29, %36), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2297 : Long(device=cpu) = prim::NumToTensor(%2296), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2298 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2297, %29), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2299 : int = aten::Int(%2298), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %x1.59 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.29, %36, %42, %2299, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2301 : int = aten::size(%k.29, %36), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2302 : Long(device=cpu) = prim::NumToTensor(%2301), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2303 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2302, %29), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2304 : int = aten::Int(%2303), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %x2.59 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.29, %36, %2304, %37, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2306 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.59), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2307 : Tensor[] = prim::ListConstruct(%2306, %x1.59), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2308 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2307, %45), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2309 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2308, %sin.33), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.429 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2295, %2309, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %2311 : int = aten::size(%hidden_states.429, %42), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2312 : int = aten::size(%hidden_states.429, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.57 : Long(device=cpu) = prim::NumToTensor(%2312), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2314 : int = aten::size(%hidden_states.429, %38), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2315 : int = aten::size(%hidden_states.429, %36), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2316 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.429, %42, %42, %37, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2317 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2316, %44, %42, %37, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2318 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2317, %38), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2319 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2318, %36, %42, %37, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2320 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2319, %35, %42, %37, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2321 : int[] = prim::ListConstruct(%2311, %2312, %35, %2314, %2315), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %hidden_states.431 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2320, %2321, %46), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2323 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.57, %28), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2324 : int = aten::Int(%2323), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2325 : int[] = prim::ListConstruct(%2311, %2324, %2314, %2315), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %key.29 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.431, %2325), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2327 : int = aten::size(%hidden_states.433, %42), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2328 : int = aten::size(%hidden_states.433, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.59 : Long(device=cpu) = prim::NumToTensor(%2328), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2330 : int = aten::size(%hidden_states.433, %38), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2331 : int = aten::size(%hidden_states.433, %36), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2332 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.433, %42, %42, %37, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2333 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2332, %44, %42, %37, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2334 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2333, %38), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2335 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2334, %36, %42, %37, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2336 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2335, %35, %42, %37, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2337 : int[] = prim::ListConstruct(%2327, %2328, %35, %2330, %2331), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %hidden_states.435 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2336, %2337, %46), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2339 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.59, %28), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2340 : int = aten::Int(%2339), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2341 : int[] = prim::ListConstruct(%2327, %2340, %2330, %2331), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %value.29 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.435, %2341), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2343 : int = aten::size(%key.29, %38), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2344 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2345 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2344, %44, %42, %37, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2346 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2345, %38, %42, %37, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.35 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2346, %36, %42, %2343, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.57 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.29, %key.29, %value.29, %attention_mask.35, %27, %46, %26, %46), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %2349 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.57, %44, %38), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.59 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2349, %42), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %2351 : int[] = prim::ListConstruct(%2257, %2258, %45), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2352 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.59, %2351), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.113 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%2352, %42), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.265 : Tensor = prim::GetAttr[name="weight"](%o_proj.29)
  %hidden_states.437 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.113, %weight.265, %41), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn/__module.model.layers.14.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.439 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2252, %hidden_states.437, %44), scope: __module.model/__module.model.layers.14 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.267 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.29)
  %hidden_states.441 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.439, %33, %46, %46, %41), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %2359 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.441, %38), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2360 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm
  %variance.59 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2359, %2360, %25, %41), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2362 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.59, %24, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %2363 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2362), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.443 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.441, %2363), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.445 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.443, %33, %46, %46, %41), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.115 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.267, %hidden_states.445), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %2367 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.115, %hidden_states.441)
  %2368 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2369 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2367)
  %down_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_183.Linear = prim::GetAttr[name="down_proj"](%mlp.29)
  %up_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_182.Linear = prim::GetAttr[name="up_proj"](%mlp.29)
  %gate_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_181.Linear = prim::GetAttr[name="gate_proj"](%mlp.29)
  %weight.269 : Tensor = prim::GetAttr[name="weight"](%gate_proj.29)
  %input.117 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%2368, %weight.269, %41), scope: __module.model/__module.model.layers.14/__module.model.layers.14.mlp/__module.model.layers.14.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2375 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.117), scope: __module.model/__module.model.layers.14/__module.model.layers.14.mlp/__module.model.layers.14.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.271 : Tensor = prim::GetAttr[name="weight"](%up_proj.29)
  %2377 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%2368, %weight.271, %41), scope: __module.model/__module.model.layers.14/__module.model.layers.14.mlp/__module.model.layers.14.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.119 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%2375, %2377), scope: __module.model/__module.model.layers.14/__module.model.layers.14.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.273 : Tensor = prim::GetAttr[name="weight"](%down_proj.29)
  %hidden_states.447 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.119, %weight.273, %41), scope: __module.model/__module.model.layers.14/__module.model.layers.14.mlp/__module.model.layers.14.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.449 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2369, %hidden_states.447, %44), scope: __module.model/__module.model.layers.14 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.31 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_198.Qwen2MLP = prim::GetAttr[name="mlp"](%_15)
  %post_attention_layernorm.31 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_200.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_15)
  %self_attn.31 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_193.Qwen2Attention = prim::GetAttr[name="self_attn"](%_15)
  %input_layernorm.31 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_199.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_15)
  %weight.275 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.31)
  %hidden_states.451 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.449, %33, %46, %46, %41), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %2388 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.451, %38), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2389 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm
  %variance.61 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2388, %2389, %25, %41), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2391 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.61, %24, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %2392 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2391), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.453 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.451, %2392), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.455 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.453, %33, %46, %46, %41), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.457 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.275, %hidden_states.455), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %2396 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.457, %hidden_states.451)
  %2397 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2398 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2396)
  %o_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_192.Linear = prim::GetAttr[name="o_proj"](%self_attn.31)
  %v_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_191.Linear = prim::GetAttr[name="v_proj"](%self_attn.31)
  %k_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_190.Linear = prim::GetAttr[name="k_proj"](%self_attn.31)
  %q_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_189.Linear = prim::GetAttr[name="q_proj"](%self_attn.31)
  %2403 : int = aten::size(%2397, %42), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %2404 : int = aten::size(%2397, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.91 : Tensor = prim::GetAttr[name="bias"](%q_proj.31)
  %weight.277 : Tensor = prim::GetAttr[name="weight"](%q_proj.31)
  %2407 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2397, %weight.277, %bias.91), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn/__module.model.layers.15.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2408 : int[] = prim::ListConstruct(%2403, %2404, %45, %30), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2409 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%2407, %2408), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.31 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2409, %44, %38), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.93 : Tensor = prim::GetAttr[name="bias"](%k_proj.31)
  %weight.279 : Tensor = prim::GetAttr[name="weight"](%k_proj.31)
  %2413 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2397, %weight.279, %bias.93), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn/__module.model.layers.15.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2414 : int[] = prim::ListConstruct(%2403, %2404, %45, %30), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2415 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2413, %2414), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.31 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2415, %44, %38), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.95 : Tensor = prim::GetAttr[name="bias"](%v_proj.31)
  %weight.281 : Tensor = prim::GetAttr[name="weight"](%v_proj.31)
  %2419 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2397, %weight.281, %bias.95), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn/__module.model.layers.15.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2420 : int[] = prim::ListConstruct(%2403, %2404, %45, %30), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2421 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2419, %2420), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.463 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2421, %44, %38), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.35 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.35 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %2425 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.31, %cos.35), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %2426 : int = aten::size(%q.31, %36), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2427 : Long(device=cpu) = prim::NumToTensor(%2426), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2428 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2427, %29), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2429 : int = aten::Int(%2428), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %x1.61 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.31, %36, %42, %2429, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2431 : int = aten::size(%q.31, %36), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2432 : Long(device=cpu) = prim::NumToTensor(%2431), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2433 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2432, %29), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2434 : int = aten::Int(%2433), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %x2.61 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.31, %36, %2434, %37, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2436 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.61), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2437 : Tensor[] = prim::ListConstruct(%2436, %x1.61), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2438 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2437, %45), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2439 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2438, %sin.35), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.31 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2425, %2439, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %2441 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.31, %cos.35), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %2442 : int = aten::size(%k.31, %36), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2443 : Long(device=cpu) = prim::NumToTensor(%2442), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2444 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2443, %29), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2445 : int = aten::Int(%2444), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %x1.63 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.31, %36, %42, %2445, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2447 : int = aten::size(%k.31, %36), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2448 : Long(device=cpu) = prim::NumToTensor(%2447), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2449 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2448, %29), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2450 : int = aten::Int(%2449), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %x2.63 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.31, %36, %2450, %37, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2452 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.63), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2453 : Tensor[] = prim::ListConstruct(%2452, %x1.63), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2454 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2453, %45), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2455 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2454, %sin.35), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.459 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2441, %2455, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %2457 : int = aten::size(%hidden_states.459, %42), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2458 : int = aten::size(%hidden_states.459, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.61 : Long(device=cpu) = prim::NumToTensor(%2458), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2460 : int = aten::size(%hidden_states.459, %38), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2461 : int = aten::size(%hidden_states.459, %36), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2462 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.459, %42, %42, %37, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2463 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2462, %44, %42, %37, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2464 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2463, %38), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2465 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2464, %36, %42, %37, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2466 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2465, %35, %42, %37, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2467 : int[] = prim::ListConstruct(%2457, %2458, %35, %2460, %2461), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %hidden_states.461 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2466, %2467, %46), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2469 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.61, %28), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2470 : int = aten::Int(%2469), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2471 : int[] = prim::ListConstruct(%2457, %2470, %2460, %2461), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %key.31 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.461, %2471), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2473 : int = aten::size(%hidden_states.463, %42), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2474 : int = aten::size(%hidden_states.463, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.63 : Long(device=cpu) = prim::NumToTensor(%2474), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2476 : int = aten::size(%hidden_states.463, %38), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2477 : int = aten::size(%hidden_states.463, %36), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2478 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.463, %42, %42, %37, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2479 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2478, %44, %42, %37, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2480 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2479, %38), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2481 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2480, %36, %42, %37, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2482 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2481, %35, %42, %37, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2483 : int[] = prim::ListConstruct(%2473, %2474, %35, %2476, %2477), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %hidden_states.465 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2482, %2483, %46), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2485 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.63, %28), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2486 : int = aten::Int(%2485), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2487 : int[] = prim::ListConstruct(%2473, %2486, %2476, %2477), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %value.31 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.465, %2487), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2489 : int = aten::size(%key.31, %38), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2490 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2491 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2490, %44, %42, %37, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2492 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2491, %38, %42, %37, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.37 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2492, %36, %42, %2489, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.61 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.31, %key.31, %value.31, %attention_mask.37, %27, %46, %26, %46), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %2495 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.61, %44, %38), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.63 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2495, %42), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %2497 : int[] = prim::ListConstruct(%2403, %2404, %45), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2498 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.63, %2497), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.121 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%2498, %42), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.283 : Tensor = prim::GetAttr[name="weight"](%o_proj.31)
  %hidden_states.467 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.121, %weight.283, %41), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn/__module.model.layers.15.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.469 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2398, %hidden_states.467, %44), scope: __module.model/__module.model.layers.15 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.285 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.31)
  %hidden_states.471 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.469, %33, %46, %46, %41), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %2505 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.471, %38), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2506 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm
  %variance.63 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2505, %2506, %25, %41), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2508 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.63, %24, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %2509 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2508), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.473 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.471, %2509), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.475 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.473, %33, %46, %46, %41), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.123 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.285, %hidden_states.475), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %2513 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.123, %hidden_states.471)
  %2514 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2515 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2513)
  %down_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_196.Linear = prim::GetAttr[name="down_proj"](%mlp.31)
  %up_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_195.Linear = prim::GetAttr[name="up_proj"](%mlp.31)
  %gate_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_194.Linear = prim::GetAttr[name="gate_proj"](%mlp.31)
  %weight.287 : Tensor = prim::GetAttr[name="weight"](%gate_proj.31)
  %input.125 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%2514, %weight.287, %41), scope: __module.model/__module.model.layers.15/__module.model.layers.15.mlp/__module.model.layers.15.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2521 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.125), scope: __module.model/__module.model.layers.15/__module.model.layers.15.mlp/__module.model.layers.15.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.289 : Tensor = prim::GetAttr[name="weight"](%up_proj.31)
  %2523 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%2514, %weight.289, %41), scope: __module.model/__module.model.layers.15/__module.model.layers.15.mlp/__module.model.layers.15.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.127 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%2521, %2523), scope: __module.model/__module.model.layers.15/__module.model.layers.15.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.291 : Tensor = prim::GetAttr[name="weight"](%down_proj.31)
  %hidden_states.477 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.127, %weight.291, %41), scope: __module.model/__module.model.layers.15/__module.model.layers.15.mlp/__module.model.layers.15.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.479 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2515, %hidden_states.477, %44), scope: __module.model/__module.model.layers.15 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.33 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_211.Qwen2MLP = prim::GetAttr[name="mlp"](%_16)
  %post_attention_layernorm.33 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_213.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_16)
  %self_attn.33 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_206.Qwen2Attention = prim::GetAttr[name="self_attn"](%_16)
  %input_layernorm.33 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_212.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_16)
  %weight.293 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.33)
  %hidden_states.481 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.479, %33, %46, %46, %41), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %2534 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.481, %38), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2535 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm
  %variance.65 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2534, %2535, %25, %41), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2537 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.65, %24, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %2538 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2537), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.483 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.481, %2538), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.485 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.483, %33, %46, %46, %41), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.487 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.293, %hidden_states.485), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %2542 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.487, %hidden_states.481)
  %2543 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2544 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2542)
  %o_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_205.Linear = prim::GetAttr[name="o_proj"](%self_attn.33)
  %v_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_204.Linear = prim::GetAttr[name="v_proj"](%self_attn.33)
  %k_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_203.Linear = prim::GetAttr[name="k_proj"](%self_attn.33)
  %q_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_202.Linear = prim::GetAttr[name="q_proj"](%self_attn.33)
  %2549 : int = aten::size(%2543, %42), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %2550 : int = aten::size(%2543, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.97 : Tensor = prim::GetAttr[name="bias"](%q_proj.33)
  %weight.295 : Tensor = prim::GetAttr[name="weight"](%q_proj.33)
  %2553 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2543, %weight.295, %bias.97), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn/__module.model.layers.16.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2554 : int[] = prim::ListConstruct(%2549, %2550, %45, %30), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2555 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%2553, %2554), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.33 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2555, %44, %38), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.99 : Tensor = prim::GetAttr[name="bias"](%k_proj.33)
  %weight.297 : Tensor = prim::GetAttr[name="weight"](%k_proj.33)
  %2559 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2543, %weight.297, %bias.99), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn/__module.model.layers.16.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2560 : int[] = prim::ListConstruct(%2549, %2550, %45, %30), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2561 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2559, %2560), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.33 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2561, %44, %38), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.101 : Tensor = prim::GetAttr[name="bias"](%v_proj.33)
  %weight.299 : Tensor = prim::GetAttr[name="weight"](%v_proj.33)
  %2565 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2543, %weight.299, %bias.101), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn/__module.model.layers.16.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2566 : int[] = prim::ListConstruct(%2549, %2550, %45, %30), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2567 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2565, %2566), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.493 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2567, %44, %38), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.37 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.37 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %2571 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.33, %cos.37), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %2572 : int = aten::size(%q.33, %36), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2573 : Long(device=cpu) = prim::NumToTensor(%2572), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2574 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2573, %29), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2575 : int = aten::Int(%2574), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %x1.65 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.33, %36, %42, %2575, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2577 : int = aten::size(%q.33, %36), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2578 : Long(device=cpu) = prim::NumToTensor(%2577), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2579 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2578, %29), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2580 : int = aten::Int(%2579), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %x2.65 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.33, %36, %2580, %37, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2582 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.65), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2583 : Tensor[] = prim::ListConstruct(%2582, %x1.65), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2584 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2583, %45), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2585 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2584, %sin.37), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.33 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2571, %2585, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %2587 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.33, %cos.37), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %2588 : int = aten::size(%k.33, %36), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2589 : Long(device=cpu) = prim::NumToTensor(%2588), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2590 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2589, %29), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2591 : int = aten::Int(%2590), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %x1.67 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.33, %36, %42, %2591, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2593 : int = aten::size(%k.33, %36), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2594 : Long(device=cpu) = prim::NumToTensor(%2593), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2595 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2594, %29), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2596 : int = aten::Int(%2595), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %x2.67 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.33, %36, %2596, %37, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2598 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.67), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2599 : Tensor[] = prim::ListConstruct(%2598, %x1.67), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2600 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2599, %45), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2601 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2600, %sin.37), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.489 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2587, %2601, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %2603 : int = aten::size(%hidden_states.489, %42), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2604 : int = aten::size(%hidden_states.489, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.65 : Long(device=cpu) = prim::NumToTensor(%2604), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2606 : int = aten::size(%hidden_states.489, %38), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2607 : int = aten::size(%hidden_states.489, %36), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2608 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.489, %42, %42, %37, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2609 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2608, %44, %42, %37, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2610 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2609, %38), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2611 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2610, %36, %42, %37, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2612 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2611, %35, %42, %37, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2613 : int[] = prim::ListConstruct(%2603, %2604, %35, %2606, %2607), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %hidden_states.491 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2612, %2613, %46), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2615 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.65, %28), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2616 : int = aten::Int(%2615), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2617 : int[] = prim::ListConstruct(%2603, %2616, %2606, %2607), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %key.33 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.491, %2617), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2619 : int = aten::size(%hidden_states.493, %42), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2620 : int = aten::size(%hidden_states.493, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.67 : Long(device=cpu) = prim::NumToTensor(%2620), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2622 : int = aten::size(%hidden_states.493, %38), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2623 : int = aten::size(%hidden_states.493, %36), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2624 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.493, %42, %42, %37, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2625 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2624, %44, %42, %37, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2626 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2625, %38), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2627 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2626, %36, %42, %37, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2628 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2627, %35, %42, %37, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2629 : int[] = prim::ListConstruct(%2619, %2620, %35, %2622, %2623), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %hidden_states.495 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2628, %2629, %46), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2631 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.67, %28), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2632 : int = aten::Int(%2631), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2633 : int[] = prim::ListConstruct(%2619, %2632, %2622, %2623), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %value.33 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.495, %2633), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2635 : int = aten::size(%key.33, %38), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2636 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2637 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2636, %44, %42, %37, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2638 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2637, %38, %42, %37, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.39 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2638, %36, %42, %2635, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.65 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.33, %key.33, %value.33, %attention_mask.39, %27, %46, %26, %46), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %2641 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.65, %44, %38), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.67 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2641, %42), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %2643 : int[] = prim::ListConstruct(%2549, %2550, %45), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2644 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.67, %2643), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.129 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%2644, %42), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.301 : Tensor = prim::GetAttr[name="weight"](%o_proj.33)
  %hidden_states.497 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.129, %weight.301, %41), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn/__module.model.layers.16.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.499 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2544, %hidden_states.497, %44), scope: __module.model/__module.model.layers.16 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.303 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.33)
  %hidden_states.501 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.499, %33, %46, %46, %41), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %2651 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.501, %38), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2652 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm
  %variance.67 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2651, %2652, %25, %41), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2654 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.67, %24, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %2655 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2654), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.503 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.501, %2655), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.505 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.503, %33, %46, %46, %41), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.131 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.303, %hidden_states.505), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %2659 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.131, %hidden_states.501)
  %2660 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2661 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2659)
  %down_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_209.Linear = prim::GetAttr[name="down_proj"](%mlp.33)
  %up_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_208.Linear = prim::GetAttr[name="up_proj"](%mlp.33)
  %gate_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_207.Linear = prim::GetAttr[name="gate_proj"](%mlp.33)
  %weight.305 : Tensor = prim::GetAttr[name="weight"](%gate_proj.33)
  %input.133 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%2660, %weight.305, %41), scope: __module.model/__module.model.layers.16/__module.model.layers.16.mlp/__module.model.layers.16.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2667 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.133), scope: __module.model/__module.model.layers.16/__module.model.layers.16.mlp/__module.model.layers.16.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.307 : Tensor = prim::GetAttr[name="weight"](%up_proj.33)
  %2669 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%2660, %weight.307, %41), scope: __module.model/__module.model.layers.16/__module.model.layers.16.mlp/__module.model.layers.16.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.135 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%2667, %2669), scope: __module.model/__module.model.layers.16/__module.model.layers.16.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.309 : Tensor = prim::GetAttr[name="weight"](%down_proj.33)
  %hidden_states.507 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.135, %weight.309, %41), scope: __module.model/__module.model.layers.16/__module.model.layers.16.mlp/__module.model.layers.16.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.509 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2661, %hidden_states.507, %44), scope: __module.model/__module.model.layers.16 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.35 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_224.Qwen2MLP = prim::GetAttr[name="mlp"](%_17)
  %post_attention_layernorm.35 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_226.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_17)
  %self_attn.35 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_219.Qwen2Attention = prim::GetAttr[name="self_attn"](%_17)
  %input_layernorm.35 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_225.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_17)
  %weight.311 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.35)
  %hidden_states.511 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.509, %33, %46, %46, %41), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %2680 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.511, %38), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2681 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm
  %variance.69 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2680, %2681, %25, %41), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2683 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.69, %24, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %2684 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2683), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.513 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.511, %2684), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.515 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.513, %33, %46, %46, %41), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.517 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.311, %hidden_states.515), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %2688 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.517, %hidden_states.511)
  %2689 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2690 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2688)
  %o_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_218.Linear = prim::GetAttr[name="o_proj"](%self_attn.35)
  %v_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_217.Linear = prim::GetAttr[name="v_proj"](%self_attn.35)
  %k_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_216.Linear = prim::GetAttr[name="k_proj"](%self_attn.35)
  %q_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_215.Linear = prim::GetAttr[name="q_proj"](%self_attn.35)
  %2695 : int = aten::size(%2689, %42), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %2696 : int = aten::size(%2689, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.103 : Tensor = prim::GetAttr[name="bias"](%q_proj.35)
  %weight.313 : Tensor = prim::GetAttr[name="weight"](%q_proj.35)
  %2699 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2689, %weight.313, %bias.103), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn/__module.model.layers.17.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2700 : int[] = prim::ListConstruct(%2695, %2696, %45, %30), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2701 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%2699, %2700), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.35 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2701, %44, %38), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.105 : Tensor = prim::GetAttr[name="bias"](%k_proj.35)
  %weight.315 : Tensor = prim::GetAttr[name="weight"](%k_proj.35)
  %2705 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2689, %weight.315, %bias.105), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn/__module.model.layers.17.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2706 : int[] = prim::ListConstruct(%2695, %2696, %45, %30), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2707 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2705, %2706), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.35 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2707, %44, %38), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.107 : Tensor = prim::GetAttr[name="bias"](%v_proj.35)
  %weight.317 : Tensor = prim::GetAttr[name="weight"](%v_proj.35)
  %2711 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2689, %weight.317, %bias.107), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn/__module.model.layers.17.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2712 : int[] = prim::ListConstruct(%2695, %2696, %45, %30), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2713 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2711, %2712), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.523 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2713, %44, %38), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.39 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.39 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %2717 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.35, %cos.39), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %2718 : int = aten::size(%q.35, %36), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2719 : Long(device=cpu) = prim::NumToTensor(%2718), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2720 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2719, %29), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2721 : int = aten::Int(%2720), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %x1.69 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.35, %36, %42, %2721, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2723 : int = aten::size(%q.35, %36), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2724 : Long(device=cpu) = prim::NumToTensor(%2723), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2725 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2724, %29), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2726 : int = aten::Int(%2725), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %x2.69 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.35, %36, %2726, %37, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2728 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.69), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2729 : Tensor[] = prim::ListConstruct(%2728, %x1.69), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2730 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2729, %45), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2731 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2730, %sin.39), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.35 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2717, %2731, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %2733 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.35, %cos.39), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %2734 : int = aten::size(%k.35, %36), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2735 : Long(device=cpu) = prim::NumToTensor(%2734), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2736 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2735, %29), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2737 : int = aten::Int(%2736), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %x1.71 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.35, %36, %42, %2737, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2739 : int = aten::size(%k.35, %36), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2740 : Long(device=cpu) = prim::NumToTensor(%2739), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2741 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2740, %29), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2742 : int = aten::Int(%2741), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %x2.71 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.35, %36, %2742, %37, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2744 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.71), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2745 : Tensor[] = prim::ListConstruct(%2744, %x1.71), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2746 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2745, %45), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2747 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2746, %sin.39), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.519 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2733, %2747, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %2749 : int = aten::size(%hidden_states.519, %42), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2750 : int = aten::size(%hidden_states.519, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.69 : Long(device=cpu) = prim::NumToTensor(%2750), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2752 : int = aten::size(%hidden_states.519, %38), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2753 : int = aten::size(%hidden_states.519, %36), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2754 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.519, %42, %42, %37, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2755 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2754, %44, %42, %37, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2756 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2755, %38), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2757 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2756, %36, %42, %37, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2758 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2757, %35, %42, %37, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2759 : int[] = prim::ListConstruct(%2749, %2750, %35, %2752, %2753), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %hidden_states.521 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2758, %2759, %46), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2761 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.69, %28), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2762 : int = aten::Int(%2761), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2763 : int[] = prim::ListConstruct(%2749, %2762, %2752, %2753), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %key.35 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.521, %2763), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2765 : int = aten::size(%hidden_states.523, %42), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2766 : int = aten::size(%hidden_states.523, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.71 : Long(device=cpu) = prim::NumToTensor(%2766), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2768 : int = aten::size(%hidden_states.523, %38), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2769 : int = aten::size(%hidden_states.523, %36), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2770 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.523, %42, %42, %37, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2771 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2770, %44, %42, %37, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2772 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2771, %38), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2773 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2772, %36, %42, %37, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2774 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2773, %35, %42, %37, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2775 : int[] = prim::ListConstruct(%2765, %2766, %35, %2768, %2769), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %hidden_states.525 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2774, %2775, %46), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2777 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.71, %28), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2778 : int = aten::Int(%2777), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2779 : int[] = prim::ListConstruct(%2765, %2778, %2768, %2769), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %value.35 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.525, %2779), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2781 : int = aten::size(%key.35, %38), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2782 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2783 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2782, %44, %42, %37, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2784 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2783, %38, %42, %37, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.41 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2784, %36, %42, %2781, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.69 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.35, %key.35, %value.35, %attention_mask.41, %27, %46, %26, %46), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %2787 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.69, %44, %38), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.71 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2787, %42), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %2789 : int[] = prim::ListConstruct(%2695, %2696, %45), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2790 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.71, %2789), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.137 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%2790, %42), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.319 : Tensor = prim::GetAttr[name="weight"](%o_proj.35)
  %hidden_states.527 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.137, %weight.319, %41), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn/__module.model.layers.17.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.529 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2690, %hidden_states.527, %44), scope: __module.model/__module.model.layers.17 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.321 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.35)
  %hidden_states.531 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.529, %33, %46, %46, %41), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %2797 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.531, %38), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2798 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm
  %variance.71 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2797, %2798, %25, %41), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2800 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.71, %24, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %2801 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2800), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.533 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.531, %2801), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.535 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.533, %33, %46, %46, %41), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.139 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.321, %hidden_states.535), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %2805 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.139, %hidden_states.531)
  %2806 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2807 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2805)
  %down_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_222.Linear = prim::GetAttr[name="down_proj"](%mlp.35)
  %up_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_221.Linear = prim::GetAttr[name="up_proj"](%mlp.35)
  %gate_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_220.Linear = prim::GetAttr[name="gate_proj"](%mlp.35)
  %weight.323 : Tensor = prim::GetAttr[name="weight"](%gate_proj.35)
  %input.141 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%2806, %weight.323, %41), scope: __module.model/__module.model.layers.17/__module.model.layers.17.mlp/__module.model.layers.17.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2813 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.141), scope: __module.model/__module.model.layers.17/__module.model.layers.17.mlp/__module.model.layers.17.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.325 : Tensor = prim::GetAttr[name="weight"](%up_proj.35)
  %2815 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%2806, %weight.325, %41), scope: __module.model/__module.model.layers.17/__module.model.layers.17.mlp/__module.model.layers.17.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.143 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%2813, %2815), scope: __module.model/__module.model.layers.17/__module.model.layers.17.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.327 : Tensor = prim::GetAttr[name="weight"](%down_proj.35)
  %hidden_states.537 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.143, %weight.327, %41), scope: __module.model/__module.model.layers.17/__module.model.layers.17.mlp/__module.model.layers.17.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.539 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2807, %hidden_states.537, %44), scope: __module.model/__module.model.layers.17 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.37 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_237.Qwen2MLP = prim::GetAttr[name="mlp"](%_18)
  %post_attention_layernorm.37 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_239.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_18)
  %self_attn.37 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_232.Qwen2Attention = prim::GetAttr[name="self_attn"](%_18)
  %input_layernorm.37 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_238.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_18)
  %weight.329 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.37)
  %hidden_states.541 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.539, %33, %46, %46, %41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %2826 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.541, %38), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2827 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm
  %variance.73 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2826, %2827, %25, %41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2829 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.73, %24, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %2830 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2829), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.543 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.541, %2830), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.545 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.543, %33, %46, %46, %41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.547 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.329, %hidden_states.545), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %2834 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.547, %hidden_states.541)
  %2835 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2836 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2834)
  %o_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_231.Linear = prim::GetAttr[name="o_proj"](%self_attn.37)
  %v_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_230.Linear = prim::GetAttr[name="v_proj"](%self_attn.37)
  %k_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_229.Linear = prim::GetAttr[name="k_proj"](%self_attn.37)
  %q_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_228.Linear = prim::GetAttr[name="q_proj"](%self_attn.37)
  %2841 : int = aten::size(%2835, %42), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %2842 : int = aten::size(%2835, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.109 : Tensor = prim::GetAttr[name="bias"](%q_proj.37)
  %weight.331 : Tensor = prim::GetAttr[name="weight"](%q_proj.37)
  %2845 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2835, %weight.331, %bias.109), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn/__module.model.layers.18.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2846 : int[] = prim::ListConstruct(%2841, %2842, %45, %30), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2847 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%2845, %2846), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.37 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2847, %44, %38), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.111 : Tensor = prim::GetAttr[name="bias"](%k_proj.37)
  %weight.333 : Tensor = prim::GetAttr[name="weight"](%k_proj.37)
  %2851 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2835, %weight.333, %bias.111), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn/__module.model.layers.18.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2852 : int[] = prim::ListConstruct(%2841, %2842, %45, %30), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2853 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2851, %2852), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.37 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2853, %44, %38), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.113 : Tensor = prim::GetAttr[name="bias"](%v_proj.37)
  %weight.335 : Tensor = prim::GetAttr[name="weight"](%v_proj.37)
  %2857 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2835, %weight.335, %bias.113), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn/__module.model.layers.18.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2858 : int[] = prim::ListConstruct(%2841, %2842, %45, %30), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2859 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2857, %2858), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.553 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2859, %44, %38), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.41 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.41 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %2863 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.37, %cos.41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %2864 : int = aten::size(%q.37, %36), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2865 : Long(device=cpu) = prim::NumToTensor(%2864), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2866 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2865, %29), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2867 : int = aten::Int(%2866), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %x1.73 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.37, %36, %42, %2867, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2869 : int = aten::size(%q.37, %36), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2870 : Long(device=cpu) = prim::NumToTensor(%2869), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2871 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2870, %29), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2872 : int = aten::Int(%2871), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %x2.73 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.37, %36, %2872, %37, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2874 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.73), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2875 : Tensor[] = prim::ListConstruct(%2874, %x1.73), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2876 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2875, %45), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2877 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2876, %sin.41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.37 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2863, %2877, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %2879 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.37, %cos.41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %2880 : int = aten::size(%k.37, %36), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2881 : Long(device=cpu) = prim::NumToTensor(%2880), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2882 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2881, %29), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2883 : int = aten::Int(%2882), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %x1.75 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.37, %36, %42, %2883, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %2885 : int = aten::size(%k.37, %36), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2886 : Long(device=cpu) = prim::NumToTensor(%2885), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2887 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2886, %29), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2888 : int = aten::Int(%2887), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %x2.75 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.37, %36, %2888, %37, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %2890 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.75), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2891 : Tensor[] = prim::ListConstruct(%2890, %x1.75), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2892 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2891, %45), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %2893 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2892, %sin.41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.549 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2879, %2893, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %2895 : int = aten::size(%hidden_states.549, %42), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2896 : int = aten::size(%hidden_states.549, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.73 : Long(device=cpu) = prim::NumToTensor(%2896), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2898 : int = aten::size(%hidden_states.549, %38), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2899 : int = aten::size(%hidden_states.549, %36), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2900 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.549, %42, %42, %37, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2901 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2900, %44, %42, %37, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2902 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2901, %38), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2903 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2902, %36, %42, %37, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2904 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2903, %35, %42, %37, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2905 : int[] = prim::ListConstruct(%2895, %2896, %35, %2898, %2899), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %hidden_states.551 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2904, %2905, %46), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2907 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.73, %28), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2908 : int = aten::Int(%2907), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2909 : int[] = prim::ListConstruct(%2895, %2908, %2898, %2899), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %key.37 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.551, %2909), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2911 : int = aten::size(%hidden_states.553, %42), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2912 : int = aten::size(%hidden_states.553, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.75 : Long(device=cpu) = prim::NumToTensor(%2912), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2914 : int = aten::size(%hidden_states.553, %38), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2915 : int = aten::size(%hidden_states.553, %36), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2916 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.553, %42, %42, %37, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2917 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2916, %44, %42, %37, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2918 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2917, %38), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2919 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2918, %36, %42, %37, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2920 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2919, %35, %42, %37, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2921 : int[] = prim::ListConstruct(%2911, %2912, %35, %2914, %2915), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %hidden_states.555 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2920, %2921, %46), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2923 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.75, %28), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2924 : int = aten::Int(%2923), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2925 : int[] = prim::ListConstruct(%2911, %2924, %2914, %2915), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %value.37 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.555, %2925), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2927 : int = aten::size(%key.37, %38), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2928 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2929 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2928, %44, %42, %37, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2930 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2929, %38, %42, %37, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.43 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2930, %36, %42, %2927, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.73 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.37, %key.37, %value.37, %attention_mask.43, %27, %46, %26, %46), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %2933 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.73, %44, %38), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.75 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2933, %42), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %2935 : int[] = prim::ListConstruct(%2841, %2842, %45), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2936 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.75, %2935), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.145 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%2936, %42), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.337 : Tensor = prim::GetAttr[name="weight"](%o_proj.37)
  %hidden_states.557 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.145, %weight.337, %41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn/__module.model.layers.18.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.559 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2836, %hidden_states.557, %44), scope: __module.model/__module.model.layers.18 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.339 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.37)
  %hidden_states.561 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.559, %33, %46, %46, %41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %2943 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.561, %38), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2944 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm
  %variance.75 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2943, %2944, %25, %41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2946 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.75, %24, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %2947 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2946), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.563 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.561, %2947), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.565 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.563, %33, %46, %46, %41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.147 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.339, %hidden_states.565), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %2951 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.147, %hidden_states.561)
  %2952 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2953 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2951)
  %down_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_235.Linear = prim::GetAttr[name="down_proj"](%mlp.37)
  %up_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_234.Linear = prim::GetAttr[name="up_proj"](%mlp.37)
  %gate_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_233.Linear = prim::GetAttr[name="gate_proj"](%mlp.37)
  %weight.341 : Tensor = prim::GetAttr[name="weight"](%gate_proj.37)
  %input.149 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%2952, %weight.341, %41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.mlp/__module.model.layers.18.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2959 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.149), scope: __module.model/__module.model.layers.18/__module.model.layers.18.mlp/__module.model.layers.18.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.343 : Tensor = prim::GetAttr[name="weight"](%up_proj.37)
  %2961 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%2952, %weight.343, %41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.mlp/__module.model.layers.18.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.151 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%2959, %2961), scope: __module.model/__module.model.layers.18/__module.model.layers.18.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.345 : Tensor = prim::GetAttr[name="weight"](%down_proj.37)
  %hidden_states.567 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.151, %weight.345, %41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.mlp/__module.model.layers.18.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.569 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2953, %hidden_states.567, %44), scope: __module.model/__module.model.layers.18 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.39 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_250.Qwen2MLP = prim::GetAttr[name="mlp"](%_19)
  %post_attention_layernorm.39 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_252.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_19)
  %self_attn.39 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_245.Qwen2Attention = prim::GetAttr[name="self_attn"](%_19)
  %input_layernorm.39 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_251.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_19)
  %weight.347 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.39)
  %hidden_states.571 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.569, %33, %46, %46, %41), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %2972 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.571, %38), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2973 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm
  %variance.77 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2972, %2973, %25, %41), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %2975 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.77, %24, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %2976 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2975), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.573 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.571, %2976), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.575 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.573, %33, %46, %46, %41), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.577 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.347, %hidden_states.575), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %2980 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.577, %hidden_states.571)
  %2981 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2982 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2980)
  %o_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_244.Linear = prim::GetAttr[name="o_proj"](%self_attn.39)
  %v_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_243.Linear = prim::GetAttr[name="v_proj"](%self_attn.39)
  %k_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_242.Linear = prim::GetAttr[name="k_proj"](%self_attn.39)
  %q_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_241.Linear = prim::GetAttr[name="q_proj"](%self_attn.39)
  %2987 : int = aten::size(%2981, %42), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %2988 : int = aten::size(%2981, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.115 : Tensor = prim::GetAttr[name="bias"](%q_proj.39)
  %weight.349 : Tensor = prim::GetAttr[name="weight"](%q_proj.39)
  %2991 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2981, %weight.349, %bias.115), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn/__module.model.layers.19.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2992 : int[] = prim::ListConstruct(%2987, %2988, %45, %30), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %2993 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%2991, %2992), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.39 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2993, %44, %38), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.117 : Tensor = prim::GetAttr[name="bias"](%k_proj.39)
  %weight.351 : Tensor = prim::GetAttr[name="weight"](%k_proj.39)
  %2997 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2981, %weight.351, %bias.117), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn/__module.model.layers.19.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %2998 : int[] = prim::ListConstruct(%2987, %2988, %45, %30), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %2999 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2997, %2998), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.39 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2999, %44, %38), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.119 : Tensor = prim::GetAttr[name="bias"](%v_proj.39)
  %weight.353 : Tensor = prim::GetAttr[name="weight"](%v_proj.39)
  %3003 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2981, %weight.353, %bias.119), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn/__module.model.layers.19.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3004 : int[] = prim::ListConstruct(%2987, %2988, %45, %30), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3005 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3003, %3004), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.583 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3005, %44, %38), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.43 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.43 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %3009 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.39, %cos.43), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %3010 : int = aten::size(%q.39, %36), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3011 : Long(device=cpu) = prim::NumToTensor(%3010), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3012 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3011, %29), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3013 : int = aten::Int(%3012), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %x1.77 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.39, %36, %42, %3013, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3015 : int = aten::size(%q.39, %36), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3016 : Long(device=cpu) = prim::NumToTensor(%3015), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3017 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3016, %29), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3018 : int = aten::Int(%3017), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %x2.77 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.39, %36, %3018, %37, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3020 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.77), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3021 : Tensor[] = prim::ListConstruct(%3020, %x1.77), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3022 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3021, %45), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3023 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3022, %sin.43), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.39 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3009, %3023, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %3025 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.39, %cos.43), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %3026 : int = aten::size(%k.39, %36), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3027 : Long(device=cpu) = prim::NumToTensor(%3026), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3028 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3027, %29), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3029 : int = aten::Int(%3028), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %x1.79 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.39, %36, %42, %3029, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3031 : int = aten::size(%k.39, %36), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3032 : Long(device=cpu) = prim::NumToTensor(%3031), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3033 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3032, %29), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3034 : int = aten::Int(%3033), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %x2.79 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.39, %36, %3034, %37, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3036 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.79), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3037 : Tensor[] = prim::ListConstruct(%3036, %x1.79), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3038 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3037, %45), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3039 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3038, %sin.43), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.579 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3025, %3039, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %3041 : int = aten::size(%hidden_states.579, %42), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3042 : int = aten::size(%hidden_states.579, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.77 : Long(device=cpu) = prim::NumToTensor(%3042), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3044 : int = aten::size(%hidden_states.579, %38), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3045 : int = aten::size(%hidden_states.579, %36), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3046 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.579, %42, %42, %37, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3047 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3046, %44, %42, %37, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3048 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3047, %38), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3049 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3048, %36, %42, %37, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3050 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3049, %35, %42, %37, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3051 : int[] = prim::ListConstruct(%3041, %3042, %35, %3044, %3045), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %hidden_states.581 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3050, %3051, %46), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3053 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.77, %28), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3054 : int = aten::Int(%3053), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3055 : int[] = prim::ListConstruct(%3041, %3054, %3044, %3045), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %key.39 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.581, %3055), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3057 : int = aten::size(%hidden_states.583, %42), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3058 : int = aten::size(%hidden_states.583, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.79 : Long(device=cpu) = prim::NumToTensor(%3058), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3060 : int = aten::size(%hidden_states.583, %38), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3061 : int = aten::size(%hidden_states.583, %36), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3062 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.583, %42, %42, %37, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3063 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3062, %44, %42, %37, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3064 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3063, %38), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3065 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3064, %36, %42, %37, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3066 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3065, %35, %42, %37, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3067 : int[] = prim::ListConstruct(%3057, %3058, %35, %3060, %3061), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %hidden_states.585 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3066, %3067, %46), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3069 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.79, %28), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3070 : int = aten::Int(%3069), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3071 : int[] = prim::ListConstruct(%3057, %3070, %3060, %3061), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %value.39 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.585, %3071), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3073 : int = aten::size(%key.39, %38), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3074 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3075 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3074, %44, %42, %37, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3076 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3075, %38, %42, %37, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.45 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3076, %36, %42, %3073, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.77 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.39, %key.39, %value.39, %attention_mask.45, %27, %46, %26, %46), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %3079 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.77, %44, %38), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.79 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3079, %42), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %3081 : int[] = prim::ListConstruct(%2987, %2988, %45), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3082 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.79, %3081), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.153 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%3082, %42), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.355 : Tensor = prim::GetAttr[name="weight"](%o_proj.39)
  %hidden_states.587 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.153, %weight.355, %41), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn/__module.model.layers.19.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.589 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2982, %hidden_states.587, %44), scope: __module.model/__module.model.layers.19 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.357 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.39)
  %hidden_states.591 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.589, %33, %46, %46, %41), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %3089 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.591, %38), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3090 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm
  %variance.79 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3089, %3090, %25, %41), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3092 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.79, %24, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %3093 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3092), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.593 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.591, %3093), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.595 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.593, %33, %46, %46, %41), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.155 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.357, %hidden_states.595), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %3097 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.155, %hidden_states.591)
  %3098 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3099 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3097)
  %down_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_248.Linear = prim::GetAttr[name="down_proj"](%mlp.39)
  %up_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_247.Linear = prim::GetAttr[name="up_proj"](%mlp.39)
  %gate_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_246.Linear = prim::GetAttr[name="gate_proj"](%mlp.39)
  %weight.359 : Tensor = prim::GetAttr[name="weight"](%gate_proj.39)
  %input.157 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%3098, %weight.359, %41), scope: __module.model/__module.model.layers.19/__module.model.layers.19.mlp/__module.model.layers.19.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3105 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.157), scope: __module.model/__module.model.layers.19/__module.model.layers.19.mlp/__module.model.layers.19.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.361 : Tensor = prim::GetAttr[name="weight"](%up_proj.39)
  %3107 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%3098, %weight.361, %41), scope: __module.model/__module.model.layers.19/__module.model.layers.19.mlp/__module.model.layers.19.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.159 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%3105, %3107), scope: __module.model/__module.model.layers.19/__module.model.layers.19.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.363 : Tensor = prim::GetAttr[name="weight"](%down_proj.39)
  %hidden_states.597 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.159, %weight.363, %41), scope: __module.model/__module.model.layers.19/__module.model.layers.19.mlp/__module.model.layers.19.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.599 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3099, %hidden_states.597, %44), scope: __module.model/__module.model.layers.19 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.41 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_263.Qwen2MLP = prim::GetAttr[name="mlp"](%_20)
  %post_attention_layernorm.41 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_265.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_20)
  %self_attn.41 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_258.Qwen2Attention = prim::GetAttr[name="self_attn"](%_20)
  %input_layernorm.41 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_264.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_20)
  %weight.365 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.41)
  %hidden_states.601 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.599, %33, %46, %46, %41), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %3118 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.601, %38), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3119 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm
  %variance.81 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3118, %3119, %25, %41), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3121 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.81, %24, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %3122 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3121), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.603 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.601, %3122), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.605 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.603, %33, %46, %46, %41), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.607 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.365, %hidden_states.605), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %3126 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.607, %hidden_states.601)
  %3127 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3128 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3126)
  %o_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_257.Linear = prim::GetAttr[name="o_proj"](%self_attn.41)
  %v_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_256.Linear = prim::GetAttr[name="v_proj"](%self_attn.41)
  %k_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_255.Linear = prim::GetAttr[name="k_proj"](%self_attn.41)
  %q_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_254.Linear = prim::GetAttr[name="q_proj"](%self_attn.41)
  %3133 : int = aten::size(%3127, %42), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %3134 : int = aten::size(%3127, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.121 : Tensor = prim::GetAttr[name="bias"](%q_proj.41)
  %weight.367 : Tensor = prim::GetAttr[name="weight"](%q_proj.41)
  %3137 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3127, %weight.367, %bias.121), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn/__module.model.layers.20.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3138 : int[] = prim::ListConstruct(%3133, %3134, %45, %30), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3139 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%3137, %3138), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.41 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3139, %44, %38), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.123 : Tensor = prim::GetAttr[name="bias"](%k_proj.41)
  %weight.369 : Tensor = prim::GetAttr[name="weight"](%k_proj.41)
  %3143 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3127, %weight.369, %bias.123), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn/__module.model.layers.20.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3144 : int[] = prim::ListConstruct(%3133, %3134, %45, %30), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3145 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3143, %3144), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.41 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3145, %44, %38), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.125 : Tensor = prim::GetAttr[name="bias"](%v_proj.41)
  %weight.371 : Tensor = prim::GetAttr[name="weight"](%v_proj.41)
  %3149 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3127, %weight.371, %bias.125), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn/__module.model.layers.20.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3150 : int[] = prim::ListConstruct(%3133, %3134, %45, %30), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3151 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3149, %3150), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.613 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3151, %44, %38), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.45 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.45 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %3155 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.41, %cos.45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %3156 : int = aten::size(%q.41, %36), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3157 : Long(device=cpu) = prim::NumToTensor(%3156), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3158 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3157, %29), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3159 : int = aten::Int(%3158), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %x1.81 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.41, %36, %42, %3159, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3161 : int = aten::size(%q.41, %36), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3162 : Long(device=cpu) = prim::NumToTensor(%3161), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3163 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3162, %29), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3164 : int = aten::Int(%3163), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %x2.81 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.41, %36, %3164, %37, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3166 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.81), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3167 : Tensor[] = prim::ListConstruct(%3166, %x1.81), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3168 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3167, %45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3169 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3168, %sin.45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.41 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3155, %3169, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %3171 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.41, %cos.45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %3172 : int = aten::size(%k.41, %36), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3173 : Long(device=cpu) = prim::NumToTensor(%3172), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3174 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3173, %29), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3175 : int = aten::Int(%3174), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %x1.83 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.41, %36, %42, %3175, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3177 : int = aten::size(%k.41, %36), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3178 : Long(device=cpu) = prim::NumToTensor(%3177), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3179 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3178, %29), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3180 : int = aten::Int(%3179), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %x2.83 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.41, %36, %3180, %37, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3182 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.83), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3183 : Tensor[] = prim::ListConstruct(%3182, %x1.83), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3184 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3183, %45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3185 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3184, %sin.45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.609 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3171, %3185, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %3187 : int = aten::size(%hidden_states.609, %42), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3188 : int = aten::size(%hidden_states.609, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.81 : Long(device=cpu) = prim::NumToTensor(%3188), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3190 : int = aten::size(%hidden_states.609, %38), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3191 : int = aten::size(%hidden_states.609, %36), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3192 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.609, %42, %42, %37, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3193 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3192, %44, %42, %37, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3194 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3193, %38), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3195 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3194, %36, %42, %37, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3196 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3195, %35, %42, %37, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3197 : int[] = prim::ListConstruct(%3187, %3188, %35, %3190, %3191), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %hidden_states.611 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3196, %3197, %46), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3199 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.81, %28), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3200 : int = aten::Int(%3199), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3201 : int[] = prim::ListConstruct(%3187, %3200, %3190, %3191), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %key.41 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.611, %3201), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3203 : int = aten::size(%hidden_states.613, %42), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3204 : int = aten::size(%hidden_states.613, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.83 : Long(device=cpu) = prim::NumToTensor(%3204), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3206 : int = aten::size(%hidden_states.613, %38), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3207 : int = aten::size(%hidden_states.613, %36), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3208 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.613, %42, %42, %37, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3209 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3208, %44, %42, %37, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3210 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3209, %38), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3211 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3210, %36, %42, %37, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3212 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3211, %35, %42, %37, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3213 : int[] = prim::ListConstruct(%3203, %3204, %35, %3206, %3207), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %hidden_states.615 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3212, %3213, %46), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3215 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.83, %28), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3216 : int = aten::Int(%3215), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3217 : int[] = prim::ListConstruct(%3203, %3216, %3206, %3207), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %value.41 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.615, %3217), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3219 : int = aten::size(%key.41, %38), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3220 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3221 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3220, %44, %42, %37, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3222 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3221, %38, %42, %37, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.47 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3222, %36, %42, %3219, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.81 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.41, %key.41, %value.41, %attention_mask.47, %27, %46, %26, %46), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %3225 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.81, %44, %38), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.83 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3225, %42), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %3227 : int[] = prim::ListConstruct(%3133, %3134, %45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3228 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.83, %3227), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.161 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%3228, %42), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.373 : Tensor = prim::GetAttr[name="weight"](%o_proj.41)
  %hidden_states.617 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.161, %weight.373, %41), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn/__module.model.layers.20.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.619 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3128, %hidden_states.617, %44), scope: __module.model/__module.model.layers.20 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.375 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.41)
  %hidden_states.621 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.619, %33, %46, %46, %41), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %3235 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.621, %38), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3236 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm
  %variance.83 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3235, %3236, %25, %41), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3238 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.83, %24, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %3239 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3238), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.623 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.621, %3239), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.625 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.623, %33, %46, %46, %41), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.163 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.375, %hidden_states.625), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %3243 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.163, %hidden_states.621)
  %3244 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3245 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3243)
  %down_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_261.Linear = prim::GetAttr[name="down_proj"](%mlp.41)
  %up_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_260.Linear = prim::GetAttr[name="up_proj"](%mlp.41)
  %gate_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_259.Linear = prim::GetAttr[name="gate_proj"](%mlp.41)
  %weight.377 : Tensor = prim::GetAttr[name="weight"](%gate_proj.41)
  %input.165 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%3244, %weight.377, %41), scope: __module.model/__module.model.layers.20/__module.model.layers.20.mlp/__module.model.layers.20.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3251 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.165), scope: __module.model/__module.model.layers.20/__module.model.layers.20.mlp/__module.model.layers.20.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.379 : Tensor = prim::GetAttr[name="weight"](%up_proj.41)
  %3253 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%3244, %weight.379, %41), scope: __module.model/__module.model.layers.20/__module.model.layers.20.mlp/__module.model.layers.20.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.167 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%3251, %3253), scope: __module.model/__module.model.layers.20/__module.model.layers.20.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.381 : Tensor = prim::GetAttr[name="weight"](%down_proj.41)
  %hidden_states.627 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.167, %weight.381, %41), scope: __module.model/__module.model.layers.20/__module.model.layers.20.mlp/__module.model.layers.20.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.629 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3245, %hidden_states.627, %44), scope: __module.model/__module.model.layers.20 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.43 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_276.Qwen2MLP = prim::GetAttr[name="mlp"](%_21)
  %post_attention_layernorm.43 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_278.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_21)
  %self_attn.43 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_271.Qwen2Attention = prim::GetAttr[name="self_attn"](%_21)
  %input_layernorm.43 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_277.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_21)
  %weight.383 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.43)
  %hidden_states.631 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.629, %33, %46, %46, %41), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %3264 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.631, %38), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3265 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm
  %variance.85 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3264, %3265, %25, %41), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3267 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.85, %24, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %3268 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3267), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.633 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.631, %3268), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.635 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.633, %33, %46, %46, %41), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.637 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.383, %hidden_states.635), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %3272 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.637, %hidden_states.631)
  %3273 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3274 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3272)
  %o_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_270.Linear = prim::GetAttr[name="o_proj"](%self_attn.43)
  %v_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_269.Linear = prim::GetAttr[name="v_proj"](%self_attn.43)
  %k_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_268.Linear = prim::GetAttr[name="k_proj"](%self_attn.43)
  %q_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_267.Linear = prim::GetAttr[name="q_proj"](%self_attn.43)
  %3279 : int = aten::size(%3273, %42), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %3280 : int = aten::size(%3273, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.127 : Tensor = prim::GetAttr[name="bias"](%q_proj.43)
  %weight.385 : Tensor = prim::GetAttr[name="weight"](%q_proj.43)
  %3283 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3273, %weight.385, %bias.127), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn/__module.model.layers.21.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3284 : int[] = prim::ListConstruct(%3279, %3280, %45, %30), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3285 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%3283, %3284), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.43 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3285, %44, %38), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.129 : Tensor = prim::GetAttr[name="bias"](%k_proj.43)
  %weight.387 : Tensor = prim::GetAttr[name="weight"](%k_proj.43)
  %3289 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3273, %weight.387, %bias.129), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn/__module.model.layers.21.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3290 : int[] = prim::ListConstruct(%3279, %3280, %45, %30), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3291 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3289, %3290), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.43 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3291, %44, %38), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.131 : Tensor = prim::GetAttr[name="bias"](%v_proj.43)
  %weight.389 : Tensor = prim::GetAttr[name="weight"](%v_proj.43)
  %3295 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3273, %weight.389, %bias.131), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn/__module.model.layers.21.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3296 : int[] = prim::ListConstruct(%3279, %3280, %45, %30), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3297 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3295, %3296), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.643 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3297, %44, %38), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.47 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.47 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %3301 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.43, %cos.47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %3302 : int = aten::size(%q.43, %36), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3303 : Long(device=cpu) = prim::NumToTensor(%3302), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3304 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3303, %29), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3305 : int = aten::Int(%3304), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %x1.85 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.43, %36, %42, %3305, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3307 : int = aten::size(%q.43, %36), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3308 : Long(device=cpu) = prim::NumToTensor(%3307), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3309 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3308, %29), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3310 : int = aten::Int(%3309), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %x2.85 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.43, %36, %3310, %37, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3312 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.85), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3313 : Tensor[] = prim::ListConstruct(%3312, %x1.85), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3314 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3313, %45), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3315 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3314, %sin.47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.43 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3301, %3315, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %3317 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.43, %cos.47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %3318 : int = aten::size(%k.43, %36), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3319 : Long(device=cpu) = prim::NumToTensor(%3318), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3320 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3319, %29), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3321 : int = aten::Int(%3320), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %x1.87 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.43, %36, %42, %3321, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3323 : int = aten::size(%k.43, %36), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3324 : Long(device=cpu) = prim::NumToTensor(%3323), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3325 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3324, %29), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3326 : int = aten::Int(%3325), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %x2.87 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.43, %36, %3326, %37, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3328 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.87), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3329 : Tensor[] = prim::ListConstruct(%3328, %x1.87), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3330 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3329, %45), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3331 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3330, %sin.47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.639 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3317, %3331, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %3333 : int = aten::size(%hidden_states.639, %42), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3334 : int = aten::size(%hidden_states.639, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.85 : Long(device=cpu) = prim::NumToTensor(%3334), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3336 : int = aten::size(%hidden_states.639, %38), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3337 : int = aten::size(%hidden_states.639, %36), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3338 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.639, %42, %42, %37, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3339 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3338, %44, %42, %37, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3340 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3339, %38), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3341 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3340, %36, %42, %37, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3342 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3341, %35, %42, %37, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3343 : int[] = prim::ListConstruct(%3333, %3334, %35, %3336, %3337), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %hidden_states.641 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3342, %3343, %46), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3345 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.85, %28), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3346 : int = aten::Int(%3345), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3347 : int[] = prim::ListConstruct(%3333, %3346, %3336, %3337), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %key.43 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.641, %3347), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3349 : int = aten::size(%hidden_states.643, %42), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3350 : int = aten::size(%hidden_states.643, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.87 : Long(device=cpu) = prim::NumToTensor(%3350), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3352 : int = aten::size(%hidden_states.643, %38), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3353 : int = aten::size(%hidden_states.643, %36), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3354 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.643, %42, %42, %37, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3355 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3354, %44, %42, %37, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3356 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3355, %38), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3357 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3356, %36, %42, %37, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3358 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3357, %35, %42, %37, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3359 : int[] = prim::ListConstruct(%3349, %3350, %35, %3352, %3353), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %hidden_states.645 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3358, %3359, %46), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3361 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.87, %28), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3362 : int = aten::Int(%3361), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3363 : int[] = prim::ListConstruct(%3349, %3362, %3352, %3353), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %value.43 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.645, %3363), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3365 : int = aten::size(%key.43, %38), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3366 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3367 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3366, %44, %42, %37, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3368 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3367, %38, %42, %37, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.49 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3368, %36, %42, %3365, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.85 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.43, %key.43, %value.43, %attention_mask.49, %27, %46, %26, %46), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %3371 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.85, %44, %38), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.87 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3371, %42), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %3373 : int[] = prim::ListConstruct(%3279, %3280, %45), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3374 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.87, %3373), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.169 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%3374, %42), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.391 : Tensor = prim::GetAttr[name="weight"](%o_proj.43)
  %hidden_states.647 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.169, %weight.391, %41), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn/__module.model.layers.21.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.649 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3274, %hidden_states.647, %44), scope: __module.model/__module.model.layers.21 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.393 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.43)
  %hidden_states.651 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.649, %33, %46, %46, %41), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %3381 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.651, %38), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3382 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm
  %variance.87 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3381, %3382, %25, %41), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3384 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.87, %24, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %3385 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3384), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.653 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.651, %3385), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.655 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.653, %33, %46, %46, %41), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.171 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.393, %hidden_states.655), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %3389 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.171, %hidden_states.651)
  %3390 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3391 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3389)
  %down_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_274.Linear = prim::GetAttr[name="down_proj"](%mlp.43)
  %up_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_273.Linear = prim::GetAttr[name="up_proj"](%mlp.43)
  %gate_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_272.Linear = prim::GetAttr[name="gate_proj"](%mlp.43)
  %weight.395 : Tensor = prim::GetAttr[name="weight"](%gate_proj.43)
  %input.173 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%3390, %weight.395, %41), scope: __module.model/__module.model.layers.21/__module.model.layers.21.mlp/__module.model.layers.21.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3397 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.173), scope: __module.model/__module.model.layers.21/__module.model.layers.21.mlp/__module.model.layers.21.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.397 : Tensor = prim::GetAttr[name="weight"](%up_proj.43)
  %3399 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%3390, %weight.397, %41), scope: __module.model/__module.model.layers.21/__module.model.layers.21.mlp/__module.model.layers.21.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.175 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%3397, %3399), scope: __module.model/__module.model.layers.21/__module.model.layers.21.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.399 : Tensor = prim::GetAttr[name="weight"](%down_proj.43)
  %hidden_states.657 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.175, %weight.399, %41), scope: __module.model/__module.model.layers.21/__module.model.layers.21.mlp/__module.model.layers.21.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.659 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3391, %hidden_states.657, %44), scope: __module.model/__module.model.layers.21 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.45 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_289.Qwen2MLP = prim::GetAttr[name="mlp"](%_22)
  %post_attention_layernorm.45 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_291.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_22)
  %self_attn.45 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_284.Qwen2Attention = prim::GetAttr[name="self_attn"](%_22)
  %input_layernorm.45 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_290.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_22)
  %weight.401 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.45)
  %hidden_states.661 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.659, %33, %46, %46, %41), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %3410 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.661, %38), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3411 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm
  %variance.89 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3410, %3411, %25, %41), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3413 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.89, %24, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %3414 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3413), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.663 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.661, %3414), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.665 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.663, %33, %46, %46, %41), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.667 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.401, %hidden_states.665), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %3418 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.667, %hidden_states.661)
  %3419 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3420 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3418)
  %o_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_283.Linear = prim::GetAttr[name="o_proj"](%self_attn.45)
  %v_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_282.Linear = prim::GetAttr[name="v_proj"](%self_attn.45)
  %k_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_281.Linear = prim::GetAttr[name="k_proj"](%self_attn.45)
  %q_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_280.Linear = prim::GetAttr[name="q_proj"](%self_attn.45)
  %3425 : int = aten::size(%3419, %42), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %3426 : int = aten::size(%3419, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.133 : Tensor = prim::GetAttr[name="bias"](%q_proj.45)
  %weight.403 : Tensor = prim::GetAttr[name="weight"](%q_proj.45)
  %3429 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3419, %weight.403, %bias.133), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn/__module.model.layers.22.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3430 : int[] = prim::ListConstruct(%3425, %3426, %45, %30), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3431 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%3429, %3430), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.45 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3431, %44, %38), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.135 : Tensor = prim::GetAttr[name="bias"](%k_proj.45)
  %weight.405 : Tensor = prim::GetAttr[name="weight"](%k_proj.45)
  %3435 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3419, %weight.405, %bias.135), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn/__module.model.layers.22.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3436 : int[] = prim::ListConstruct(%3425, %3426, %45, %30), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3437 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3435, %3436), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.45 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3437, %44, %38), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.137 : Tensor = prim::GetAttr[name="bias"](%v_proj.45)
  %weight.407 : Tensor = prim::GetAttr[name="weight"](%v_proj.45)
  %3441 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3419, %weight.407, %bias.137), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn/__module.model.layers.22.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3442 : int[] = prim::ListConstruct(%3425, %3426, %45, %30), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3443 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3441, %3442), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.673 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3443, %44, %38), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.49 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.49 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %3447 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.45, %cos.49), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %3448 : int = aten::size(%q.45, %36), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3449 : Long(device=cpu) = prim::NumToTensor(%3448), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3450 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3449, %29), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3451 : int = aten::Int(%3450), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %x1.89 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.45, %36, %42, %3451, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3453 : int = aten::size(%q.45, %36), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3454 : Long(device=cpu) = prim::NumToTensor(%3453), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3455 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3454, %29), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3456 : int = aten::Int(%3455), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %x2.89 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.45, %36, %3456, %37, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3458 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.89), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3459 : Tensor[] = prim::ListConstruct(%3458, %x1.89), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3460 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3459, %45), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3461 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3460, %sin.49), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.45 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3447, %3461, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %3463 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.45, %cos.49), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %3464 : int = aten::size(%k.45, %36), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3465 : Long(device=cpu) = prim::NumToTensor(%3464), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3466 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3465, %29), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3467 : int = aten::Int(%3466), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %x1.91 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.45, %36, %42, %3467, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3469 : int = aten::size(%k.45, %36), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3470 : Long(device=cpu) = prim::NumToTensor(%3469), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3471 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3470, %29), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3472 : int = aten::Int(%3471), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %x2.91 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.45, %36, %3472, %37, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3474 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.91), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3475 : Tensor[] = prim::ListConstruct(%3474, %x1.91), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3476 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3475, %45), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3477 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3476, %sin.49), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.669 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3463, %3477, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %3479 : int = aten::size(%hidden_states.669, %42), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3480 : int = aten::size(%hidden_states.669, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.89 : Long(device=cpu) = prim::NumToTensor(%3480), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3482 : int = aten::size(%hidden_states.669, %38), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3483 : int = aten::size(%hidden_states.669, %36), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3484 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.669, %42, %42, %37, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3485 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3484, %44, %42, %37, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3486 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3485, %38), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3487 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3486, %36, %42, %37, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3488 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3487, %35, %42, %37, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3489 : int[] = prim::ListConstruct(%3479, %3480, %35, %3482, %3483), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %hidden_states.671 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3488, %3489, %46), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3491 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.89, %28), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3492 : int = aten::Int(%3491), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3493 : int[] = prim::ListConstruct(%3479, %3492, %3482, %3483), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %key.45 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.671, %3493), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3495 : int = aten::size(%hidden_states.673, %42), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3496 : int = aten::size(%hidden_states.673, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.91 : Long(device=cpu) = prim::NumToTensor(%3496), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3498 : int = aten::size(%hidden_states.673, %38), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3499 : int = aten::size(%hidden_states.673, %36), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3500 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.673, %42, %42, %37, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3501 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3500, %44, %42, %37, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3502 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3501, %38), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3503 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3502, %36, %42, %37, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3504 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3503, %35, %42, %37, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3505 : int[] = prim::ListConstruct(%3495, %3496, %35, %3498, %3499), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %hidden_states.675 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3504, %3505, %46), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3507 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.91, %28), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3508 : int = aten::Int(%3507), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3509 : int[] = prim::ListConstruct(%3495, %3508, %3498, %3499), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %value.45 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.675, %3509), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3511 : int = aten::size(%key.45, %38), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3512 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3513 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3512, %44, %42, %37, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3514 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3513, %38, %42, %37, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.51 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3514, %36, %42, %3511, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.89 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.45, %key.45, %value.45, %attention_mask.51, %27, %46, %26, %46), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %3517 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.89, %44, %38), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.91 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3517, %42), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %3519 : int[] = prim::ListConstruct(%3425, %3426, %45), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3520 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.91, %3519), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.177 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%3520, %42), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.409 : Tensor = prim::GetAttr[name="weight"](%o_proj.45)
  %hidden_states.677 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.177, %weight.409, %41), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn/__module.model.layers.22.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.679 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3420, %hidden_states.677, %44), scope: __module.model/__module.model.layers.22 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.411 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.45)
  %hidden_states.681 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.679, %33, %46, %46, %41), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %3527 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.681, %38), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3528 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm
  %variance.91 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3527, %3528, %25, %41), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3530 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.91, %24, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %3531 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3530), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.683 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.681, %3531), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.685 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.683, %33, %46, %46, %41), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.179 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.411, %hidden_states.685), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %3535 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.179, %hidden_states.681)
  %3536 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3537 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3535)
  %down_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_287.Linear = prim::GetAttr[name="down_proj"](%mlp.45)
  %up_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_286.Linear = prim::GetAttr[name="up_proj"](%mlp.45)
  %gate_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_285.Linear = prim::GetAttr[name="gate_proj"](%mlp.45)
  %weight.413 : Tensor = prim::GetAttr[name="weight"](%gate_proj.45)
  %input.181 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%3536, %weight.413, %41), scope: __module.model/__module.model.layers.22/__module.model.layers.22.mlp/__module.model.layers.22.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3543 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.181), scope: __module.model/__module.model.layers.22/__module.model.layers.22.mlp/__module.model.layers.22.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.415 : Tensor = prim::GetAttr[name="weight"](%up_proj.45)
  %3545 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%3536, %weight.415, %41), scope: __module.model/__module.model.layers.22/__module.model.layers.22.mlp/__module.model.layers.22.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.183 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%3543, %3545), scope: __module.model/__module.model.layers.22/__module.model.layers.22.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.417 : Tensor = prim::GetAttr[name="weight"](%down_proj.45)
  %hidden_states.687 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.183, %weight.417, %41), scope: __module.model/__module.model.layers.22/__module.model.layers.22.mlp/__module.model.layers.22.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.689 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3537, %hidden_states.687, %44), scope: __module.model/__module.model.layers.22 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.47 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_302.Qwen2MLP = prim::GetAttr[name="mlp"](%_23)
  %post_attention_layernorm.47 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_304.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_23)
  %self_attn.47 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_297.Qwen2Attention = prim::GetAttr[name="self_attn"](%_23)
  %input_layernorm.47 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_303.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_23)
  %weight.419 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.47)
  %hidden_states.691 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.689, %33, %46, %46, %41), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %3556 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.691, %38), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3557 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm
  %variance.93 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3556, %3557, %25, %41), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3559 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.93, %24, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %3560 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3559), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.693 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.691, %3560), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.695 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.693, %33, %46, %46, %41), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.697 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.419, %hidden_states.695), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %3564 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.697, %hidden_states.691)
  %3565 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3566 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3564)
  %o_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_296.Linear = prim::GetAttr[name="o_proj"](%self_attn.47)
  %v_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_295.Linear = prim::GetAttr[name="v_proj"](%self_attn.47)
  %k_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_294.Linear = prim::GetAttr[name="k_proj"](%self_attn.47)
  %q_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_293.Linear = prim::GetAttr[name="q_proj"](%self_attn.47)
  %3571 : int = aten::size(%3565, %42), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %3572 : int = aten::size(%3565, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.139 : Tensor = prim::GetAttr[name="bias"](%q_proj.47)
  %weight.421 : Tensor = prim::GetAttr[name="weight"](%q_proj.47)
  %3575 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3565, %weight.421, %bias.139), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn/__module.model.layers.23.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3576 : int[] = prim::ListConstruct(%3571, %3572, %45, %30), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3577 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%3575, %3576), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.47 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3577, %44, %38), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.141 : Tensor = prim::GetAttr[name="bias"](%k_proj.47)
  %weight.423 : Tensor = prim::GetAttr[name="weight"](%k_proj.47)
  %3581 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3565, %weight.423, %bias.141), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn/__module.model.layers.23.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3582 : int[] = prim::ListConstruct(%3571, %3572, %45, %30), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3583 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3581, %3582), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.47 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3583, %44, %38), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.143 : Tensor = prim::GetAttr[name="bias"](%v_proj.47)
  %weight.425 : Tensor = prim::GetAttr[name="weight"](%v_proj.47)
  %3587 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3565, %weight.425, %bias.143), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn/__module.model.layers.23.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3588 : int[] = prim::ListConstruct(%3571, %3572, %45, %30), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3589 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3587, %3588), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.703 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3589, %44, %38), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.51 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.51 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %3593 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.47, %cos.51), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %3594 : int = aten::size(%q.47, %36), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3595 : Long(device=cpu) = prim::NumToTensor(%3594), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3596 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3595, %29), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3597 : int = aten::Int(%3596), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %x1.93 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.47, %36, %42, %3597, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3599 : int = aten::size(%q.47, %36), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3600 : Long(device=cpu) = prim::NumToTensor(%3599), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3601 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3600, %29), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3602 : int = aten::Int(%3601), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %x2.93 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.47, %36, %3602, %37, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3604 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.93), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3605 : Tensor[] = prim::ListConstruct(%3604, %x1.93), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3606 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3605, %45), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3607 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3606, %sin.51), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.47 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3593, %3607, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %3609 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.47, %cos.51), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %3610 : int = aten::size(%k.47, %36), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3611 : Long(device=cpu) = prim::NumToTensor(%3610), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3612 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3611, %29), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3613 : int = aten::Int(%3612), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %x1.95 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.47, %36, %42, %3613, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3615 : int = aten::size(%k.47, %36), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3616 : Long(device=cpu) = prim::NumToTensor(%3615), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3617 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3616, %29), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3618 : int = aten::Int(%3617), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %x2.95 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.47, %36, %3618, %37, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3620 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.95), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3621 : Tensor[] = prim::ListConstruct(%3620, %x1.95), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3622 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3621, %45), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3623 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3622, %sin.51), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.699 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3609, %3623, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %3625 : int = aten::size(%hidden_states.699, %42), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3626 : int = aten::size(%hidden_states.699, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.93 : Long(device=cpu) = prim::NumToTensor(%3626), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3628 : int = aten::size(%hidden_states.699, %38), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3629 : int = aten::size(%hidden_states.699, %36), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3630 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.699, %42, %42, %37, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3631 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3630, %44, %42, %37, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3632 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3631, %38), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3633 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3632, %36, %42, %37, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3634 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3633, %35, %42, %37, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3635 : int[] = prim::ListConstruct(%3625, %3626, %35, %3628, %3629), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %hidden_states.701 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3634, %3635, %46), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3637 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.93, %28), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3638 : int = aten::Int(%3637), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3639 : int[] = prim::ListConstruct(%3625, %3638, %3628, %3629), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %key.47 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.701, %3639), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3641 : int = aten::size(%hidden_states.703, %42), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3642 : int = aten::size(%hidden_states.703, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.95 : Long(device=cpu) = prim::NumToTensor(%3642), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3644 : int = aten::size(%hidden_states.703, %38), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3645 : int = aten::size(%hidden_states.703, %36), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3646 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.703, %42, %42, %37, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3647 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3646, %44, %42, %37, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3648 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3647, %38), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3649 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3648, %36, %42, %37, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3650 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3649, %35, %42, %37, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3651 : int[] = prim::ListConstruct(%3641, %3642, %35, %3644, %3645), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %hidden_states.705 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3650, %3651, %46), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3653 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.95, %28), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3654 : int = aten::Int(%3653), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3655 : int[] = prim::ListConstruct(%3641, %3654, %3644, %3645), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %value.47 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.705, %3655), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3657 : int = aten::size(%key.47, %38), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3658 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3659 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3658, %44, %42, %37, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3660 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3659, %38, %42, %37, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.53 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3660, %36, %42, %3657, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.93 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.47, %key.47, %value.47, %attention_mask.53, %27, %46, %26, %46), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %3663 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.93, %44, %38), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.95 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3663, %42), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %3665 : int[] = prim::ListConstruct(%3571, %3572, %45), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3666 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.95, %3665), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.185 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%3666, %42), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.427 : Tensor = prim::GetAttr[name="weight"](%o_proj.47)
  %hidden_states.707 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.185, %weight.427, %41), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn/__module.model.layers.23.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.709 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3566, %hidden_states.707, %44), scope: __module.model/__module.model.layers.23 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.429 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.47)
  %hidden_states.711 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.709, %33, %46, %46, %41), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %3673 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.711, %38), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3674 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm
  %variance.95 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3673, %3674, %25, %41), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3676 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.95, %24, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %3677 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3676), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.713 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.711, %3677), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.715 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.713, %33, %46, %46, %41), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.187 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.429, %hidden_states.715), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %3681 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.187, %hidden_states.711)
  %3682 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3683 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3681)
  %down_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_300.Linear = prim::GetAttr[name="down_proj"](%mlp.47)
  %up_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_299.Linear = prim::GetAttr[name="up_proj"](%mlp.47)
  %gate_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_298.Linear = prim::GetAttr[name="gate_proj"](%mlp.47)
  %weight.431 : Tensor = prim::GetAttr[name="weight"](%gate_proj.47)
  %input.189 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%3682, %weight.431, %41), scope: __module.model/__module.model.layers.23/__module.model.layers.23.mlp/__module.model.layers.23.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3689 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.189), scope: __module.model/__module.model.layers.23/__module.model.layers.23.mlp/__module.model.layers.23.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.433 : Tensor = prim::GetAttr[name="weight"](%up_proj.47)
  %3691 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%3682, %weight.433, %41), scope: __module.model/__module.model.layers.23/__module.model.layers.23.mlp/__module.model.layers.23.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.191 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%3689, %3691), scope: __module.model/__module.model.layers.23/__module.model.layers.23.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.435 : Tensor = prim::GetAttr[name="weight"](%down_proj.47)
  %hidden_states.717 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.191, %weight.435, %41), scope: __module.model/__module.model.layers.23/__module.model.layers.23.mlp/__module.model.layers.23.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.719 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3683, %hidden_states.717, %44), scope: __module.model/__module.model.layers.23 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.49 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_315.Qwen2MLP = prim::GetAttr[name="mlp"](%_24)
  %post_attention_layernorm.49 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_317.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_24)
  %self_attn.49 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_310.Qwen2Attention = prim::GetAttr[name="self_attn"](%_24)
  %input_layernorm.49 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_316.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_24)
  %weight.437 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.49)
  %hidden_states.721 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.719, %33, %46, %46, %41), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %3702 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.721, %38), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3703 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm
  %variance.97 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3702, %3703, %25, %41), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3705 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.97, %24, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %3706 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3705), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.723 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.721, %3706), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.725 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.723, %33, %46, %46, %41), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.727 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.437, %hidden_states.725), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %3710 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.727, %hidden_states.721)
  %3711 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3712 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3710)
  %o_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_309.Linear = prim::GetAttr[name="o_proj"](%self_attn.49)
  %v_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_308.Linear = prim::GetAttr[name="v_proj"](%self_attn.49)
  %k_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_307.Linear = prim::GetAttr[name="k_proj"](%self_attn.49)
  %q_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_306.Linear = prim::GetAttr[name="q_proj"](%self_attn.49)
  %3717 : int = aten::size(%3711, %42), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %3718 : int = aten::size(%3711, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.145 : Tensor = prim::GetAttr[name="bias"](%q_proj.49)
  %weight.439 : Tensor = prim::GetAttr[name="weight"](%q_proj.49)
  %3721 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3711, %weight.439, %bias.145), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn/__module.model.layers.24.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3722 : int[] = prim::ListConstruct(%3717, %3718, %45, %30), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3723 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%3721, %3722), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.49 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3723, %44, %38), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.147 : Tensor = prim::GetAttr[name="bias"](%k_proj.49)
  %weight.441 : Tensor = prim::GetAttr[name="weight"](%k_proj.49)
  %3727 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3711, %weight.441, %bias.147), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn/__module.model.layers.24.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3728 : int[] = prim::ListConstruct(%3717, %3718, %45, %30), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3729 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3727, %3728), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.49 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3729, %44, %38), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.149 : Tensor = prim::GetAttr[name="bias"](%v_proj.49)
  %weight.443 : Tensor = prim::GetAttr[name="weight"](%v_proj.49)
  %3733 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3711, %weight.443, %bias.149), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn/__module.model.layers.24.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3734 : int[] = prim::ListConstruct(%3717, %3718, %45, %30), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3735 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3733, %3734), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.733 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3735, %44, %38), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.53 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.53 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %3739 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.49, %cos.53), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %3740 : int = aten::size(%q.49, %36), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3741 : Long(device=cpu) = prim::NumToTensor(%3740), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3742 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3741, %29), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3743 : int = aten::Int(%3742), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %x1.97 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.49, %36, %42, %3743, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3745 : int = aten::size(%q.49, %36), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3746 : Long(device=cpu) = prim::NumToTensor(%3745), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3747 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3746, %29), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3748 : int = aten::Int(%3747), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %x2.97 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.49, %36, %3748, %37, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3750 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.97), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3751 : Tensor[] = prim::ListConstruct(%3750, %x1.97), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3752 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3751, %45), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3753 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3752, %sin.53), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.49 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3739, %3753, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %3755 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.49, %cos.53), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %3756 : int = aten::size(%k.49, %36), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3757 : Long(device=cpu) = prim::NumToTensor(%3756), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3758 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3757, %29), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3759 : int = aten::Int(%3758), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %x1.99 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.49, %36, %42, %3759, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3761 : int = aten::size(%k.49, %36), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3762 : Long(device=cpu) = prim::NumToTensor(%3761), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3763 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3762, %29), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3764 : int = aten::Int(%3763), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %x2.99 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.49, %36, %3764, %37, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3766 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.99), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3767 : Tensor[] = prim::ListConstruct(%3766, %x1.99), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3768 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3767, %45), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3769 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3768, %sin.53), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.729 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3755, %3769, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %3771 : int = aten::size(%hidden_states.729, %42), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3772 : int = aten::size(%hidden_states.729, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.97 : Long(device=cpu) = prim::NumToTensor(%3772), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3774 : int = aten::size(%hidden_states.729, %38), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3775 : int = aten::size(%hidden_states.729, %36), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3776 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.729, %42, %42, %37, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3777 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3776, %44, %42, %37, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3778 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3777, %38), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3779 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3778, %36, %42, %37, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3780 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3779, %35, %42, %37, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3781 : int[] = prim::ListConstruct(%3771, %3772, %35, %3774, %3775), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %hidden_states.731 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3780, %3781, %46), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3783 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.97, %28), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3784 : int = aten::Int(%3783), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3785 : int[] = prim::ListConstruct(%3771, %3784, %3774, %3775), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %key.49 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.731, %3785), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3787 : int = aten::size(%hidden_states.733, %42), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3788 : int = aten::size(%hidden_states.733, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.99 : Long(device=cpu) = prim::NumToTensor(%3788), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3790 : int = aten::size(%hidden_states.733, %38), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3791 : int = aten::size(%hidden_states.733, %36), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3792 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.733, %42, %42, %37, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3793 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3792, %44, %42, %37, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3794 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3793, %38), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3795 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3794, %36, %42, %37, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3796 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3795, %35, %42, %37, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3797 : int[] = prim::ListConstruct(%3787, %3788, %35, %3790, %3791), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %hidden_states.735 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3796, %3797, %46), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3799 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.99, %28), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3800 : int = aten::Int(%3799), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3801 : int[] = prim::ListConstruct(%3787, %3800, %3790, %3791), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %value.49 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.735, %3801), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3803 : int = aten::size(%key.49, %38), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3804 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3805 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3804, %44, %42, %37, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3806 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3805, %38, %42, %37, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.55 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3806, %36, %42, %3803, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.97 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.49, %key.49, %value.49, %attention_mask.55, %27, %46, %26, %46), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %3809 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.97, %44, %38), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.99 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3809, %42), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %3811 : int[] = prim::ListConstruct(%3717, %3718, %45), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3812 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.99, %3811), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.193 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%3812, %42), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.445 : Tensor = prim::GetAttr[name="weight"](%o_proj.49)
  %hidden_states.737 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.193, %weight.445, %41), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn/__module.model.layers.24.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.739 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3712, %hidden_states.737, %44), scope: __module.model/__module.model.layers.24 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.447 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.49)
  %hidden_states.741 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.739, %33, %46, %46, %41), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %3819 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.741, %38), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3820 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm
  %variance.99 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3819, %3820, %25, %41), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3822 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.99, %24, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %3823 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3822), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.743 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.741, %3823), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.745 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.743, %33, %46, %46, %41), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.195 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.447, %hidden_states.745), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %3827 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.195, %hidden_states.741)
  %3828 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3829 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3827)
  %down_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_313.Linear = prim::GetAttr[name="down_proj"](%mlp.49)
  %up_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_312.Linear = prim::GetAttr[name="up_proj"](%mlp.49)
  %gate_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_311.Linear = prim::GetAttr[name="gate_proj"](%mlp.49)
  %weight.449 : Tensor = prim::GetAttr[name="weight"](%gate_proj.49)
  %input.197 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%3828, %weight.449, %41), scope: __module.model/__module.model.layers.24/__module.model.layers.24.mlp/__module.model.layers.24.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3835 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.197), scope: __module.model/__module.model.layers.24/__module.model.layers.24.mlp/__module.model.layers.24.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.451 : Tensor = prim::GetAttr[name="weight"](%up_proj.49)
  %3837 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%3828, %weight.451, %41), scope: __module.model/__module.model.layers.24/__module.model.layers.24.mlp/__module.model.layers.24.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.199 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%3835, %3837), scope: __module.model/__module.model.layers.24/__module.model.layers.24.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.453 : Tensor = prim::GetAttr[name="weight"](%down_proj.49)
  %hidden_states.747 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.199, %weight.453, %41), scope: __module.model/__module.model.layers.24/__module.model.layers.24.mlp/__module.model.layers.24.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.749 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3829, %hidden_states.747, %44), scope: __module.model/__module.model.layers.24 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.51 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_328.Qwen2MLP = prim::GetAttr[name="mlp"](%_25)
  %post_attention_layernorm.51 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_330.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_25)
  %self_attn.51 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_323.Qwen2Attention = prim::GetAttr[name="self_attn"](%_25)
  %input_layernorm.51 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_329.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_25)
  %weight.455 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.51)
  %hidden_states.751 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.749, %33, %46, %46, %41), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %3848 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.751, %38), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3849 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm
  %variance.101 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3848, %3849, %25, %41), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3851 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.101, %24, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %3852 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3851), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.753 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.751, %3852), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.755 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.753, %33, %46, %46, %41), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.757 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.455, %hidden_states.755), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %3856 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.757, %hidden_states.751)
  %3857 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3858 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3856)
  %o_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_322.Linear = prim::GetAttr[name="o_proj"](%self_attn.51)
  %v_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_321.Linear = prim::GetAttr[name="v_proj"](%self_attn.51)
  %k_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_320.Linear = prim::GetAttr[name="k_proj"](%self_attn.51)
  %q_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_319.Linear = prim::GetAttr[name="q_proj"](%self_attn.51)
  %3863 : int = aten::size(%3857, %42), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %3864 : int = aten::size(%3857, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.151 : Tensor = prim::GetAttr[name="bias"](%q_proj.51)
  %weight.457 : Tensor = prim::GetAttr[name="weight"](%q_proj.51)
  %3867 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3857, %weight.457, %bias.151), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn/__module.model.layers.25.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3868 : int[] = prim::ListConstruct(%3863, %3864, %45, %30), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3869 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%3867, %3868), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.51 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3869, %44, %38), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.153 : Tensor = prim::GetAttr[name="bias"](%k_proj.51)
  %weight.459 : Tensor = prim::GetAttr[name="weight"](%k_proj.51)
  %3873 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3857, %weight.459, %bias.153), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn/__module.model.layers.25.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3874 : int[] = prim::ListConstruct(%3863, %3864, %45, %30), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3875 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3873, %3874), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.51 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3875, %44, %38), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.155 : Tensor = prim::GetAttr[name="bias"](%v_proj.51)
  %weight.461 : Tensor = prim::GetAttr[name="weight"](%v_proj.51)
  %3879 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3857, %weight.461, %bias.155), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn/__module.model.layers.25.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3880 : int[] = prim::ListConstruct(%3863, %3864, %45, %30), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3881 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3879, %3880), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.763 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3881, %44, %38), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.55 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.55 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %3885 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.51, %cos.55), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %3886 : int = aten::size(%q.51, %36), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3887 : Long(device=cpu) = prim::NumToTensor(%3886), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3888 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3887, %29), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3889 : int = aten::Int(%3888), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %x1.101 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.51, %36, %42, %3889, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3891 : int = aten::size(%q.51, %36), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3892 : Long(device=cpu) = prim::NumToTensor(%3891), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3893 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3892, %29), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3894 : int = aten::Int(%3893), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %x2.101 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.51, %36, %3894, %37, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3896 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.101), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3897 : Tensor[] = prim::ListConstruct(%3896, %x1.101), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3898 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3897, %45), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3899 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3898, %sin.55), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.51 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3885, %3899, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %3901 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.51, %cos.55), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %3902 : int = aten::size(%k.51, %36), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3903 : Long(device=cpu) = prim::NumToTensor(%3902), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3904 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3903, %29), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3905 : int = aten::Int(%3904), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %x1.103 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.51, %36, %42, %3905, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %3907 : int = aten::size(%k.51, %36), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3908 : Long(device=cpu) = prim::NumToTensor(%3907), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3909 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3908, %29), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3910 : int = aten::Int(%3909), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %x2.103 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.51, %36, %3910, %37, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %3912 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.103), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3913 : Tensor[] = prim::ListConstruct(%3912, %x1.103), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3914 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3913, %45), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %3915 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3914, %sin.55), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.759 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3901, %3915, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %3917 : int = aten::size(%hidden_states.759, %42), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3918 : int = aten::size(%hidden_states.759, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.101 : Long(device=cpu) = prim::NumToTensor(%3918), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3920 : int = aten::size(%hidden_states.759, %38), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3921 : int = aten::size(%hidden_states.759, %36), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3922 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.759, %42, %42, %37, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3923 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3922, %44, %42, %37, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3924 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3923, %38), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3925 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3924, %36, %42, %37, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3926 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3925, %35, %42, %37, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3927 : int[] = prim::ListConstruct(%3917, %3918, %35, %3920, %3921), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %hidden_states.761 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3926, %3927, %46), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3929 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.101, %28), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3930 : int = aten::Int(%3929), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3931 : int[] = prim::ListConstruct(%3917, %3930, %3920, %3921), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %key.51 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.761, %3931), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3933 : int = aten::size(%hidden_states.763, %42), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3934 : int = aten::size(%hidden_states.763, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.103 : Long(device=cpu) = prim::NumToTensor(%3934), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3936 : int = aten::size(%hidden_states.763, %38), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3937 : int = aten::size(%hidden_states.763, %36), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3938 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.763, %42, %42, %37, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3939 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3938, %44, %42, %37, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3940 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3939, %38), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3941 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3940, %36, %42, %37, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3942 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3941, %35, %42, %37, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3943 : int[] = prim::ListConstruct(%3933, %3934, %35, %3936, %3937), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %hidden_states.765 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3942, %3943, %46), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3945 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.103, %28), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3946 : int = aten::Int(%3945), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3947 : int[] = prim::ListConstruct(%3933, %3946, %3936, %3937), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %value.51 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.765, %3947), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3949 : int = aten::size(%key.51, %38), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3950 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3951 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3950, %44, %42, %37, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3952 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3951, %38, %42, %37, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.57 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3952, %36, %42, %3949, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.101 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.51, %key.51, %value.51, %attention_mask.57, %27, %46, %26, %46), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %3955 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.101, %44, %38), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.103 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3955, %42), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %3957 : int[] = prim::ListConstruct(%3863, %3864, %45), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3958 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.103, %3957), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.201 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%3958, %42), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.463 : Tensor = prim::GetAttr[name="weight"](%o_proj.51)
  %hidden_states.767 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.201, %weight.463, %41), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn/__module.model.layers.25.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.769 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3858, %hidden_states.767, %44), scope: __module.model/__module.model.layers.25 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.465 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.51)
  %hidden_states.771 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.769, %33, %46, %46, %41), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %3965 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.771, %38), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3966 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm
  %variance.103 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3965, %3966, %25, %41), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3968 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.103, %24, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %3969 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3968), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.773 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.771, %3969), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.775 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.773, %33, %46, %46, %41), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.203 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.465, %hidden_states.775), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %3973 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.203, %hidden_states.771)
  %3974 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3975 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3973)
  %down_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_326.Linear = prim::GetAttr[name="down_proj"](%mlp.51)
  %up_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_325.Linear = prim::GetAttr[name="up_proj"](%mlp.51)
  %gate_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_324.Linear = prim::GetAttr[name="gate_proj"](%mlp.51)
  %weight.467 : Tensor = prim::GetAttr[name="weight"](%gate_proj.51)
  %input.205 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%3974, %weight.467, %41), scope: __module.model/__module.model.layers.25/__module.model.layers.25.mlp/__module.model.layers.25.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %3981 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.205), scope: __module.model/__module.model.layers.25/__module.model.layers.25.mlp/__module.model.layers.25.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.469 : Tensor = prim::GetAttr[name="weight"](%up_proj.51)
  %3983 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%3974, %weight.469, %41), scope: __module.model/__module.model.layers.25/__module.model.layers.25.mlp/__module.model.layers.25.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.207 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%3981, %3983), scope: __module.model/__module.model.layers.25/__module.model.layers.25.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.471 : Tensor = prim::GetAttr[name="weight"](%down_proj.51)
  %hidden_states.777 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.207, %weight.471, %41), scope: __module.model/__module.model.layers.25/__module.model.layers.25.mlp/__module.model.layers.25.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.779 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3975, %hidden_states.777, %44), scope: __module.model/__module.model.layers.25 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.53 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_341.Qwen2MLP = prim::GetAttr[name="mlp"](%_26)
  %post_attention_layernorm.53 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_343.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_26)
  %self_attn.53 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_336.Qwen2Attention = prim::GetAttr[name="self_attn"](%_26)
  %input_layernorm.53 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_342.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_26)
  %weight.473 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.53)
  %hidden_states.781 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.779, %33, %46, %46, %41), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %3994 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.781, %38), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3995 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm
  %variance.105 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3994, %3995, %25, %41), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %3997 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.105, %24, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %3998 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3997), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.783 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.781, %3998), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.785 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.783, %33, %46, %46, %41), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.787 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.473, %hidden_states.785), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %4002 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.787, %hidden_states.781)
  %4003 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4004 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4002)
  %o_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_335.Linear = prim::GetAttr[name="o_proj"](%self_attn.53)
  %v_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_334.Linear = prim::GetAttr[name="v_proj"](%self_attn.53)
  %k_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_333.Linear = prim::GetAttr[name="k_proj"](%self_attn.53)
  %q_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_332.Linear = prim::GetAttr[name="q_proj"](%self_attn.53)
  %4009 : int = aten::size(%4003, %42), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %4010 : int = aten::size(%4003, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.157 : Tensor = prim::GetAttr[name="bias"](%q_proj.53)
  %weight.475 : Tensor = prim::GetAttr[name="weight"](%q_proj.53)
  %4013 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4003, %weight.475, %bias.157), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn/__module.model.layers.26.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4014 : int[] = prim::ListConstruct(%4009, %4010, %45, %30), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4015 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%4013, %4014), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.53 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4015, %44, %38), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.159 : Tensor = prim::GetAttr[name="bias"](%k_proj.53)
  %weight.477 : Tensor = prim::GetAttr[name="weight"](%k_proj.53)
  %4019 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4003, %weight.477, %bias.159), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn/__module.model.layers.26.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4020 : int[] = prim::ListConstruct(%4009, %4010, %45, %30), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4021 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4019, %4020), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.53 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4021, %44, %38), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.161 : Tensor = prim::GetAttr[name="bias"](%v_proj.53)
  %weight.479 : Tensor = prim::GetAttr[name="weight"](%v_proj.53)
  %4025 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4003, %weight.479, %bias.161), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn/__module.model.layers.26.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4026 : int[] = prim::ListConstruct(%4009, %4010, %45, %30), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4027 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4025, %4026), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.793 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4027, %44, %38), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.57 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.57 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %4031 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.53, %cos.57), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %4032 : int = aten::size(%q.53, %36), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4033 : Long(device=cpu) = prim::NumToTensor(%4032), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4034 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4033, %29), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4035 : int = aten::Int(%4034), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %x1.105 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.53, %36, %42, %4035, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4037 : int = aten::size(%q.53, %36), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4038 : Long(device=cpu) = prim::NumToTensor(%4037), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4039 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4038, %29), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4040 : int = aten::Int(%4039), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %x2.105 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.53, %36, %4040, %37, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4042 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.105), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4043 : Tensor[] = prim::ListConstruct(%4042, %x1.105), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4044 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4043, %45), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4045 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4044, %sin.57), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.53 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4031, %4045, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %4047 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.53, %cos.57), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %4048 : int = aten::size(%k.53, %36), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4049 : Long(device=cpu) = prim::NumToTensor(%4048), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4050 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4049, %29), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4051 : int = aten::Int(%4050), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %x1.107 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.53, %36, %42, %4051, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4053 : int = aten::size(%k.53, %36), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4054 : Long(device=cpu) = prim::NumToTensor(%4053), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4055 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4054, %29), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4056 : int = aten::Int(%4055), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %x2.107 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.53, %36, %4056, %37, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4058 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.107), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4059 : Tensor[] = prim::ListConstruct(%4058, %x1.107), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4060 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4059, %45), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4061 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4060, %sin.57), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.789 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4047, %4061, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %4063 : int = aten::size(%hidden_states.789, %42), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4064 : int = aten::size(%hidden_states.789, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.105 : Long(device=cpu) = prim::NumToTensor(%4064), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4066 : int = aten::size(%hidden_states.789, %38), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4067 : int = aten::size(%hidden_states.789, %36), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4068 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.789, %42, %42, %37, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4069 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4068, %44, %42, %37, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4070 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4069, %38), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4071 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4070, %36, %42, %37, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4072 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4071, %35, %42, %37, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4073 : int[] = prim::ListConstruct(%4063, %4064, %35, %4066, %4067), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %hidden_states.791 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4072, %4073, %46), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4075 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.105, %28), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4076 : int = aten::Int(%4075), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4077 : int[] = prim::ListConstruct(%4063, %4076, %4066, %4067), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %key.53 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.791, %4077), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4079 : int = aten::size(%hidden_states.793, %42), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4080 : int = aten::size(%hidden_states.793, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.107 : Long(device=cpu) = prim::NumToTensor(%4080), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4082 : int = aten::size(%hidden_states.793, %38), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4083 : int = aten::size(%hidden_states.793, %36), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4084 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.793, %42, %42, %37, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4085 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4084, %44, %42, %37, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4086 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4085, %38), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4087 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4086, %36, %42, %37, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4088 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4087, %35, %42, %37, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4089 : int[] = prim::ListConstruct(%4079, %4080, %35, %4082, %4083), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %hidden_states.795 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4088, %4089, %46), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4091 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.107, %28), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4092 : int = aten::Int(%4091), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4093 : int[] = prim::ListConstruct(%4079, %4092, %4082, %4083), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %value.53 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.795, %4093), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4095 : int = aten::size(%key.53, %38), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4096 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4097 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4096, %44, %42, %37, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4098 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4097, %38, %42, %37, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.59 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4098, %36, %42, %4095, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.105 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.53, %key.53, %value.53, %attention_mask.59, %27, %46, %26, %46), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %4101 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.105, %44, %38), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.107 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4101, %42), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %4103 : int[] = prim::ListConstruct(%4009, %4010, %45), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4104 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.107, %4103), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.209 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%4104, %42), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.481 : Tensor = prim::GetAttr[name="weight"](%o_proj.53)
  %hidden_states.797 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.209, %weight.481, %41), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn/__module.model.layers.26.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.799 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4004, %hidden_states.797, %44), scope: __module.model/__module.model.layers.26 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.483 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.53)
  %hidden_states.801 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.799, %33, %46, %46, %41), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %4111 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.801, %38), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4112 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm
  %variance.107 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4111, %4112, %25, %41), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4114 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.107, %24, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %4115 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4114), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.803 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.801, %4115), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.805 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.803, %33, %46, %46, %41), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.211 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.483, %hidden_states.805), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %4119 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.211, %hidden_states.801)
  %4120 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4121 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4119)
  %down_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_339.Linear = prim::GetAttr[name="down_proj"](%mlp.53)
  %up_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_338.Linear = prim::GetAttr[name="up_proj"](%mlp.53)
  %gate_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_337.Linear = prim::GetAttr[name="gate_proj"](%mlp.53)
  %weight.485 : Tensor = prim::GetAttr[name="weight"](%gate_proj.53)
  %input.213 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%4120, %weight.485, %41), scope: __module.model/__module.model.layers.26/__module.model.layers.26.mlp/__module.model.layers.26.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4127 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.213), scope: __module.model/__module.model.layers.26/__module.model.layers.26.mlp/__module.model.layers.26.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.487 : Tensor = prim::GetAttr[name="weight"](%up_proj.53)
  %4129 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%4120, %weight.487, %41), scope: __module.model/__module.model.layers.26/__module.model.layers.26.mlp/__module.model.layers.26.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.215 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%4127, %4129), scope: __module.model/__module.model.layers.26/__module.model.layers.26.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.489 : Tensor = prim::GetAttr[name="weight"](%down_proj.53)
  %hidden_states.807 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.215, %weight.489, %41), scope: __module.model/__module.model.layers.26/__module.model.layers.26.mlp/__module.model.layers.26.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.809 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4121, %hidden_states.807, %44), scope: __module.model/__module.model.layers.26 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.55 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_354.Qwen2MLP = prim::GetAttr[name="mlp"](%_27)
  %post_attention_layernorm.55 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_356.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_27)
  %self_attn.55 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_349.Qwen2Attention = prim::GetAttr[name="self_attn"](%_27)
  %input_layernorm.55 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_355.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_27)
  %weight.491 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.55)
  %hidden_states.811 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.809, %33, %46, %46, %41), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %4140 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.811, %38), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4141 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm
  %variance.109 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4140, %4141, %25, %41), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4143 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.109, %24, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %4144 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4143), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.813 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.811, %4144), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.815 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.813, %33, %46, %46, %41), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.817 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.491, %hidden_states.815), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %4148 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.817, %hidden_states.811)
  %4149 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4150 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4148)
  %o_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_348.Linear = prim::GetAttr[name="o_proj"](%self_attn.55)
  %v_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_347.Linear = prim::GetAttr[name="v_proj"](%self_attn.55)
  %k_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_346.Linear = prim::GetAttr[name="k_proj"](%self_attn.55)
  %q_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_345.Linear = prim::GetAttr[name="q_proj"](%self_attn.55)
  %4155 : int = aten::size(%4149, %42), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %4156 : int = aten::size(%4149, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.163 : Tensor = prim::GetAttr[name="bias"](%q_proj.55)
  %weight.493 : Tensor = prim::GetAttr[name="weight"](%q_proj.55)
  %4159 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4149, %weight.493, %bias.163), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn/__module.model.layers.27.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4160 : int[] = prim::ListConstruct(%4155, %4156, %45, %30), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4161 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%4159, %4160), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.55 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4161, %44, %38), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.165 : Tensor = prim::GetAttr[name="bias"](%k_proj.55)
  %weight.495 : Tensor = prim::GetAttr[name="weight"](%k_proj.55)
  %4165 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4149, %weight.495, %bias.165), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn/__module.model.layers.27.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4166 : int[] = prim::ListConstruct(%4155, %4156, %45, %30), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4167 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4165, %4166), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.55 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4167, %44, %38), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.167 : Tensor = prim::GetAttr[name="bias"](%v_proj.55)
  %weight.497 : Tensor = prim::GetAttr[name="weight"](%v_proj.55)
  %4171 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4149, %weight.497, %bias.167), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn/__module.model.layers.27.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4172 : int[] = prim::ListConstruct(%4155, %4156, %45, %30), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4173 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4171, %4172), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.823 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4173, %44, %38), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.59 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.59 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %4177 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.55, %cos.59), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %4178 : int = aten::size(%q.55, %36), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4179 : Long(device=cpu) = prim::NumToTensor(%4178), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4180 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4179, %29), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4181 : int = aten::Int(%4180), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %x1.109 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.55, %36, %42, %4181, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4183 : int = aten::size(%q.55, %36), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4184 : Long(device=cpu) = prim::NumToTensor(%4183), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4185 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4184, %29), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4186 : int = aten::Int(%4185), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %x2.109 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.55, %36, %4186, %37, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4188 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.109), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4189 : Tensor[] = prim::ListConstruct(%4188, %x1.109), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4190 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4189, %45), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4191 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4190, %sin.59), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.55 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4177, %4191, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %4193 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.55, %cos.59), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %4194 : int = aten::size(%k.55, %36), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4195 : Long(device=cpu) = prim::NumToTensor(%4194), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4196 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4195, %29), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4197 : int = aten::Int(%4196), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %x1.111 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.55, %36, %42, %4197, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4199 : int = aten::size(%k.55, %36), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4200 : Long(device=cpu) = prim::NumToTensor(%4199), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4201 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4200, %29), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4202 : int = aten::Int(%4201), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %x2.111 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.55, %36, %4202, %37, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4204 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.111), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4205 : Tensor[] = prim::ListConstruct(%4204, %x1.111), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4206 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4205, %45), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4207 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4206, %sin.59), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.819 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4193, %4207, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %4209 : int = aten::size(%hidden_states.819, %42), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4210 : int = aten::size(%hidden_states.819, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.109 : Long(device=cpu) = prim::NumToTensor(%4210), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4212 : int = aten::size(%hidden_states.819, %38), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4213 : int = aten::size(%hidden_states.819, %36), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4214 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.819, %42, %42, %37, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4215 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4214, %44, %42, %37, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4216 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4215, %38), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4217 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4216, %36, %42, %37, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4218 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4217, %35, %42, %37, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4219 : int[] = prim::ListConstruct(%4209, %4210, %35, %4212, %4213), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %hidden_states.821 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4218, %4219, %46), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4221 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.109, %28), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4222 : int = aten::Int(%4221), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4223 : int[] = prim::ListConstruct(%4209, %4222, %4212, %4213), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %key.55 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.821, %4223), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4225 : int = aten::size(%hidden_states.823, %42), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4226 : int = aten::size(%hidden_states.823, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.111 : Long(device=cpu) = prim::NumToTensor(%4226), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4228 : int = aten::size(%hidden_states.823, %38), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4229 : int = aten::size(%hidden_states.823, %36), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4230 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.823, %42, %42, %37, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4231 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4230, %44, %42, %37, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4232 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4231, %38), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4233 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4232, %36, %42, %37, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4234 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4233, %35, %42, %37, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4235 : int[] = prim::ListConstruct(%4225, %4226, %35, %4228, %4229), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %hidden_states.825 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4234, %4235, %46), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4237 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.111, %28), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4238 : int = aten::Int(%4237), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4239 : int[] = prim::ListConstruct(%4225, %4238, %4228, %4229), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %value.55 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.825, %4239), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4241 : int = aten::size(%key.55, %38), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4242 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4243 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4242, %44, %42, %37, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4244 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4243, %38, %42, %37, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.61 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4244, %36, %42, %4241, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.109 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.55, %key.55, %value.55, %attention_mask.61, %27, %46, %26, %46), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %4247 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.109, %44, %38), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.111 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4247, %42), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %4249 : int[] = prim::ListConstruct(%4155, %4156, %45), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4250 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.111, %4249), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.217 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%4250, %42), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.499 : Tensor = prim::GetAttr[name="weight"](%o_proj.55)
  %hidden_states.827 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.217, %weight.499, %41), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn/__module.model.layers.27.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.829 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4150, %hidden_states.827, %44), scope: __module.model/__module.model.layers.27 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.501 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.55)
  %hidden_states.831 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.829, %33, %46, %46, %41), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %4257 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.831, %38), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4258 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm
  %variance.111 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4257, %4258, %25, %41), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4260 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.111, %24, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %4261 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4260), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.833 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.831, %4261), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.835 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.833, %33, %46, %46, %41), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.219 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.501, %hidden_states.835), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %4265 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.219, %hidden_states.831)
  %4266 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4267 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4265)
  %down_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_352.Linear = prim::GetAttr[name="down_proj"](%mlp.55)
  %up_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_351.Linear = prim::GetAttr[name="up_proj"](%mlp.55)
  %gate_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_350.Linear = prim::GetAttr[name="gate_proj"](%mlp.55)
  %weight.503 : Tensor = prim::GetAttr[name="weight"](%gate_proj.55)
  %input.221 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%4266, %weight.503, %41), scope: __module.model/__module.model.layers.27/__module.model.layers.27.mlp/__module.model.layers.27.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4273 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.221), scope: __module.model/__module.model.layers.27/__module.model.layers.27.mlp/__module.model.layers.27.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.505 : Tensor = prim::GetAttr[name="weight"](%up_proj.55)
  %4275 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%4266, %weight.505, %41), scope: __module.model/__module.model.layers.27/__module.model.layers.27.mlp/__module.model.layers.27.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.223 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%4273, %4275), scope: __module.model/__module.model.layers.27/__module.model.layers.27.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.507 : Tensor = prim::GetAttr[name="weight"](%down_proj.55)
  %hidden_states.837 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.223, %weight.507, %41), scope: __module.model/__module.model.layers.27/__module.model.layers.27.mlp/__module.model.layers.27.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.839 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4267, %hidden_states.837, %44), scope: __module.model/__module.model.layers.27 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.57 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_367.Qwen2MLP = prim::GetAttr[name="mlp"](%_28)
  %post_attention_layernorm.57 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_369.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_28)
  %self_attn.57 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_362.Qwen2Attention = prim::GetAttr[name="self_attn"](%_28)
  %input_layernorm.57 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_368.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_28)
  %weight.509 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.57)
  %hidden_states.841 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.839, %33, %46, %46, %41), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %4286 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.841, %38), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4287 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm
  %variance.113 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4286, %4287, %25, %41), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4289 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.113, %24, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %4290 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4289), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.843 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.841, %4290), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.845 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.843, %33, %46, %46, %41), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.847 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.509, %hidden_states.845), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %4294 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.847, %hidden_states.841)
  %4295 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4296 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4294)
  %o_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_361.Linear = prim::GetAttr[name="o_proj"](%self_attn.57)
  %v_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_360.Linear = prim::GetAttr[name="v_proj"](%self_attn.57)
  %k_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_359.Linear = prim::GetAttr[name="k_proj"](%self_attn.57)
  %q_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_358.Linear = prim::GetAttr[name="q_proj"](%self_attn.57)
  %4301 : int = aten::size(%4295, %42), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %4302 : int = aten::size(%4295, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.169 : Tensor = prim::GetAttr[name="bias"](%q_proj.57)
  %weight.511 : Tensor = prim::GetAttr[name="weight"](%q_proj.57)
  %4305 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4295, %weight.511, %bias.169), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn/__module.model.layers.28.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4306 : int[] = prim::ListConstruct(%4301, %4302, %45, %30), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4307 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%4305, %4306), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.57 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4307, %44, %38), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.171 : Tensor = prim::GetAttr[name="bias"](%k_proj.57)
  %weight.513 : Tensor = prim::GetAttr[name="weight"](%k_proj.57)
  %4311 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4295, %weight.513, %bias.171), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn/__module.model.layers.28.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4312 : int[] = prim::ListConstruct(%4301, %4302, %45, %30), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4313 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4311, %4312), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.57 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4313, %44, %38), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.173 : Tensor = prim::GetAttr[name="bias"](%v_proj.57)
  %weight.515 : Tensor = prim::GetAttr[name="weight"](%v_proj.57)
  %4317 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4295, %weight.515, %bias.173), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn/__module.model.layers.28.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4318 : int[] = prim::ListConstruct(%4301, %4302, %45, %30), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4319 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4317, %4318), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.853 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4319, %44, %38), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.61 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.61 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %4323 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.57, %cos.61), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %4324 : int = aten::size(%q.57, %36), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4325 : Long(device=cpu) = prim::NumToTensor(%4324), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4326 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4325, %29), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4327 : int = aten::Int(%4326), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %x1.113 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.57, %36, %42, %4327, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4329 : int = aten::size(%q.57, %36), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4330 : Long(device=cpu) = prim::NumToTensor(%4329), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4331 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4330, %29), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4332 : int = aten::Int(%4331), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %x2.113 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.57, %36, %4332, %37, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4334 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.113), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4335 : Tensor[] = prim::ListConstruct(%4334, %x1.113), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4336 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4335, %45), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4337 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4336, %sin.61), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.57 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4323, %4337, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %4339 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.57, %cos.61), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %4340 : int = aten::size(%k.57, %36), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4341 : Long(device=cpu) = prim::NumToTensor(%4340), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4342 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4341, %29), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4343 : int = aten::Int(%4342), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %x1.115 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.57, %36, %42, %4343, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4345 : int = aten::size(%k.57, %36), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4346 : Long(device=cpu) = prim::NumToTensor(%4345), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4347 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4346, %29), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4348 : int = aten::Int(%4347), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %x2.115 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.57, %36, %4348, %37, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4350 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.115), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4351 : Tensor[] = prim::ListConstruct(%4350, %x1.115), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4352 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4351, %45), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4353 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4352, %sin.61), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.849 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4339, %4353, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %4355 : int = aten::size(%hidden_states.849, %42), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4356 : int = aten::size(%hidden_states.849, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.113 : Long(device=cpu) = prim::NumToTensor(%4356), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4358 : int = aten::size(%hidden_states.849, %38), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4359 : int = aten::size(%hidden_states.849, %36), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4360 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.849, %42, %42, %37, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4361 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4360, %44, %42, %37, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4362 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4361, %38), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4363 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4362, %36, %42, %37, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4364 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4363, %35, %42, %37, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4365 : int[] = prim::ListConstruct(%4355, %4356, %35, %4358, %4359), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %hidden_states.851 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4364, %4365, %46), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4367 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.113, %28), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4368 : int = aten::Int(%4367), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4369 : int[] = prim::ListConstruct(%4355, %4368, %4358, %4359), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %key.57 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.851, %4369), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4371 : int = aten::size(%hidden_states.853, %42), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4372 : int = aten::size(%hidden_states.853, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.115 : Long(device=cpu) = prim::NumToTensor(%4372), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4374 : int = aten::size(%hidden_states.853, %38), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4375 : int = aten::size(%hidden_states.853, %36), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4376 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.853, %42, %42, %37, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4377 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4376, %44, %42, %37, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4378 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4377, %38), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4379 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4378, %36, %42, %37, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4380 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4379, %35, %42, %37, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4381 : int[] = prim::ListConstruct(%4371, %4372, %35, %4374, %4375), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %hidden_states.855 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4380, %4381, %46), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4383 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.115, %28), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4384 : int = aten::Int(%4383), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4385 : int[] = prim::ListConstruct(%4371, %4384, %4374, %4375), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %value.57 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.855, %4385), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4387 : int = aten::size(%key.57, %38), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4388 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4389 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4388, %44, %42, %37, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4390 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4389, %38, %42, %37, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.63 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4390, %36, %42, %4387, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.113 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.57, %key.57, %value.57, %attention_mask.63, %27, %46, %26, %46), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %4393 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.113, %44, %38), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.115 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4393, %42), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %4395 : int[] = prim::ListConstruct(%4301, %4302, %45), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4396 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.115, %4395), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.225 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%4396, %42), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.517 : Tensor = prim::GetAttr[name="weight"](%o_proj.57)
  %hidden_states.857 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.225, %weight.517, %41), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn/__module.model.layers.28.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.859 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4296, %hidden_states.857, %44), scope: __module.model/__module.model.layers.28 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.519 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.57)
  %hidden_states.861 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.859, %33, %46, %46, %41), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %4403 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.861, %38), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4404 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm
  %variance.115 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4403, %4404, %25, %41), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4406 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.115, %24, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %4407 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4406), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.863 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.861, %4407), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.865 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.863, %33, %46, %46, %41), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.227 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.519, %hidden_states.865), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %4411 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.227, %hidden_states.861)
  %4412 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4413 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4411)
  %down_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_365.Linear = prim::GetAttr[name="down_proj"](%mlp.57)
  %up_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_364.Linear = prim::GetAttr[name="up_proj"](%mlp.57)
  %gate_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_363.Linear = prim::GetAttr[name="gate_proj"](%mlp.57)
  %weight.521 : Tensor = prim::GetAttr[name="weight"](%gate_proj.57)
  %input.229 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%4412, %weight.521, %41), scope: __module.model/__module.model.layers.28/__module.model.layers.28.mlp/__module.model.layers.28.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4419 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.229), scope: __module.model/__module.model.layers.28/__module.model.layers.28.mlp/__module.model.layers.28.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.523 : Tensor = prim::GetAttr[name="weight"](%up_proj.57)
  %4421 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%4412, %weight.523, %41), scope: __module.model/__module.model.layers.28/__module.model.layers.28.mlp/__module.model.layers.28.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.231 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%4419, %4421), scope: __module.model/__module.model.layers.28/__module.model.layers.28.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.525 : Tensor = prim::GetAttr[name="weight"](%down_proj.57)
  %hidden_states.867 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.231, %weight.525, %41), scope: __module.model/__module.model.layers.28/__module.model.layers.28.mlp/__module.model.layers.28.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.869 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4413, %hidden_states.867, %44), scope: __module.model/__module.model.layers.28 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.59 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_380.Qwen2MLP = prim::GetAttr[name="mlp"](%_29)
  %post_attention_layernorm.59 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_382.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_29)
  %self_attn.59 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_375.Qwen2Attention = prim::GetAttr[name="self_attn"](%_29)
  %input_layernorm.59 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_381.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_29)
  %weight.527 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.59)
  %hidden_states.871 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.869, %33, %46, %46, %41), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %4432 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.871, %38), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4433 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm
  %variance.117 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4432, %4433, %25, %41), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4435 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.117, %24, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %4436 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4435), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.873 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.871, %4436), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.875 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.873, %33, %46, %46, %41), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.877 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.527, %hidden_states.875), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %4440 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.877, %hidden_states.871)
  %4441 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4442 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4440)
  %o_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_374.Linear = prim::GetAttr[name="o_proj"](%self_attn.59)
  %v_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_373.Linear = prim::GetAttr[name="v_proj"](%self_attn.59)
  %k_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_372.Linear = prim::GetAttr[name="k_proj"](%self_attn.59)
  %q_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_371.Linear = prim::GetAttr[name="q_proj"](%self_attn.59)
  %4447 : int = aten::size(%4441, %42), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %4448 : int = aten::size(%4441, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.175 : Tensor = prim::GetAttr[name="bias"](%q_proj.59)
  %weight.529 : Tensor = prim::GetAttr[name="weight"](%q_proj.59)
  %4451 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4441, %weight.529, %bias.175), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn/__module.model.layers.29.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4452 : int[] = prim::ListConstruct(%4447, %4448, %45, %30), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4453 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%4451, %4452), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.59 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4453, %44, %38), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.177 : Tensor = prim::GetAttr[name="bias"](%k_proj.59)
  %weight.531 : Tensor = prim::GetAttr[name="weight"](%k_proj.59)
  %4457 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4441, %weight.531, %bias.177), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn/__module.model.layers.29.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4458 : int[] = prim::ListConstruct(%4447, %4448, %45, %30), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4459 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4457, %4458), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.59 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4459, %44, %38), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.179 : Tensor = prim::GetAttr[name="bias"](%v_proj.59)
  %weight.533 : Tensor = prim::GetAttr[name="weight"](%v_proj.59)
  %4463 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4441, %weight.533, %bias.179), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn/__module.model.layers.29.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4464 : int[] = prim::ListConstruct(%4447, %4448, %45, %30), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4465 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4463, %4464), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.883 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4465, %44, %38), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.63 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.63 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %4469 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.59, %cos.63), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %4470 : int = aten::size(%q.59, %36), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4471 : Long(device=cpu) = prim::NumToTensor(%4470), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4472 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4471, %29), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4473 : int = aten::Int(%4472), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %x1.117 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.59, %36, %42, %4473, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4475 : int = aten::size(%q.59, %36), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4476 : Long(device=cpu) = prim::NumToTensor(%4475), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4477 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4476, %29), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4478 : int = aten::Int(%4477), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %x2.117 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.59, %36, %4478, %37, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4480 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.117), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4481 : Tensor[] = prim::ListConstruct(%4480, %x1.117), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4482 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4481, %45), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4483 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4482, %sin.63), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.59 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4469, %4483, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %4485 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.59, %cos.63), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %4486 : int = aten::size(%k.59, %36), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4487 : Long(device=cpu) = prim::NumToTensor(%4486), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4488 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4487, %29), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4489 : int = aten::Int(%4488), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %x1.119 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.59, %36, %42, %4489, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4491 : int = aten::size(%k.59, %36), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4492 : Long(device=cpu) = prim::NumToTensor(%4491), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4493 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4492, %29), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4494 : int = aten::Int(%4493), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %x2.119 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.59, %36, %4494, %37, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4496 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.119), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4497 : Tensor[] = prim::ListConstruct(%4496, %x1.119), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4498 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4497, %45), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4499 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4498, %sin.63), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.879 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4485, %4499, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %4501 : int = aten::size(%hidden_states.879, %42), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4502 : int = aten::size(%hidden_states.879, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.117 : Long(device=cpu) = prim::NumToTensor(%4502), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4504 : int = aten::size(%hidden_states.879, %38), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4505 : int = aten::size(%hidden_states.879, %36), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4506 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.879, %42, %42, %37, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4507 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4506, %44, %42, %37, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4508 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4507, %38), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4509 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4508, %36, %42, %37, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4510 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4509, %35, %42, %37, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4511 : int[] = prim::ListConstruct(%4501, %4502, %35, %4504, %4505), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %hidden_states.881 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4510, %4511, %46), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4513 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.117, %28), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4514 : int = aten::Int(%4513), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4515 : int[] = prim::ListConstruct(%4501, %4514, %4504, %4505), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %key.59 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.881, %4515), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4517 : int = aten::size(%hidden_states.883, %42), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4518 : int = aten::size(%hidden_states.883, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.119 : Long(device=cpu) = prim::NumToTensor(%4518), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4520 : int = aten::size(%hidden_states.883, %38), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4521 : int = aten::size(%hidden_states.883, %36), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4522 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.883, %42, %42, %37, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4523 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4522, %44, %42, %37, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4524 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4523, %38), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4525 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4524, %36, %42, %37, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4526 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4525, %35, %42, %37, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4527 : int[] = prim::ListConstruct(%4517, %4518, %35, %4520, %4521), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %hidden_states.885 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4526, %4527, %46), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4529 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.119, %28), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4530 : int = aten::Int(%4529), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4531 : int[] = prim::ListConstruct(%4517, %4530, %4520, %4521), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %value.59 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.885, %4531), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4533 : int = aten::size(%key.59, %38), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4534 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4535 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4534, %44, %42, %37, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4536 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4535, %38, %42, %37, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.65 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4536, %36, %42, %4533, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.117 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.59, %key.59, %value.59, %attention_mask.65, %27, %46, %26, %46), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %4539 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.117, %44, %38), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.119 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4539, %42), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %4541 : int[] = prim::ListConstruct(%4447, %4448, %45), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4542 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.119, %4541), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.233 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%4542, %42), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.535 : Tensor = prim::GetAttr[name="weight"](%o_proj.59)
  %hidden_states.887 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.233, %weight.535, %41), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn/__module.model.layers.29.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.889 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4442, %hidden_states.887, %44), scope: __module.model/__module.model.layers.29 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.537 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.59)
  %hidden_states.891 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.889, %33, %46, %46, %41), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %4549 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.891, %38), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4550 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm
  %variance.119 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4549, %4550, %25, %41), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4552 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.119, %24, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %4553 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4552), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.893 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.891, %4553), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.895 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.893, %33, %46, %46, %41), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.235 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.537, %hidden_states.895), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %4557 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.235, %hidden_states.891)
  %4558 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4559 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4557)
  %down_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_378.Linear = prim::GetAttr[name="down_proj"](%mlp.59)
  %up_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_377.Linear = prim::GetAttr[name="up_proj"](%mlp.59)
  %gate_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_376.Linear = prim::GetAttr[name="gate_proj"](%mlp.59)
  %weight.539 : Tensor = prim::GetAttr[name="weight"](%gate_proj.59)
  %input.237 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%4558, %weight.539, %41), scope: __module.model/__module.model.layers.29/__module.model.layers.29.mlp/__module.model.layers.29.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4565 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.237), scope: __module.model/__module.model.layers.29/__module.model.layers.29.mlp/__module.model.layers.29.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.541 : Tensor = prim::GetAttr[name="weight"](%up_proj.59)
  %4567 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%4558, %weight.541, %41), scope: __module.model/__module.model.layers.29/__module.model.layers.29.mlp/__module.model.layers.29.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.239 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%4565, %4567), scope: __module.model/__module.model.layers.29/__module.model.layers.29.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.543 : Tensor = prim::GetAttr[name="weight"](%down_proj.59)
  %hidden_states.897 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.239, %weight.543, %41), scope: __module.model/__module.model.layers.29/__module.model.layers.29.mlp/__module.model.layers.29.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.899 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4559, %hidden_states.897, %44), scope: __module.model/__module.model.layers.29 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.61 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_393.Qwen2MLP = prim::GetAttr[name="mlp"](%_30)
  %post_attention_layernorm.61 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_395.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_30)
  %self_attn.61 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_388.Qwen2Attention = prim::GetAttr[name="self_attn"](%_30)
  %input_layernorm.61 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_394.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_30)
  %weight.545 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.61)
  %hidden_states.901 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.899, %33, %46, %46, %41), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %4578 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.901, %38), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4579 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm
  %variance.121 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4578, %4579, %25, %41), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4581 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.121, %24, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %4582 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4581), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.903 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.901, %4582), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.905 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.903, %33, %46, %46, %41), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.907 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.545, %hidden_states.905), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %4586 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.907, %hidden_states.901)
  %4587 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4588 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4586)
  %o_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_387.Linear = prim::GetAttr[name="o_proj"](%self_attn.61)
  %v_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_386.Linear = prim::GetAttr[name="v_proj"](%self_attn.61)
  %k_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_385.Linear = prim::GetAttr[name="k_proj"](%self_attn.61)
  %q_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_384.Linear = prim::GetAttr[name="q_proj"](%self_attn.61)
  %4593 : int = aten::size(%4587, %42), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %4594 : int = aten::size(%4587, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.181 : Tensor = prim::GetAttr[name="bias"](%q_proj.61)
  %weight.547 : Tensor = prim::GetAttr[name="weight"](%q_proj.61)
  %4597 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4587, %weight.547, %bias.181), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn/__module.model.layers.30.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4598 : int[] = prim::ListConstruct(%4593, %4594, %45, %30), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4599 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%4597, %4598), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.61 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4599, %44, %38), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.183 : Tensor = prim::GetAttr[name="bias"](%k_proj.61)
  %weight.549 : Tensor = prim::GetAttr[name="weight"](%k_proj.61)
  %4603 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4587, %weight.549, %bias.183), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn/__module.model.layers.30.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4604 : int[] = prim::ListConstruct(%4593, %4594, %45, %30), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4605 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4603, %4604), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.61 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4605, %44, %38), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.185 : Tensor = prim::GetAttr[name="bias"](%v_proj.61)
  %weight.551 : Tensor = prim::GetAttr[name="weight"](%v_proj.61)
  %4609 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4587, %weight.551, %bias.185), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn/__module.model.layers.30.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4610 : int[] = prim::ListConstruct(%4593, %4594, %45, %30), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4611 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4609, %4610), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.913 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4611, %44, %38), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.65 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.65 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %4615 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.61, %cos.65), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %4616 : int = aten::size(%q.61, %36), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4617 : Long(device=cpu) = prim::NumToTensor(%4616), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4618 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4617, %29), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4619 : int = aten::Int(%4618), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %x1.121 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.61, %36, %42, %4619, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4621 : int = aten::size(%q.61, %36), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4622 : Long(device=cpu) = prim::NumToTensor(%4621), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4623 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4622, %29), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4624 : int = aten::Int(%4623), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %x2.121 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.61, %36, %4624, %37, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4626 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.121), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4627 : Tensor[] = prim::ListConstruct(%4626, %x1.121), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4628 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4627, %45), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4629 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4628, %sin.65), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.61 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4615, %4629, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %4631 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.61, %cos.65), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %4632 : int = aten::size(%k.61, %36), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4633 : Long(device=cpu) = prim::NumToTensor(%4632), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4634 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4633, %29), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4635 : int = aten::Int(%4634), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %x1.123 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.61, %36, %42, %4635, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4637 : int = aten::size(%k.61, %36), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4638 : Long(device=cpu) = prim::NumToTensor(%4637), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4639 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4638, %29), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4640 : int = aten::Int(%4639), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %x2.123 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.61, %36, %4640, %37, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4642 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.123), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4643 : Tensor[] = prim::ListConstruct(%4642, %x1.123), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4644 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4643, %45), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4645 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4644, %sin.65), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.909 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4631, %4645, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %4647 : int = aten::size(%hidden_states.909, %42), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4648 : int = aten::size(%hidden_states.909, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.121 : Long(device=cpu) = prim::NumToTensor(%4648), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4650 : int = aten::size(%hidden_states.909, %38), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4651 : int = aten::size(%hidden_states.909, %36), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4652 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.909, %42, %42, %37, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4653 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4652, %44, %42, %37, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4654 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4653, %38), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4655 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4654, %36, %42, %37, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4656 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4655, %35, %42, %37, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4657 : int[] = prim::ListConstruct(%4647, %4648, %35, %4650, %4651), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %hidden_states.911 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4656, %4657, %46), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4659 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.121, %28), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4660 : int = aten::Int(%4659), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4661 : int[] = prim::ListConstruct(%4647, %4660, %4650, %4651), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %key.61 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.911, %4661), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4663 : int = aten::size(%hidden_states.913, %42), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4664 : int = aten::size(%hidden_states.913, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.123 : Long(device=cpu) = prim::NumToTensor(%4664), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4666 : int = aten::size(%hidden_states.913, %38), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4667 : int = aten::size(%hidden_states.913, %36), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4668 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.913, %42, %42, %37, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4669 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4668, %44, %42, %37, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4670 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4669, %38), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4671 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4670, %36, %42, %37, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4672 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4671, %35, %42, %37, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4673 : int[] = prim::ListConstruct(%4663, %4664, %35, %4666, %4667), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %hidden_states.915 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4672, %4673, %46), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4675 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.123, %28), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4676 : int = aten::Int(%4675), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4677 : int[] = prim::ListConstruct(%4663, %4676, %4666, %4667), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %value.61 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.915, %4677), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4679 : int = aten::size(%key.61, %38), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4680 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4681 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4680, %44, %42, %37, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4682 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4681, %38, %42, %37, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.67 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4682, %36, %42, %4679, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.121 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.61, %key.61, %value.61, %attention_mask.67, %27, %46, %26, %46), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %4685 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.121, %44, %38), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.123 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4685, %42), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %4687 : int[] = prim::ListConstruct(%4593, %4594, %45), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4688 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.123, %4687), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.241 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%4688, %42), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.553 : Tensor = prim::GetAttr[name="weight"](%o_proj.61)
  %hidden_states.917 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.241, %weight.553, %41), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn/__module.model.layers.30.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.919 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4588, %hidden_states.917, %44), scope: __module.model/__module.model.layers.30 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.555 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.61)
  %hidden_states.921 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.919, %33, %46, %46, %41), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %4695 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.921, %38), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4696 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm
  %variance.123 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4695, %4696, %25, %41), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4698 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.123, %24, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %4699 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4698), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.923 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.921, %4699), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.925 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.923, %33, %46, %46, %41), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.243 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.555, %hidden_states.925), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %4703 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.243, %hidden_states.921)
  %4704 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4705 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4703)
  %down_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_391.Linear = prim::GetAttr[name="down_proj"](%mlp.61)
  %up_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_390.Linear = prim::GetAttr[name="up_proj"](%mlp.61)
  %gate_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_389.Linear = prim::GetAttr[name="gate_proj"](%mlp.61)
  %weight.557 : Tensor = prim::GetAttr[name="weight"](%gate_proj.61)
  %input.245 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%4704, %weight.557, %41), scope: __module.model/__module.model.layers.30/__module.model.layers.30.mlp/__module.model.layers.30.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4711 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.245), scope: __module.model/__module.model.layers.30/__module.model.layers.30.mlp/__module.model.layers.30.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.559 : Tensor = prim::GetAttr[name="weight"](%up_proj.61)
  %4713 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%4704, %weight.559, %41), scope: __module.model/__module.model.layers.30/__module.model.layers.30.mlp/__module.model.layers.30.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.247 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%4711, %4713), scope: __module.model/__module.model.layers.30/__module.model.layers.30.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.561 : Tensor = prim::GetAttr[name="weight"](%down_proj.61)
  %hidden_states.927 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.247, %weight.561, %41), scope: __module.model/__module.model.layers.30/__module.model.layers.30.mlp/__module.model.layers.30.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.929 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4705, %hidden_states.927, %44), scope: __module.model/__module.model.layers.30 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.63 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_406.Qwen2MLP = prim::GetAttr[name="mlp"](%_31)
  %post_attention_layernorm.63 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_408.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_31)
  %self_attn.63 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_401.Qwen2Attention = prim::GetAttr[name="self_attn"](%_31)
  %input_layernorm.63 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_407.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_31)
  %weight.563 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.63)
  %hidden_states.931 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.929, %33, %46, %46, %41), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %4724 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.931, %38), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4725 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm
  %variance.125 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4724, %4725, %25, %41), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4727 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.125, %24, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %4728 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4727), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.933 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.931, %4728), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.935 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.933, %33, %46, %46, %41), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.937 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.563, %hidden_states.935), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %4732 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.937, %hidden_states.931)
  %4733 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4734 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4732)
  %o_proj.63 : __torch__.torch.nn.modules.linear.___torch_mangle_400.Linear = prim::GetAttr[name="o_proj"](%self_attn.63)
  %v_proj.63 : __torch__.torch.nn.modules.linear.___torch_mangle_399.Linear = prim::GetAttr[name="v_proj"](%self_attn.63)
  %k_proj.63 : __torch__.torch.nn.modules.linear.___torch_mangle_398.Linear = prim::GetAttr[name="k_proj"](%self_attn.63)
  %q_proj.63 : __torch__.torch.nn.modules.linear.___torch_mangle_397.Linear = prim::GetAttr[name="q_proj"](%self_attn.63)
  %4739 : int = aten::size(%4733, %42), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %4740 : int = aten::size(%4733, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.187 : Tensor = prim::GetAttr[name="bias"](%q_proj.63)
  %weight.565 : Tensor = prim::GetAttr[name="weight"](%q_proj.63)
  %4743 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4733, %weight.565, %bias.187), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn/__module.model.layers.31.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4744 : int[] = prim::ListConstruct(%4739, %4740, %45, %30), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4745 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%4743, %4744), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.63 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4745, %44, %38), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.189 : Tensor = prim::GetAttr[name="bias"](%k_proj.63)
  %weight.567 : Tensor = prim::GetAttr[name="weight"](%k_proj.63)
  %4749 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4733, %weight.567, %bias.189), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn/__module.model.layers.31.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4750 : int[] = prim::ListConstruct(%4739, %4740, %45, %30), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4751 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4749, %4750), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.63 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4751, %44, %38), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.191 : Tensor = prim::GetAttr[name="bias"](%v_proj.63)
  %weight.569 : Tensor = prim::GetAttr[name="weight"](%v_proj.63)
  %4755 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4733, %weight.569, %bias.191), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn/__module.model.layers.31.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4756 : int[] = prim::ListConstruct(%4739, %4740, %45, %30), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4757 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4755, %4756), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.943 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4757, %44, %38), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.67 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.67 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %4761 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.63, %cos.67), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %4762 : int = aten::size(%q.63, %36), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4763 : Long(device=cpu) = prim::NumToTensor(%4762), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4764 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4763, %29), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4765 : int = aten::Int(%4764), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %x1.125 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.63, %36, %42, %4765, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4767 : int = aten::size(%q.63, %36), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4768 : Long(device=cpu) = prim::NumToTensor(%4767), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4769 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4768, %29), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4770 : int = aten::Int(%4769), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %x2.125 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.63, %36, %4770, %37, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4772 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.125), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4773 : Tensor[] = prim::ListConstruct(%4772, %x1.125), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4774 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4773, %45), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4775 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4774, %sin.67), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.63 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4761, %4775, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %4777 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.63, %cos.67), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %4778 : int = aten::size(%k.63, %36), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4779 : Long(device=cpu) = prim::NumToTensor(%4778), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4780 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4779, %29), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4781 : int = aten::Int(%4780), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %x1.127 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.63, %36, %42, %4781, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4783 : int = aten::size(%k.63, %36), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4784 : Long(device=cpu) = prim::NumToTensor(%4783), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4785 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4784, %29), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4786 : int = aten::Int(%4785), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %x2.127 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.63, %36, %4786, %37, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4788 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.127), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4789 : Tensor[] = prim::ListConstruct(%4788, %x1.127), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4790 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4789, %45), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4791 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4790, %sin.67), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.939 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4777, %4791, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %4793 : int = aten::size(%hidden_states.939, %42), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4794 : int = aten::size(%hidden_states.939, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.125 : Long(device=cpu) = prim::NumToTensor(%4794), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4796 : int = aten::size(%hidden_states.939, %38), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4797 : int = aten::size(%hidden_states.939, %36), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4798 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.939, %42, %42, %37, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4799 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4798, %44, %42, %37, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4800 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4799, %38), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4801 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4800, %36, %42, %37, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4802 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4801, %35, %42, %37, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4803 : int[] = prim::ListConstruct(%4793, %4794, %35, %4796, %4797), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %hidden_states.941 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4802, %4803, %46), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4805 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.125, %28), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4806 : int = aten::Int(%4805), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4807 : int[] = prim::ListConstruct(%4793, %4806, %4796, %4797), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %key.63 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.941, %4807), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4809 : int = aten::size(%hidden_states.943, %42), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4810 : int = aten::size(%hidden_states.943, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.127 : Long(device=cpu) = prim::NumToTensor(%4810), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4812 : int = aten::size(%hidden_states.943, %38), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4813 : int = aten::size(%hidden_states.943, %36), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4814 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.943, %42, %42, %37, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4815 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4814, %44, %42, %37, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4816 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4815, %38), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4817 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4816, %36, %42, %37, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4818 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4817, %35, %42, %37, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4819 : int[] = prim::ListConstruct(%4809, %4810, %35, %4812, %4813), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %hidden_states.945 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4818, %4819, %46), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4821 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.127, %28), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4822 : int = aten::Int(%4821), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4823 : int[] = prim::ListConstruct(%4809, %4822, %4812, %4813), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %value.63 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.945, %4823), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4825 : int = aten::size(%key.63, %38), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4826 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4827 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4826, %44, %42, %37, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4828 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4827, %38, %42, %37, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.69 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4828, %36, %42, %4825, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.125 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.63, %key.63, %value.63, %attention_mask.69, %27, %46, %26, %46), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %4831 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.125, %44, %38), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.127 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4831, %42), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %4833 : int[] = prim::ListConstruct(%4739, %4740, %45), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4834 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.127, %4833), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.249 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%4834, %42), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.571 : Tensor = prim::GetAttr[name="weight"](%o_proj.63)
  %hidden_states.947 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.249, %weight.571, %41), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn/__module.model.layers.31.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.949 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4734, %hidden_states.947, %44), scope: __module.model/__module.model.layers.31 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.573 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.63)
  %hidden_states.951 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.949, %33, %46, %46, %41), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %4841 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.951, %38), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4842 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm
  %variance.127 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4841, %4842, %25, %41), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4844 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.127, %24, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %4845 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4844), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.953 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.951, %4845), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.955 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.953, %33, %46, %46, %41), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.251 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.573, %hidden_states.955), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %4849 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.251, %hidden_states.951)
  %4850 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4851 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4849)
  %down_proj.63 : __torch__.torch.nn.modules.linear.___torch_mangle_404.Linear = prim::GetAttr[name="down_proj"](%mlp.63)
  %up_proj.63 : __torch__.torch.nn.modules.linear.___torch_mangle_403.Linear = prim::GetAttr[name="up_proj"](%mlp.63)
  %gate_proj.63 : __torch__.torch.nn.modules.linear.___torch_mangle_402.Linear = prim::GetAttr[name="gate_proj"](%mlp.63)
  %weight.575 : Tensor = prim::GetAttr[name="weight"](%gate_proj.63)
  %input.253 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%4850, %weight.575, %41), scope: __module.model/__module.model.layers.31/__module.model.layers.31.mlp/__module.model.layers.31.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4857 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.253), scope: __module.model/__module.model.layers.31/__module.model.layers.31.mlp/__module.model.layers.31.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.577 : Tensor = prim::GetAttr[name="weight"](%up_proj.63)
  %4859 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%4850, %weight.577, %41), scope: __module.model/__module.model.layers.31/__module.model.layers.31.mlp/__module.model.layers.31.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.255 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%4857, %4859), scope: __module.model/__module.model.layers.31/__module.model.layers.31.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.579 : Tensor = prim::GetAttr[name="weight"](%down_proj.63)
  %hidden_states.957 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.255, %weight.579, %41), scope: __module.model/__module.model.layers.31/__module.model.layers.31.mlp/__module.model.layers.31.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.959 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4851, %hidden_states.957, %44), scope: __module.model/__module.model.layers.31 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.65 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_419.Qwen2MLP = prim::GetAttr[name="mlp"](%_32)
  %post_attention_layernorm.65 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_421.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_32)
  %self_attn.65 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_414.Qwen2Attention = prim::GetAttr[name="self_attn"](%_32)
  %input_layernorm.65 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_420.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_32)
  %weight.581 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.65)
  %hidden_states.961 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.959, %33, %46, %46, %41), scope: __module.model/__module.model.layers.32/__module.model.layers.32.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %4870 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.961, %38), scope: __module.model/__module.model.layers.32/__module.model.layers.32.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4871 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.32/__module.model.layers.32.input_layernorm
  %variance.129 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4870, %4871, %25, %41), scope: __module.model/__module.model.layers.32/__module.model.layers.32.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4873 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.129, %24, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %4874 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4873), scope: __module.model/__module.model.layers.32/__module.model.layers.32.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.963 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.961, %4874), scope: __module.model/__module.model.layers.32/__module.model.layers.32.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.965 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.963, %33, %46, %46, %41), scope: __module.model/__module.model.layers.32/__module.model.layers.32.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.967 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.581, %hidden_states.965), scope: __module.model/__module.model.layers.32/__module.model.layers.32.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %4878 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.967, %hidden_states.961)
  %4879 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4880 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4878)
  %o_proj.65 : __torch__.torch.nn.modules.linear.___torch_mangle_413.Linear = prim::GetAttr[name="o_proj"](%self_attn.65)
  %v_proj.65 : __torch__.torch.nn.modules.linear.___torch_mangle_412.Linear = prim::GetAttr[name="v_proj"](%self_attn.65)
  %k_proj.65 : __torch__.torch.nn.modules.linear.___torch_mangle_411.Linear = prim::GetAttr[name="k_proj"](%self_attn.65)
  %q_proj.65 : __torch__.torch.nn.modules.linear.___torch_mangle_410.Linear = prim::GetAttr[name="q_proj"](%self_attn.65)
  %4885 : int = aten::size(%4879, %42), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %4886 : int = aten::size(%4879, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.193 : Tensor = prim::GetAttr[name="bias"](%q_proj.65)
  %weight.583 : Tensor = prim::GetAttr[name="weight"](%q_proj.65)
  %4889 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4879, %weight.583, %bias.193), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn/__module.model.layers.32.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4890 : int[] = prim::ListConstruct(%4885, %4886, %45, %30), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %4891 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%4889, %4890), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.65 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4891, %44, %38), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.195 : Tensor = prim::GetAttr[name="bias"](%k_proj.65)
  %weight.585 : Tensor = prim::GetAttr[name="weight"](%k_proj.65)
  %4895 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4879, %weight.585, %bias.195), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn/__module.model.layers.32.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4896 : int[] = prim::ListConstruct(%4885, %4886, %45, %30), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %4897 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4895, %4896), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.65 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4897, %44, %38), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.197 : Tensor = prim::GetAttr[name="bias"](%v_proj.65)
  %weight.587 : Tensor = prim::GetAttr[name="weight"](%v_proj.65)
  %4901 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4879, %weight.587, %bias.197), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn/__module.model.layers.32.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %4902 : int[] = prim::ListConstruct(%4885, %4886, %45, %30), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %4903 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4901, %4902), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.973 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4903, %44, %38), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.69 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.69 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %4907 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.65, %cos.69), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %4908 : int = aten::size(%q.65, %36), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4909 : Long(device=cpu) = prim::NumToTensor(%4908), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %4910 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4909, %29), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4911 : int = aten::Int(%4910), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %x1.129 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.65, %36, %42, %4911, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4913 : int = aten::size(%q.65, %36), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4914 : Long(device=cpu) = prim::NumToTensor(%4913), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %4915 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4914, %29), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4916 : int = aten::Int(%4915), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %x2.129 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.65, %36, %4916, %37, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4918 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.129), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4919 : Tensor[] = prim::ListConstruct(%4918, %x1.129), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %4920 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4919, %45), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4921 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4920, %sin.69), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.65 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4907, %4921, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %4923 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.65, %cos.69), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %4924 : int = aten::size(%k.65, %36), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4925 : Long(device=cpu) = prim::NumToTensor(%4924), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %4926 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4925, %29), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4927 : int = aten::Int(%4926), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %x1.131 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.65, %36, %42, %4927, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %4929 : int = aten::size(%k.65, %36), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4930 : Long(device=cpu) = prim::NumToTensor(%4929), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %4931 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4930, %29), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4932 : int = aten::Int(%4931), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %x2.131 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.65, %36, %4932, %37, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %4934 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.131), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4935 : Tensor[] = prim::ListConstruct(%4934, %x1.131), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %4936 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4935, %45), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %4937 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4936, %sin.69), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.969 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4923, %4937, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %4939 : int = aten::size(%hidden_states.969, %42), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4940 : int = aten::size(%hidden_states.969, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.129 : Long(device=cpu) = prim::NumToTensor(%4940), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %4942 : int = aten::size(%hidden_states.969, %38), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4943 : int = aten::size(%hidden_states.969, %36), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4944 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.969, %42, %42, %37, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4945 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4944, %44, %42, %37, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4946 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4945, %38), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4947 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4946, %36, %42, %37, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4948 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4947, %35, %42, %37, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4949 : int[] = prim::ListConstruct(%4939, %4940, %35, %4942, %4943), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %hidden_states.971 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4948, %4949, %46), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4951 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.129, %28), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4952 : int = aten::Int(%4951), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %4953 : int[] = prim::ListConstruct(%4939, %4952, %4942, %4943), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %key.65 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.971, %4953), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4955 : int = aten::size(%hidden_states.973, %42), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4956 : int = aten::size(%hidden_states.973, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.131 : Long(device=cpu) = prim::NumToTensor(%4956), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %4958 : int = aten::size(%hidden_states.973, %38), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4959 : int = aten::size(%hidden_states.973, %36), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4960 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.973, %42, %42, %37, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4961 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4960, %44, %42, %37, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4962 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4961, %38), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4963 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4962, %36, %42, %37, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4964 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4963, %35, %42, %37, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4965 : int[] = prim::ListConstruct(%4955, %4956, %35, %4958, %4959), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %hidden_states.975 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4964, %4965, %46), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4967 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.131, %28), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4968 : int = aten::Int(%4967), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %4969 : int[] = prim::ListConstruct(%4955, %4968, %4958, %4959), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %value.65 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.975, %4969), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4971 : int = aten::size(%key.65, %38), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4972 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4973 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4972, %44, %42, %37, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4974 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4973, %38, %42, %37, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.71 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4974, %36, %42, %4971, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.129 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.65, %key.65, %value.65, %attention_mask.71, %27, %46, %26, %46), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %4977 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.129, %44, %38), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.131 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4977, %42), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %4979 : int[] = prim::ListConstruct(%4885, %4886, %45), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn
  %4980 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.131, %4979), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.257 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%4980, %42), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.589 : Tensor = prim::GetAttr[name="weight"](%o_proj.65)
  %hidden_states.977 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.257, %weight.589, %41), scope: __module.model/__module.model.layers.32/__module.model.layers.32.self_attn/__module.model.layers.32.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.979 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4880, %hidden_states.977, %44), scope: __module.model/__module.model.layers.32 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.591 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.65)
  %hidden_states.981 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.979, %33, %46, %46, %41), scope: __module.model/__module.model.layers.32/__module.model.layers.32.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %4987 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.981, %38), scope: __module.model/__module.model.layers.32/__module.model.layers.32.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4988 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.32/__module.model.layers.32.post_attention_layernorm
  %variance.131 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4987, %4988, %25, %41), scope: __module.model/__module.model.layers.32/__module.model.layers.32.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %4990 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.131, %24, %44), scope: __module.model/__module.model.layers.32/__module.model.layers.32.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %4991 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4990), scope: __module.model/__module.model.layers.32/__module.model.layers.32.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.983 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.981, %4991), scope: __module.model/__module.model.layers.32/__module.model.layers.32.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.985 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.983, %33, %46, %46, %41), scope: __module.model/__module.model.layers.32/__module.model.layers.32.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.259 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.591, %hidden_states.985), scope: __module.model/__module.model.layers.32/__module.model.layers.32.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %4995 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.259, %hidden_states.981)
  %4996 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4997 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4995)
  %down_proj.65 : __torch__.torch.nn.modules.linear.___torch_mangle_417.Linear = prim::GetAttr[name="down_proj"](%mlp.65)
  %up_proj.65 : __torch__.torch.nn.modules.linear.___torch_mangle_416.Linear = prim::GetAttr[name="up_proj"](%mlp.65)
  %gate_proj.65 : __torch__.torch.nn.modules.linear.___torch_mangle_415.Linear = prim::GetAttr[name="gate_proj"](%mlp.65)
  %weight.593 : Tensor = prim::GetAttr[name="weight"](%gate_proj.65)
  %input.261 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%4996, %weight.593, %41), scope: __module.model/__module.model.layers.32/__module.model.layers.32.mlp/__module.model.layers.32.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %5003 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.261), scope: __module.model/__module.model.layers.32/__module.model.layers.32.mlp/__module.model.layers.32.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.595 : Tensor = prim::GetAttr[name="weight"](%up_proj.65)
  %5005 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%4996, %weight.595, %41), scope: __module.model/__module.model.layers.32/__module.model.layers.32.mlp/__module.model.layers.32.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.263 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%5003, %5005), scope: __module.model/__module.model.layers.32/__module.model.layers.32.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.597 : Tensor = prim::GetAttr[name="weight"](%down_proj.65)
  %hidden_states.987 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.263, %weight.597, %41), scope: __module.model/__module.model.layers.32/__module.model.layers.32.mlp/__module.model.layers.32.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.989 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4997, %hidden_states.987, %44), scope: __module.model/__module.model.layers.32 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.67 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_432.Qwen2MLP = prim::GetAttr[name="mlp"](%_33)
  %post_attention_layernorm.67 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_434.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_33)
  %self_attn.67 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_427.Qwen2Attention = prim::GetAttr[name="self_attn"](%_33)
  %input_layernorm.67 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_433.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_33)
  %weight.599 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.67)
  %hidden_states.991 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.989, %33, %46, %46, %41), scope: __module.model/__module.model.layers.33/__module.model.layers.33.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %5016 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.991, %38), scope: __module.model/__module.model.layers.33/__module.model.layers.33.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %5017 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.33/__module.model.layers.33.input_layernorm
  %variance.133 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%5016, %5017, %25, %41), scope: __module.model/__module.model.layers.33/__module.model.layers.33.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %5019 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.133, %24, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %5020 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%5019), scope: __module.model/__module.model.layers.33/__module.model.layers.33.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.993 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.991, %5020), scope: __module.model/__module.model.layers.33/__module.model.layers.33.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.995 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.993, %33, %46, %46, %41), scope: __module.model/__module.model.layers.33/__module.model.layers.33.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.997 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.599, %hidden_states.995), scope: __module.model/__module.model.layers.33/__module.model.layers.33.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %5024 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.997, %hidden_states.991)
  %5025 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5026 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5024)
  %o_proj.67 : __torch__.torch.nn.modules.linear.___torch_mangle_426.Linear = prim::GetAttr[name="o_proj"](%self_attn.67)
  %v_proj.67 : __torch__.torch.nn.modules.linear.___torch_mangle_425.Linear = prim::GetAttr[name="v_proj"](%self_attn.67)
  %k_proj.67 : __torch__.torch.nn.modules.linear.___torch_mangle_424.Linear = prim::GetAttr[name="k_proj"](%self_attn.67)
  %q_proj.67 : __torch__.torch.nn.modules.linear.___torch_mangle_423.Linear = prim::GetAttr[name="q_proj"](%self_attn.67)
  %5031 : int = aten::size(%5025, %42), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %5032 : int = aten::size(%5025, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.199 : Tensor = prim::GetAttr[name="bias"](%q_proj.67)
  %weight.601 : Tensor = prim::GetAttr[name="weight"](%q_proj.67)
  %5035 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%5025, %weight.601, %bias.199), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn/__module.model.layers.33.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %5036 : int[] = prim::ListConstruct(%5031, %5032, %45, %30), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %5037 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%5035, %5036), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.67 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%5037, %44, %38), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.201 : Tensor = prim::GetAttr[name="bias"](%k_proj.67)
  %weight.603 : Tensor = prim::GetAttr[name="weight"](%k_proj.67)
  %5041 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%5025, %weight.603, %bias.201), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn/__module.model.layers.33.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %5042 : int[] = prim::ListConstruct(%5031, %5032, %45, %30), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %5043 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%5041, %5042), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.67 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%5043, %44, %38), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.203 : Tensor = prim::GetAttr[name="bias"](%v_proj.67)
  %weight.605 : Tensor = prim::GetAttr[name="weight"](%v_proj.67)
  %5047 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%5025, %weight.605, %bias.203), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn/__module.model.layers.33.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %5048 : int[] = prim::ListConstruct(%5031, %5032, %45, %30), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %5049 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%5047, %5048), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.1003 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%5049, %44, %38), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.71 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.71 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %5053 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.67, %cos.71), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %5054 : int = aten::size(%q.67, %36), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %5055 : Long(device=cpu) = prim::NumToTensor(%5054), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %5056 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5055, %29), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %5057 : int = aten::Int(%5056), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %x1.133 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.67, %36, %42, %5057, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %5059 : int = aten::size(%q.67, %36), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %5060 : Long(device=cpu) = prim::NumToTensor(%5059), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %5061 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5060, %29), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %5062 : int = aten::Int(%5061), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %x2.133 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.67, %36, %5062, %37, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %5064 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.133), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %5065 : Tensor[] = prim::ListConstruct(%5064, %x1.133), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %5066 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5065, %45), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %5067 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%5066, %sin.71), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.67 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5053, %5067, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %5069 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.67, %cos.71), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %5070 : int = aten::size(%k.67, %36), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %5071 : Long(device=cpu) = prim::NumToTensor(%5070), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %5072 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5071, %29), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %5073 : int = aten::Int(%5072), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %x1.135 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.67, %36, %42, %5073, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %5075 : int = aten::size(%k.67, %36), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %5076 : Long(device=cpu) = prim::NumToTensor(%5075), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %5077 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5076, %29), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %5078 : int = aten::Int(%5077), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %x2.135 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.67, %36, %5078, %37, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %5080 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.135), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %5081 : Tensor[] = prim::ListConstruct(%5080, %x1.135), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %5082 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5081, %45), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %5083 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%5082, %sin.71), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.999 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%5069, %5083, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %5085 : int = aten::size(%hidden_states.999, %42), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5086 : int = aten::size(%hidden_states.999, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.133 : Long(device=cpu) = prim::NumToTensor(%5086), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %5088 : int = aten::size(%hidden_states.999, %38), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5089 : int = aten::size(%hidden_states.999, %36), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5090 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.999, %42, %42, %37, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5091 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%5090, %44, %42, %37, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5092 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%5091, %38), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5093 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%5092, %36, %42, %37, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5094 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%5093, %35, %42, %37, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5095 : int[] = prim::ListConstruct(%5085, %5086, %35, %5088, %5089), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %hidden_states.1001 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%5094, %5095, %46), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5097 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.133, %28), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %5098 : int = aten::Int(%5097), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %5099 : int[] = prim::ListConstruct(%5085, %5098, %5088, %5089), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %key.67 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.1001, %5099), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %5101 : int = aten::size(%hidden_states.1003, %42), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5102 : int = aten::size(%hidden_states.1003, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.135 : Long(device=cpu) = prim::NumToTensor(%5102), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %5104 : int = aten::size(%hidden_states.1003, %38), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5105 : int = aten::size(%hidden_states.1003, %36), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5106 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.1003, %42, %42, %37, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5107 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%5106, %44, %42, %37, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5108 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%5107, %38), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5109 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%5108, %36, %42, %37, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5110 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%5109, %35, %42, %37, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5111 : int[] = prim::ListConstruct(%5101, %5102, %35, %5104, %5105), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %hidden_states.1005 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%5110, %5111, %46), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5113 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.135, %28), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %5114 : int = aten::Int(%5113), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %5115 : int[] = prim::ListConstruct(%5101, %5114, %5104, %5105), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %value.67 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.1005, %5115), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %5117 : int = aten::size(%key.67, %38), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %5118 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %5119 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%5118, %44, %42, %37, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %5120 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%5119, %38, %42, %37, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.73 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%5120, %36, %42, %5117, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.133 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.67, %key.67, %value.67, %attention_mask.73, %27, %46, %26, %46), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %5123 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.133, %44, %38), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.135 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%5123, %42), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %5125 : int[] = prim::ListConstruct(%5031, %5032, %45), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn
  %5126 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.135, %5125), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.265 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%5126, %42), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.607 : Tensor = prim::GetAttr[name="weight"](%o_proj.67)
  %hidden_states.1007 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.265, %weight.607, %41), scope: __module.model/__module.model.layers.33/__module.model.layers.33.self_attn/__module.model.layers.33.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.1009 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5026, %hidden_states.1007, %44), scope: __module.model/__module.model.layers.33 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.609 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.67)
  %hidden_states.1011 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.1009, %33, %46, %46, %41), scope: __module.model/__module.model.layers.33/__module.model.layers.33.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %5133 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.1011, %38), scope: __module.model/__module.model.layers.33/__module.model.layers.33.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %5134 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.33/__module.model.layers.33.post_attention_layernorm
  %variance.135 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%5133, %5134, %25, %41), scope: __module.model/__module.model.layers.33/__module.model.layers.33.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %5136 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.135, %24, %44), scope: __module.model/__module.model.layers.33/__module.model.layers.33.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %5137 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%5136), scope: __module.model/__module.model.layers.33/__module.model.layers.33.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.1013 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.1011, %5137), scope: __module.model/__module.model.layers.33/__module.model.layers.33.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.1015 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.1013, %33, %46, %46, %41), scope: __module.model/__module.model.layers.33/__module.model.layers.33.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.267 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.609, %hidden_states.1015), scope: __module.model/__module.model.layers.33/__module.model.layers.33.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %5141 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.267, %hidden_states.1011)
  %5142 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5143 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5141)
  %down_proj.67 : __torch__.torch.nn.modules.linear.___torch_mangle_430.Linear = prim::GetAttr[name="down_proj"](%mlp.67)
  %up_proj.67 : __torch__.torch.nn.modules.linear.___torch_mangle_429.Linear = prim::GetAttr[name="up_proj"](%mlp.67)
  %gate_proj.67 : __torch__.torch.nn.modules.linear.___torch_mangle_428.Linear = prim::GetAttr[name="gate_proj"](%mlp.67)
  %weight.611 : Tensor = prim::GetAttr[name="weight"](%gate_proj.67)
  %input.269 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%5142, %weight.611, %41), scope: __module.model/__module.model.layers.33/__module.model.layers.33.mlp/__module.model.layers.33.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %5149 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.269), scope: __module.model/__module.model.layers.33/__module.model.layers.33.mlp/__module.model.layers.33.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.613 : Tensor = prim::GetAttr[name="weight"](%up_proj.67)
  %5151 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%5142, %weight.613, %41), scope: __module.model/__module.model.layers.33/__module.model.layers.33.mlp/__module.model.layers.33.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.271 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%5149, %5151), scope: __module.model/__module.model.layers.33/__module.model.layers.33.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.615 : Tensor = prim::GetAttr[name="weight"](%down_proj.67)
  %hidden_states.1017 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.271, %weight.615, %41), scope: __module.model/__module.model.layers.33/__module.model.layers.33.mlp/__module.model.layers.33.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.1019 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5143, %hidden_states.1017, %44), scope: __module.model/__module.model.layers.33 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp.69 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_445.Qwen2MLP = prim::GetAttr[name="mlp"](%_34)
  %post_attention_layernorm.69 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_447.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_34)
  %self_attn.69 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_440.Qwen2Attention = prim::GetAttr[name="self_attn"](%_34)
  %input_layernorm.69 : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_446.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_34)
  %weight.617 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.69)
  %hidden_states.1021 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.1019, %33, %46, %46, %41), scope: __module.model/__module.model.layers.34/__module.model.layers.34.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %5162 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.1021, %38), scope: __module.model/__module.model.layers.34/__module.model.layers.34.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %5163 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.34/__module.model.layers.34.input_layernorm
  %variance.137 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%5162, %5163, %25, %41), scope: __module.model/__module.model.layers.34/__module.model.layers.34.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %5165 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.137, %24, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %5166 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%5165), scope: __module.model/__module.model.layers.34/__module.model.layers.34.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.1023 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.1021, %5166), scope: __module.model/__module.model.layers.34/__module.model.layers.34.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.1025 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.1023, %33, %46, %46, %41), scope: __module.model/__module.model.layers.34/__module.model.layers.34.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.1027 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.617, %hidden_states.1025), scope: __module.model/__module.model.layers.34/__module.model.layers.34.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %5170 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.1027, %hidden_states.1021)
  %5171 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5172 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5170)
  %o_proj.69 : __torch__.torch.nn.modules.linear.___torch_mangle_439.Linear = prim::GetAttr[name="o_proj"](%self_attn.69)
  %v_proj.69 : __torch__.torch.nn.modules.linear.___torch_mangle_438.Linear = prim::GetAttr[name="v_proj"](%self_attn.69)
  %k_proj.69 : __torch__.torch.nn.modules.linear.___torch_mangle_437.Linear = prim::GetAttr[name="k_proj"](%self_attn.69)
  %q_proj.69 : __torch__.torch.nn.modules.linear.___torch_mangle_436.Linear = prim::GetAttr[name="q_proj"](%self_attn.69)
  %5177 : int = aten::size(%5171, %42), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %5178 : int = aten::size(%5171, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.205 : Tensor = prim::GetAttr[name="bias"](%q_proj.69)
  %weight.619 : Tensor = prim::GetAttr[name="weight"](%q_proj.69)
  %5181 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%5171, %weight.619, %bias.205), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn/__module.model.layers.34.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %5182 : int[] = prim::ListConstruct(%5177, %5178, %45, %30), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %5183 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%5181, %5182), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q.69 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%5183, %44, %38), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.207 : Tensor = prim::GetAttr[name="bias"](%k_proj.69)
  %weight.621 : Tensor = prim::GetAttr[name="weight"](%k_proj.69)
  %5187 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%5171, %weight.621, %bias.207), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn/__module.model.layers.34.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %5188 : int[] = prim::ListConstruct(%5177, %5178, %45, %30), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %5189 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%5187, %5188), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k.69 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%5189, %44, %38), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias.209 : Tensor = prim::GetAttr[name="bias"](%v_proj.69)
  %weight.623 : Tensor = prim::GetAttr[name="weight"](%v_proj.69)
  %5193 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%5171, %weight.623, %bias.209), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn/__module.model.layers.34.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %5194 : int[] = prim::ListConstruct(%5177, %5178, %45, %30), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %5195 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%5193, %5194), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.1033 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%5195, %44, %38), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos.73 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin.73 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %5199 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.69, %cos.73), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %5200 : int = aten::size(%q.69, %36), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %5201 : Long(device=cpu) = prim::NumToTensor(%5200), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %5202 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5201, %29), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %5203 : int = aten::Int(%5202), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %x1.137 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.69, %36, %42, %5203, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %5205 : int = aten::size(%q.69, %36), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %5206 : Long(device=cpu) = prim::NumToTensor(%5205), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %5207 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5206, %29), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %5208 : int = aten::Int(%5207), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %x2.137 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.69, %36, %5208, %37, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %5210 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.137), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %5211 : Tensor[] = prim::ListConstruct(%5210, %x1.137), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %5212 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5211, %45), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %5213 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%5212, %sin.73), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query.69 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5199, %5213, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %5215 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.69, %cos.73), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %5216 : int = aten::size(%k.69, %36), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %5217 : Long(device=cpu) = prim::NumToTensor(%5216), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %5218 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5217, %29), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %5219 : int = aten::Int(%5218), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %x1.139 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.69, %36, %42, %5219, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %5221 : int = aten::size(%k.69, %36), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %5222 : Long(device=cpu) = prim::NumToTensor(%5221), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %5223 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5222, %29), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %5224 : int = aten::Int(%5223), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %x2.139 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.69, %36, %5224, %37, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %5226 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.139), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %5227 : Tensor[] = prim::ListConstruct(%5226, %x1.139), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %5228 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5227, %45), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %5229 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%5228, %sin.73), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.1029 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%5215, %5229, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %5231 : int = aten::size(%hidden_states.1029, %42), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5232 : int = aten::size(%hidden_states.1029, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.137 : Long(device=cpu) = prim::NumToTensor(%5232), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %5234 : int = aten::size(%hidden_states.1029, %38), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5235 : int = aten::size(%hidden_states.1029, %36), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5236 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.1029, %42, %42, %37, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5237 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%5236, %44, %42, %37, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5238 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%5237, %38), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5239 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%5238, %36, %42, %37, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5240 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%5239, %35, %42, %37, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5241 : int[] = prim::ListConstruct(%5231, %5232, %35, %5234, %5235), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %hidden_states.1031 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%5240, %5241, %46), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5243 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.137, %28), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %5244 : int = aten::Int(%5243), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %5245 : int[] = prim::ListConstruct(%5231, %5244, %5234, %5235), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %key.69 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.1031, %5245), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %5247 : int = aten::size(%hidden_states.1033, %42), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5248 : int = aten::size(%hidden_states.1033, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.139 : Long(device=cpu) = prim::NumToTensor(%5248), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %5250 : int = aten::size(%hidden_states.1033, %38), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5251 : int = aten::size(%hidden_states.1033, %36), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5252 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.1033, %42, %42, %37, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5253 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%5252, %44, %42, %37, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5254 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%5253, %38), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5255 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%5254, %36, %42, %37, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5256 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%5255, %35, %42, %37, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5257 : int[] = prim::ListConstruct(%5247, %5248, %35, %5250, %5251), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %hidden_states.1035 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%5256, %5257, %46), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5259 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.139, %28), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %5260 : int = aten::Int(%5259), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %5261 : int[] = prim::ListConstruct(%5247, %5260, %5250, %5251), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %value.69 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.1035, %5261), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %5263 : int = aten::size(%key.69, %38), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %5264 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %5265 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%5264, %44, %42, %37, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %5266 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%5265, %38, %42, %37, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.75 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%5266, %36, %42, %5263, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.137 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.69, %key.69, %value.69, %attention_mask.75, %27, %46, %26, %46), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %5269 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.137, %44, %38), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.139 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%5269, %42), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %5271 : int[] = prim::ListConstruct(%5177, %5178, %45), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn
  %5272 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.139, %5271), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.273 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%5272, %42), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.625 : Tensor = prim::GetAttr[name="weight"](%o_proj.69)
  %hidden_states.1037 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.273, %weight.625, %41), scope: __module.model/__module.model.layers.34/__module.model.layers.34.self_attn/__module.model.layers.34.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.1039 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5172, %hidden_states.1037, %44), scope: __module.model/__module.model.layers.34 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.627 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.69)
  %hidden_states.1041 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.1039, %33, %46, %46, %41), scope: __module.model/__module.model.layers.34/__module.model.layers.34.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %5279 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.1041, %38), scope: __module.model/__module.model.layers.34/__module.model.layers.34.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %5280 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.34/__module.model.layers.34.post_attention_layernorm
  %variance.139 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%5279, %5280, %25, %41), scope: __module.model/__module.model.layers.34/__module.model.layers.34.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %5282 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.139, %24, %44), scope: __module.model/__module.model.layers.34/__module.model.layers.34.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %5283 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%5282), scope: __module.model/__module.model.layers.34/__module.model.layers.34.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.1043 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.1041, %5283), scope: __module.model/__module.model.layers.34/__module.model.layers.34.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.1045 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.1043, %33, %46, %46, %41), scope: __module.model/__module.model.layers.34/__module.model.layers.34.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.275 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.627, %hidden_states.1045), scope: __module.model/__module.model.layers.34/__module.model.layers.34.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %5287 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.275, %hidden_states.1041)
  %5288 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5289 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5287)
  %down_proj.69 : __torch__.torch.nn.modules.linear.___torch_mangle_443.Linear = prim::GetAttr[name="down_proj"](%mlp.69)
  %up_proj.69 : __torch__.torch.nn.modules.linear.___torch_mangle_442.Linear = prim::GetAttr[name="up_proj"](%mlp.69)
  %gate_proj.69 : __torch__.torch.nn.modules.linear.___torch_mangle_441.Linear = prim::GetAttr[name="gate_proj"](%mlp.69)
  %weight.629 : Tensor = prim::GetAttr[name="weight"](%gate_proj.69)
  %input.277 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%5288, %weight.629, %41), scope: __module.model/__module.model.layers.34/__module.model.layers.34.mlp/__module.model.layers.34.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %5295 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.277), scope: __module.model/__module.model.layers.34/__module.model.layers.34.mlp/__module.model.layers.34.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.631 : Tensor = prim::GetAttr[name="weight"](%up_proj.69)
  %5297 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%5288, %weight.631, %41), scope: __module.model/__module.model.layers.34/__module.model.layers.34.mlp/__module.model.layers.34.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.279 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%5295, %5297), scope: __module.model/__module.model.layers.34/__module.model.layers.34.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.633 : Tensor = prim::GetAttr[name="weight"](%down_proj.69)
  %hidden_states.1047 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.279, %weight.633, %41), scope: __module.model/__module.model.layers.34/__module.model.layers.34.mlp/__module.model.layers.34.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.1049 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5289, %hidden_states.1047, %44), scope: __module.model/__module.model.layers.34 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %mlp : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_458.Qwen2MLP = prim::GetAttr[name="mlp"](%_35)
  %post_attention_layernorm : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_460.Qwen2RMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_35)
  %self_attn : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_453.Qwen2Attention = prim::GetAttr[name="self_attn"](%_35)
  %input_layernorm : __torch__.transformers.models.qwen2.modeling_qwen2.___torch_mangle_459.Qwen2RMSNorm = prim::GetAttr[name="input_layernorm"](%_35)
  %weight.635 : Tensor = prim::GetAttr[name="weight"](%input_layernorm)
  %hidden_states.1051 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.1049, %33, %46, %46, %41), scope: __module.model/__module.model.layers.35/__module.model.layers.35.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %5308 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.1051, %38), scope: __module.model/__module.model.layers.35/__module.model.layers.35.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %5309 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.35/__module.model.layers.35.input_layernorm
  %variance.141 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%5308, %5309, %25, %41), scope: __module.model/__module.model.layers.35/__module.model.layers.35.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %5311 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.141, %24, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %5312 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%5311), scope: __module.model/__module.model.layers.35/__module.model.layers.35.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.1053 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.1051, %5312), scope: __module.model/__module.model.layers.35/__module.model.layers.35.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.1055 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.1053, %33, %46, %46, %41), scope: __module.model/__module.model.layers.35/__module.model.layers.35.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states.1057 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.635, %hidden_states.1055), scope: __module.model/__module.model.layers.35/__module.model.layers.35.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %5316 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.1057, %hidden_states.1051)
  %5317 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5318 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5316)
  %o_proj : __torch__.torch.nn.modules.linear.___torch_mangle_452.Linear = prim::GetAttr[name="o_proj"](%self_attn)
  %v_proj : __torch__.torch.nn.modules.linear.___torch_mangle_451.Linear = prim::GetAttr[name="v_proj"](%self_attn)
  %k_proj : __torch__.torch.nn.modules.linear.___torch_mangle_450.Linear = prim::GetAttr[name="k_proj"](%self_attn)
  %q_proj : __torch__.torch.nn.modules.linear.___torch_mangle_449.Linear = prim::GetAttr[name="q_proj"](%self_attn)
  %5323 : int = aten::size(%5317, %42), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %5324 : int = aten::size(%5317, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:150:0
  %bias.211 : Tensor = prim::GetAttr[name="bias"](%q_proj)
  %weight.637 : Tensor = prim::GetAttr[name="weight"](%q_proj)
  %5327 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%5317, %weight.637, %bias.211), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn/__module.model.layers.35.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %5328 : int[] = prim::ListConstruct(%5323, %5324, %45, %30), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %5329 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%5327, %5328), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %q : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%5329, %44, %38), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:153:0
  %bias.213 : Tensor = prim::GetAttr[name="bias"](%k_proj)
  %weight.639 : Tensor = prim::GetAttr[name="weight"](%k_proj)
  %5333 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%5317, %weight.639, %bias.213), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn/__module.model.layers.35.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %5334 : int[] = prim::ListConstruct(%5323, %5324, %45, %30), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %5335 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%5333, %5334), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %k : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%5335, %44, %38), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:154:0
  %bias : Tensor = prim::GetAttr[name="bias"](%v_proj)
  %weight.641 : Tensor = prim::GetAttr[name="weight"](%v_proj)
  %5339 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%5317, %weight.641, %bias), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn/__module.model.layers.35.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %5340 : int[] = prim::ListConstruct(%5323, %5324, %45, %30), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %5341 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%5339, %5340), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %hidden_states.1063 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%5341, %44, %38), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:155:0
  %cos : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%190, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:77:0
  %sin : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:78:0
  %5345 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q, %cos), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %5346 : int = aten::size(%q, %36), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %5347 : Long(device=cpu) = prim::NumToTensor(%5346), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %5348 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5347, %29), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %5349 : int = aten::Int(%5348), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %x1.141 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q, %36, %42, %5349, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %5351 : int = aten::size(%q, %36), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %5352 : Long(device=cpu) = prim::NumToTensor(%5351), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %5353 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5352, %29), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %5354 : int = aten::Int(%5353), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %x2.141 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q, %36, %5354, %37, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %5356 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.141), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %5357 : Tensor[] = prim::ListConstruct(%5356, %x1.141), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %5358 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5357, %45), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %5359 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%5358, %sin), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %query : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5345, %5359, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:79:0
  %5361 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k, %cos), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %5362 : int = aten::size(%k, %36), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %5363 : Long(device=cpu) = prim::NumToTensor(%5362), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %5364 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5363, %29), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %5365 : int = aten::Int(%5364), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %x1 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k, %36, %42, %5365, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:52:0
  %5367 : int = aten::size(%k, %36), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %5368 : Long(device=cpu) = prim::NumToTensor(%5367), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %5369 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5368, %29), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %5370 : int = aten::Int(%5369), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %x2 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k, %36, %5370, %37, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:53:0
  %5372 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %5373 : Tensor[] = prim::ListConstruct(%5372, %x1), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %5374 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5373, %45), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:54:0
  %5375 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%5374, %sin), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %hidden_states.1059 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%5361, %5375, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:80:0
  %5377 : int = aten::size(%hidden_states.1059, %42), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5378 : int = aten::size(%hidden_states.1059, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.141 : Long(device=cpu) = prim::NumToTensor(%5378), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %5380 : int = aten::size(%hidden_states.1059, %38), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5381 : int = aten::size(%hidden_states.1059, %36), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5382 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.1059, %42, %42, %37, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5383 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%5382, %44, %42, %37, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5384 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%5383, %38), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5385 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%5384, %36, %42, %37, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5386 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%5385, %35, %42, %37, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5387 : int[] = prim::ListConstruct(%5377, %5378, %35, %5380, %5381), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %hidden_states.1061 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%5386, %5387, %46), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5389 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.141, %28), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %5390 : int = aten::Int(%5389), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %5391 : int[] = prim::ListConstruct(%5377, %5390, %5380, %5381), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %key : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.1061, %5391), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %5393 : int = aten::size(%hidden_states.1063, %42), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5394 : int = aten::size(%hidden_states.1063, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads : Long(device=cpu) = prim::NumToTensor(%5394), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %5396 : int = aten::size(%hidden_states.1063, %38), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5397 : int = aten::size(%hidden_states.1063, %36), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5398 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.1063, %42, %42, %37, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5399 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%5398, %44, %42, %37, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5400 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%5399, %38), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5401 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%5400, %36, %42, %37, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5402 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%5401, %35, %42, %37, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5403 : int[] = prim::ListConstruct(%5393, %5394, %35, %5396, %5397), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %hidden_states.1065 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%5402, %5403, %46), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5405 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads, %28), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %5406 : int = aten::Int(%5405), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %5407 : int[] = prim::ListConstruct(%5393, %5406, %5396, %5397), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %value : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.1065, %5407), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %5409 : int = aten::size(%key, %38), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %5410 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %42, %42, %37, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %5411 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%5410, %44, %42, %37, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %5412 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%5411, %38, %42, %37, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%5412, %36, %42, %5409, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.141 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query, %key, %value, %attention_mask, %27, %46, %26, %46), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %5415 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.141, %44, %38), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%5415, %42), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %5417 : int[] = prim::ListConstruct(%5323, %5324, %45), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn
  %5418 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output, %5417), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %input.281 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%5418, %42), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:181:0
  %weight.643 : Tensor = prim::GetAttr[name="weight"](%o_proj)
  %hidden_states.1067 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.281, %weight.643, %41), scope: __module.model/__module.model.layers.35/__module.model.layers.35.self_attn/__module.model.layers.35.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.1069 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5318, %hidden_states.1067, %44), scope: __module.model/__module.model.layers.35 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:244:0
  %weight.645 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm)
  %hidden_states.1071 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.1069, %33, %46, %46, %41), scope: __module.model/__module.model.layers.35/__module.model.layers.35.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %5425 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.1071, %38), scope: __module.model/__module.model.layers.35/__module.model.layers.35.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %5426 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.layers.35/__module.model.layers.35.post_attention_layernorm
  %variance.143 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%5425, %5426, %25, %41), scope: __module.model/__module.model.layers.35/__module.model.layers.35.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %5428 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.143, %24, %44), scope: __module.model/__module.model.layers.35/__module.model.layers.35.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %5429 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%5428), scope: __module.model/__module.model.layers.35/__module.model.layers.35.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.1073 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.1071, %5429), scope: __module.model/__module.model.layers.35/__module.model.layers.35.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.1075 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.1073, %33, %46, %46, %41), scope: __module.model/__module.model.layers.35/__module.model.layers.35.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %input.283 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.645, %hidden_states.1075), scope: __module.model/__module.model.layers.35/__module.model.layers.35.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %5433 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%input.283, %hidden_states.1071)
  %5434 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5435 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5433)
  %down_proj : __torch__.torch.nn.modules.linear.___torch_mangle_456.Linear = prim::GetAttr[name="down_proj"](%mlp)
  %up_proj : __torch__.torch.nn.modules.linear.___torch_mangle_455.Linear = prim::GetAttr[name="up_proj"](%mlp)
  %gate_proj : __torch__.torch.nn.modules.linear.___torch_mangle_454.Linear = prim::GetAttr[name="gate_proj"](%mlp)
  %weight.647 : Tensor = prim::GetAttr[name="weight"](%gate_proj)
  %input.285 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%5434, %weight.647, %41), scope: __module.model/__module.model.layers.35/__module.model.layers.35.mlp/__module.model.layers.35.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %5441 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::silu(%input.285), scope: __module.model/__module.model.layers.35/__module.model.layers.35.mlp/__module.model.layers.35.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.649 : Tensor = prim::GetAttr[name="weight"](%up_proj)
  %5443 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::linear(%5434, %weight.649, %41), scope: __module.model/__module.model.layers.35/__module.model.layers.35.mlp/__module.model.layers.35.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %input.287 : Float(2, 16, 12288, strides=[196608, 12288, 1], requires_grad=0, device=cpu) = aten::mul(%5441, %5443), scope: __module.model/__module.model.layers.35/__module.model.layers.35.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:46:0
  %weight.651 : Tensor = prim::GetAttr[name="weight"](%down_proj)
  %hidden_states.1077 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%input.287, %weight.651, %41), scope: __module.model/__module.model.layers.35/__module.model.layers.35.mlp/__module.model.layers.35.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %hidden_states.1079 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5435, %hidden_states.1077, %44), scope: __module.model/__module.model.layers.35 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:250:0
  %weight.653 : Tensor = prim::GetAttr[name="weight"](%norm)
  %hidden_states.1081 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.1079, %33, %46, %46, %41), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:198:0
  %5450 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.1081, %38), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %5451 : int[] = prim::ListConstruct(%45), scope: __module.model/__module.model.norm
  %variance : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%5450, %5451, %25, %41), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:199:0
  %5453 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance, %24, %44), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %5454 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%5453), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.1083 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.1081, %5454), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:200:0
  %hidden_states.1085 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.1083, %33, %46, %46, %41), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %hidden_states : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.653, %hidden_states.1085), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:201:0
  %7 : int = prim::Constant[value=0]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:463:0
  %8 : int = prim::Constant[value=0]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:463:0
  %9 : int = prim::Constant[value=9223372036854775807]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:463:0
  %10 : int = prim::Constant[value=1]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:463:0
  %11 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states, %7, %8, %9, %10) # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:463:0
  %12 : int = prim::Constant[value=1]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:463:0
  %13 : int = prim::Constant[value=0]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:463:0
  %14 : int = prim::Constant[value=9223372036854775807]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:463:0
  %15 : int = prim::Constant[value=1]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:463:0
  %16 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%11, %12, %13, %14, %15) # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:463:0
  %17 : int = prim::Constant[value=2]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:463:0
  %18 : int = prim::Constant[value=0]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:463:0
  %19 : int = prim::Constant[value=9223372036854775807]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:463:0
  %20 : int = prim::Constant[value=1]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:463:0
  %input : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%16, %17, %18, %19, %20) # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:463:0
  %5458 : NoneType = prim::Constant(), scope: __module.lm_head
  %weight : Tensor = prim::GetAttr[name="weight"](%lm_head)
  %5460 : Float(2, 16, 151936, strides=[2430976, 151936, 1], requires_grad=0, device=cpu) = aten::linear(%input, %weight, %5458), scope: __module.lm_head # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/modules/linear.py:134:0
  %23 : (Float(2, 16, 151936, strides=[2430976, 151936, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%5460)
  return (%23)

2026-01-15 22:07:33,147 - INFO -     > Intermediate Model Size: 4.52 GB
2026-01-15 22:07:35,378 - INFO - >>> [Bake] Stage 2: Reshaping to [1, 4096]...
2026-01-15 22:07:35,448 - INFO -     > Input: input_ids [?,?]
2026-01-15 22:07:35,448 - INFO -       -> Locking to [1, 4096]
2026-01-15 22:07:35,448 - INFO -     > Input: attention_mask [?,?]
2026-01-15 22:07:35,448 - INFO -       -> Locking to [1, 4096]
2026-01-15 22:07:35,448 - INFO -     > Input: position_ids [?,?]
2026-01-15 22:07:35,448 - INFO -       -> Locking to [1, 4096]
2026-01-15 22:07:35,505 - INFO - >>> [Bake] Saving final static model to ./models/qwen3_int4...
2026-01-15 22:07:37,074 - INFO - >>> [Bake] DONE.
