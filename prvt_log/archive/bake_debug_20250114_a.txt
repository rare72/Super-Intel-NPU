2026-01-14 09:30:15,640 - INFO - >>> [Bake] Starting process for model: Intel/neural-chat-7b-v3-1
2026-01-14 09:30:15,640 - INFO - >>> [Bake] Downloading to ./model_staging...
2026-01-14 09:30:15,642 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2026-01-14 09:30:15,937 - DEBUG - https://huggingface.co:443 "GET /api/models/Intel/neural-chat-7b-v3-1/revision/main HTTP/1.1" 200 7138
2026-01-14 09:30:15,941 - DEBUG - Attempting to acquire lock 135756480541856 on model_staging/.cache/huggingface/download/LICENSE.lock
2026-01-14 09:30:15,941 - DEBUG - Lock 135756480541856 acquired on model_staging/.cache/huggingface/download/LICENSE.lock
2026-01-14 09:30:15,943 - DEBUG - Attempting to release lock 135756480541856 on model_staging/.cache/huggingface/download/LICENSE.lock
2026-01-14 09:30:15,943 - DEBUG - Attempting to acquire lock 135756480543440 on model_staging/.cache/huggingface/download/README.md.lock
2026-01-14 09:30:15,943 - DEBUG - Lock 135756480541856 released on model_staging/.cache/huggingface/download/LICENSE.lock
2026-01-14 09:30:15,944 - DEBUG - Attempting to acquire lock 135756480807040 on model_staging/.cache/huggingface/download/generation_config.json.lock
2026-01-14 09:30:15,946 - DEBUG - Attempting to acquire lock 135765197228080 on model_staging/.cache/huggingface/download/.gitattributes.lock
2026-01-14 09:30:15,946 - DEBUG - Lock 135756480543440 acquired on model_staging/.cache/huggingface/download/README.md.lock
2026-01-14 09:30:15,946 - DEBUG - Attempting to acquire lock 135756480808480 on model_staging/.cache/huggingface/download/model-00002-of-00002.safetensors.lock
2026-01-14 09:30:15,946 - DEBUG - Attempting to acquire lock 135756480809008 on model_staging/.cache/huggingface/download/model.safetensors.index.json.lock
2026-01-14 09:30:15,947 - DEBUG - Attempting to acquire lock 135756480811072 on model_staging/.cache/huggingface/download/config.json.lock
2026-01-14 09:30:15,947 - DEBUG - Lock 135756480807040 acquired on model_staging/.cache/huggingface/download/generation_config.json.lock
2026-01-14 09:30:15,947 - DEBUG - Lock 135765197228080 acquired on model_staging/.cache/huggingface/download/.gitattributes.lock
2026-01-14 09:30:15,947 - DEBUG - Attempting to acquire lock 135756480808144 on model_staging/.cache/huggingface/download/model-00001-of-00002.safetensors.lock
2026-01-14 09:30:15,947 - DEBUG - Attempting to release lock 135756480543440 on model_staging/.cache/huggingface/download/README.md.lock
2026-01-14 09:30:15,947 - DEBUG - Attempting to acquire lock 135756480812320 on model_staging/.cache/huggingface/download/pytorch_model-00001-of-00002.bin.lock
2026-01-14 09:30:15,948 - DEBUG - Lock 135756480809008 acquired on model_staging/.cache/huggingface/download/model.safetensors.index.json.lock
2026-01-14 09:30:15,948 - DEBUG - Lock 135756480808480 acquired on model_staging/.cache/huggingface/download/model-00002-of-00002.safetensors.lock
2026-01-14 09:30:15,948 - DEBUG - Lock 135756480811072 acquired on model_staging/.cache/huggingface/download/config.json.lock
2026-01-14 09:30:15,948 - DEBUG - Attempting to release lock 135765197228080 on model_staging/.cache/huggingface/download/.gitattributes.lock
2026-01-14 09:30:15,948 - DEBUG - Attempting to release lock 135756480807040 on model_staging/.cache/huggingface/download/generation_config.json.lock
2026-01-14 09:30:15,948 - DEBUG - Lock 135756480543440 released on model_staging/.cache/huggingface/download/README.md.lock
2026-01-14 09:30:15,948 - DEBUG - Lock 135756480808144 acquired on model_staging/.cache/huggingface/download/model-00001-of-00002.safetensors.lock
2026-01-14 09:30:15,948 - DEBUG - Lock 135756480812320 acquired on model_staging/.cache/huggingface/download/pytorch_model-00001-of-00002.bin.lock
2026-01-14 09:30:15,948 - DEBUG - Attempting to release lock 135756480809008 on model_staging/.cache/huggingface/download/model.safetensors.index.json.lock
2026-01-14 09:30:15,949 - DEBUG - Attempting to release lock 135756480808480 on model_staging/.cache/huggingface/download/model-00002-of-00002.safetensors.lock
2026-01-14 09:30:15,949 - DEBUG - Attempting to release lock 135756480811072 on model_staging/.cache/huggingface/download/config.json.lock
2026-01-14 09:30:15,949 - DEBUG - Lock 135756480807040 released on model_staging/.cache/huggingface/download/generation_config.json.lock
2026-01-14 09:30:15,949 - DEBUG - Lock 135765197228080 released on model_staging/.cache/huggingface/download/.gitattributes.lock
2026-01-14 09:30:15,949 - DEBUG - Attempting to release lock 135756480808144 on model_staging/.cache/huggingface/download/model-00001-of-00002.safetensors.lock
2026-01-14 09:30:15,949 - DEBUG - Attempting to acquire lock 135756480813136 on model_staging/.cache/huggingface/download/pytorch_model-00002-of-00002.bin.lock
2026-01-14 09:30:15,949 - DEBUG - Attempting to release lock 135756480812320 on model_staging/.cache/huggingface/download/pytorch_model-00001-of-00002.bin.lock
2026-01-14 09:30:15,949 - DEBUG - Lock 135756480809008 released on model_staging/.cache/huggingface/download/model.safetensors.index.json.lock
2026-01-14 09:30:15,949 - DEBUG - Lock 135756480808480 released on model_staging/.cache/huggingface/download/model-00002-of-00002.safetensors.lock
2026-01-14 09:30:15,949 - DEBUG - Lock 135756480811072 released on model_staging/.cache/huggingface/download/config.json.lock
2026-01-14 09:30:15,950 - DEBUG - Attempting to acquire lock 135756480812800 on model_staging/.cache/huggingface/download/pytorch_model.bin.index.json.lock
2026-01-14 09:30:15,950 - DEBUG - Lock 135756480808144 released on model_staging/.cache/huggingface/download/model-00001-of-00002.safetensors.lock
2026-01-14 09:30:15,950 - DEBUG - Lock 135756480812320 released on model_staging/.cache/huggingface/download/pytorch_model-00001-of-00002.bin.lock
2026-01-14 09:30:15,950 - DEBUG - Lock 135756480813136 acquired on model_staging/.cache/huggingface/download/pytorch_model-00002-of-00002.bin.lock
2026-01-14 09:30:15,950 - DEBUG - Attempting to acquire lock 135756480807376 on model_staging/.cache/huggingface/download/tokenizer.json.lock
2026-01-14 09:30:15,951 - DEBUG - Lock 135756480812800 acquired on model_staging/.cache/huggingface/download/pytorch_model.bin.index.json.lock
2026-01-14 09:30:15,951 - DEBUG - Attempting to acquire lock 135756480808000 on model_staging/.cache/huggingface/download/tokenizer.model.lock
2026-01-14 09:30:15,951 - DEBUG - Attempting to acquire lock 135756480543296 on model_staging/.cache/huggingface/download/tokenizer_config.json.lock
2026-01-14 09:30:15,951 - DEBUG - Attempting to acquire lock 135756480807280 on model_staging/.cache/huggingface/download/special_tokens_map.json.lock
2026-01-14 09:30:15,951 - DEBUG - Attempting to release lock 135756480813136 on model_staging/.cache/huggingface/download/pytorch_model-00002-of-00002.bin.lock
2026-01-14 09:30:15,951 - DEBUG - Lock 135756480807376 acquired on model_staging/.cache/huggingface/download/tokenizer.json.lock
2026-01-14 09:30:15,951 - DEBUG - Attempting to release lock 135756480812800 on model_staging/.cache/huggingface/download/pytorch_model.bin.index.json.lock
2026-01-14 09:30:15,951 - DEBUG - Lock 135756480808000 acquired on model_staging/.cache/huggingface/download/tokenizer.model.lock
2026-01-14 09:30:15,951 - DEBUG - Lock 135756480543296 acquired on model_staging/.cache/huggingface/download/tokenizer_config.json.lock
2026-01-14 09:30:15,951 - DEBUG - Lock 135756480813136 released on model_staging/.cache/huggingface/download/pytorch_model-00002-of-00002.bin.lock
2026-01-14 09:30:15,951 - DEBUG - Lock 135756480812800 released on model_staging/.cache/huggingface/download/pytorch_model.bin.index.json.lock
2026-01-14 09:30:15,952 - DEBUG - Attempting to release lock 135756480808000 on model_staging/.cache/huggingface/download/tokenizer.model.lock
2026-01-14 09:30:15,952 - DEBUG - Attempting to release lock 135756480543296 on model_staging/.cache/huggingface/download/tokenizer_config.json.lock
2026-01-14 09:30:15,952 - DEBUG - Attempting to release lock 135756480807376 on model_staging/.cache/huggingface/download/tokenizer.json.lock
2026-01-14 09:30:15,952 - DEBUG - Lock 135756480807280 acquired on model_staging/.cache/huggingface/download/special_tokens_map.json.lock
2026-01-14 09:30:15,952 - DEBUG - Lock 135756480808000 released on model_staging/.cache/huggingface/download/tokenizer.model.lock
2026-01-14 09:30:15,952 - DEBUG - Lock 135756480543296 released on model_staging/.cache/huggingface/download/tokenizer_config.json.lock
2026-01-14 09:30:15,952 - DEBUG - Lock 135756480807376 released on model_staging/.cache/huggingface/download/tokenizer.json.lock
2026-01-14 09:30:15,952 - DEBUG - Attempting to release lock 135756480807280 on model_staging/.cache/huggingface/download/special_tokens_map.json.lock
2026-01-14 09:30:15,952 - DEBUG - Lock 135756480807280 released on model_staging/.cache/huggingface/download/special_tokens_map.json.lock
2026-01-14 09:30:15,952 - INFO - >>> [Bake] Loading NNCF config from src/python/nncf_config.json...
2026-01-14 09:30:15,953 - INFO - >>> [Bake] Loading model and applying Data-Free NNCF INT4 Compression...
2026-01-14 09:30:16,123 - DEBUG - Patching module Embedding(32000, 4096)
2026-01-14 09:30:16,123 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,123 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,123 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,123 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,123 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,123 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,123 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,124 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,125 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,126 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,127 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,128 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,129 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,130 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,131 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,132 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,132 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 09:30:16,132 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 09:30:16,132 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,132 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 09:30:16,132 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 09:30:16,132 - DEBUG - Patching module Linear(in_features=4096, out_features=32000, bias=False)
2026-01-14 09:30:16,132 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,132 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,132 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,132 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,132 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,132 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,132 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,132 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,132 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,132 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,132 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,132 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,133 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:16,134 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 09:30:18,448 - DEBUG - Inlined graph:
graph(%self.1 : __torch__.transformers.models.mistral.modeling_mistral.MistralForCausalLM,
      %input_ids : Long(2, 16, strides=[16, 1], requires_grad=0, device=cpu),
      %attention_mask.1 : Long(2, 32, strides=[32, 1], requires_grad=0, device=cpu),
      %position_ids : Long(2, 16, strides=[16, 1], requires_grad=0, device=cpu),
      %past_key_values : (Tensor, Tensor)[]):
  %lm_head : __torch__.torch.nn.modules.linear.___torch_mangle_411.Linear = prim::GetAttr[name="lm_head"](%self.1)
  %model : __torch__.transformers.models.mistral.modeling_mistral.MistralModel = prim::GetAttr[name="model"](%self.1)
  %7 : (Tensor, Tensor), %8 : (Tensor, Tensor), %9 : (Tensor, Tensor), %10 : (Tensor, Tensor), %11 : (Tensor, Tensor), %12 : (Tensor, Tensor), %13 : (Tensor, Tensor), %14 : (Tensor, Tensor), %15 : (Tensor, Tensor), %16 : (Tensor, Tensor), %17 : (Tensor, Tensor), %18 : (Tensor, Tensor), %19 : (Tensor, Tensor), %20 : (Tensor, Tensor), %21 : (Tensor, Tensor), %22 : (Tensor, Tensor), %23 : (Tensor, Tensor), %24 : (Tensor, Tensor), %25 : (Tensor, Tensor), %26 : (Tensor, Tensor), %27 : (Tensor, Tensor), %28 : (Tensor, Tensor), %29 : (Tensor, Tensor), %30 : (Tensor, Tensor), %31 : (Tensor, Tensor), %32 : (Tensor, Tensor), %33 : (Tensor, Tensor), %34 : (Tensor, Tensor), %35 : (Tensor, Tensor), %36 : (Tensor, Tensor), %37 : (Tensor, Tensor), %38 : (Tensor, Tensor) = prim::ListUnpack(%past_key_values)
  %39 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %40 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%7)
  %41 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %42 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%8)
  %43 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %44 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%9)
  %45 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %46 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%10)
  %47 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %48 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%11)
  %49 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %50 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%12)
  %51 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %52 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%13)
  %53 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %54 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%14)
  %55 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %56 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%15)
  %57 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %58 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%16)
  %59 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %60 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%17)
  %61 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %62 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%18)
  %63 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %64 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%19)
  %65 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %66 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%20)
  %67 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %68 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%21)
  %69 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %70 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%22)
  %71 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %72 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%23)
  %73 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %74 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%24)
  %75 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %76 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%25)
  %77 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %78 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%26)
  %79 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %80 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%27)
  %81 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %82 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%28)
  %83 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %84 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%29)
  %85 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %86 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%30)
  %87 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %88 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%31)
  %89 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %90 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%32)
  %91 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %92 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%33)
  %93 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %94 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%34)
  %95 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %96 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%35)
  %97 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %98 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%36)
  %99 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %100 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%37)
  %101 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %102 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%38)
  %219 : Double(requires_grad=0, device=cpu) = prim::Constant[value={1e-05}](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %220 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %221 : float = prim::Constant[value=0.088388347648318447](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %222 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %223 : Long(requires_grad=0, device=cpu) = prim::Constant[value={4}](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %224 : int = prim::Constant[value=-2](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %225 : Long(requires_grad=0, device=cpu) = prim::Constant[value={2}](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %226 : int = prim::Constant[value=128](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %227 : Double(requires_grad=0, device=cpu) = prim::Constant[value={1}](), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:300:0
  %228 : Float(requires_grad=0, device=cpu) = prim::Constant[value={-65504}](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:208:0
  %229 : int = prim::Constant[value=6](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:207:0
  %230 : Float(requires_grad=0, device=cpu) = prim::Constant[value={0}](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:207:0
  %231 : int = prim::Constant[value=-1](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:412:0
  %232 : Long(requires_grad=0, device=cpu) = prim::Constant[value={4096}](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:88:0
  %233 : Long(requires_grad=0, device=cpu) = prim::Constant[value={0}](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:408:0
  %234 : int = prim::Constant[value=4](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:407:0
  %235 : int = prim::Constant[value=3](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:405:0
  %236 : int = prim::Constant[value=9223372036854775807](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:405:0
  %237 : int = prim::Constant[value=0](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/cache_utils.py:126:0
  %238 : int = prim::Constant[value=11](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:723:0
  %239 : bool = prim::Constant[value=0](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:348:0
  %240 : Device = prim::Constant[value="cpu"](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:348:0
  %241 : NoneType = prim::Constant(), scope: __module.model
  %242 : int = prim::Constant[value=1](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:349:0
  %243 : int = prim::Constant[value=2](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/cache_utils.py:134:0
  %norm : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_410.MistralRMSNorm = prim::GetAttr[name="norm"](%model)
  %layers : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_31 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_409.MistralDecoderLayer = prim::GetAttr[name="31"](%layers)
  %layers.61 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_30 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_396.MistralDecoderLayer = prim::GetAttr[name="30"](%layers.61)
  %layers.59 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_29 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_383.MistralDecoderLayer = prim::GetAttr[name="29"](%layers.59)
  %layers.57 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_28 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_370.MistralDecoderLayer = prim::GetAttr[name="28"](%layers.57)
  %layers.55 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_27 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_357.MistralDecoderLayer = prim::GetAttr[name="27"](%layers.55)
  %layers.53 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_26 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_344.MistralDecoderLayer = prim::GetAttr[name="26"](%layers.53)
  %layers.51 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_25 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_331.MistralDecoderLayer = prim::GetAttr[name="25"](%layers.51)
  %layers.49 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_24 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_318.MistralDecoderLayer = prim::GetAttr[name="24"](%layers.49)
  %layers.47 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_23 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_305.MistralDecoderLayer = prim::GetAttr[name="23"](%layers.47)
  %layers.45 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_22 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_292.MistralDecoderLayer = prim::GetAttr[name="22"](%layers.45)
  %layers.43 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_21 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_279.MistralDecoderLayer = prim::GetAttr[name="21"](%layers.43)
  %layers.41 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_20 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_266.MistralDecoderLayer = prim::GetAttr[name="20"](%layers.41)
  %layers.39 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_19 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_253.MistralDecoderLayer = prim::GetAttr[name="19"](%layers.39)
  %layers.37 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_18 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_240.MistralDecoderLayer = prim::GetAttr[name="18"](%layers.37)
  %layers.35 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_17 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_227.MistralDecoderLayer = prim::GetAttr[name="17"](%layers.35)
  %layers.33 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_16 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_214.MistralDecoderLayer = prim::GetAttr[name="16"](%layers.33)
  %layers.31 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_15 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_201.MistralDecoderLayer = prim::GetAttr[name="15"](%layers.31)
  %layers.29 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_14 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_188.MistralDecoderLayer = prim::GetAttr[name="14"](%layers.29)
  %layers.27 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_13 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_175.MistralDecoderLayer = prim::GetAttr[name="13"](%layers.27)
  %layers.25 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_12 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_162.MistralDecoderLayer = prim::GetAttr[name="12"](%layers.25)
  %layers.23 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_11 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_149.MistralDecoderLayer = prim::GetAttr[name="11"](%layers.23)
  %layers.21 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_10 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_136.MistralDecoderLayer = prim::GetAttr[name="10"](%layers.21)
  %layers.19 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_9 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_123.MistralDecoderLayer = prim::GetAttr[name="9"](%layers.19)
  %layers.17 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_8 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_110.MistralDecoderLayer = prim::GetAttr[name="8"](%layers.17)
  %layers.15 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_7 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_97.MistralDecoderLayer = prim::GetAttr[name="7"](%layers.15)
  %layers.13 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_6 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_84.MistralDecoderLayer = prim::GetAttr[name="6"](%layers.13)
  %layers.11 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_5 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_71.MistralDecoderLayer = prim::GetAttr[name="5"](%layers.11)
  %layers.9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_4 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_58.MistralDecoderLayer = prim::GetAttr[name="4"](%layers.9)
  %layers.7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_3 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_45.MistralDecoderLayer = prim::GetAttr[name="3"](%layers.7)
  %layers.5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_2 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_32.MistralDecoderLayer = prim::GetAttr[name="2"](%layers.5)
  %layers.3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_1 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_19.MistralDecoderLayer = prim::GetAttr[name="1"](%layers.3)
  %layers.1 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_0 : __torch__.transformers.models.mistral.modeling_mistral.MistralDecoderLayer = prim::GetAttr[name="0"](%layers.1)
  %rotary_emb : __torch__.transformers.models.mistral.modeling_mistral.MistralRotaryEmbedding = prim::GetAttr[name="rotary_emb"](%model)
  %embed_tokens : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="embed_tokens"](%model)
  %weight.1 : Tensor = prim::GetAttr[name="weight"](%embed_tokens)
  %inputs_embeds : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None, False, False)(%weight.1, %input_ids), scope: __module.model/__module.model.embed_tokens # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %313 : int = aten::size(%39, %243), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/cache_utils.py:134:0
  %past_seen_tokens : Long(device=cpu) = prim::NumToTensor(%313), scope: __module.model
  %315 : int = aten::size(%inputs_embeds, %242), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:349:0
  %316 : Long(device=cpu) = prim::NumToTensor(%315), scope: __module.model
  %317 : Long(requires_grad=0, device=cpu) = aten::add(%past_seen_tokens, %316, %242), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:349:0
  %318 : Scalar = aten::ScalarImplicit(%317), scope: __module.model
  %cache_position : Long(16, strides=[1], requires_grad=0, device=cpu) = aten::arange(%313, %318, %241, %241, %240, %239), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:348:0
  %attention_mask.3 : Bool(2, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::to(%attention_mask.1, %240, %238, %239, %239, %241), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:723:0
  %321 : int = aten::size(%cache_position, %237), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/cache_utils.py:126:0
  %query_length : Long(device=cpu) = prim::NumToTensor(%321), scope: __module.model
  %323 : int = aten::size(%39, %243), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/cache_utils.py:134:0
  %324 : Long(device=cpu) = prim::NumToTensor(%323), scope: __module.model
  %kv_length : Long(requires_grad=0, device=cpu) = aten::add(%324, %query_length, %242), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/cache_utils.py:127:0
  %326 : int = aten::Int(%kv_length), scope: __module.model
  %327 : Scalar = aten::ScalarImplicit(%kv_length), scope: __module.model
  %328 : int = aten::size(%inputs_embeds, %237), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:893:0
  %329 : int = aten::size(%cache_position, %237), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:391:0
  %330 : Long(1, 16, strides=[16, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%cache_position, %237), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:405:0
  %331 : Long(1, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%330, %242), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:405:0
  %332 : Long(1, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::slice(%331, %243, %237, %236, %242), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:405:0
  %q_idx : Long(1, 1, 16, 1, strides=[16, 16, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%332, %235), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:405:0
  %334 : Long(2, strides=[1], requires_grad=0, device=cpu) = aten::arange(%328, %234, %241, %240, %239), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:407:0
  %335 : Long(2, strides=[1], requires_grad=0, device=cpu) = aten::slice(%334, %237, %237, %236, %242), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:407:0
  %336 : Long(2, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%335, %242), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:407:0
  %337 : Long(2, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%336, %243), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:407:0
  %batch_idx : Long(2, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%337, %235), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:407:0
  %339 : Long(32, strides=[1], requires_grad=0, device=cpu) = aten::arange(%327, %234, %241, %240, %239), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:408:0
  %340 : Long(1, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%339, %237), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:408:0
  %341 : Long(1, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%340, %242), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:408:0
  %342 : Long(1, 1, 1, 32, strides=[32, 32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%341, %243), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:408:0
  %343 : Long(1, 1, 1, 32, strides=[32, 32, 32, 1], requires_grad=0, device=cpu) = aten::slice(%342, %235, %237, %236, %242), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:408:0
  %kv_idx : Long(1, 1, 1, 32, strides=[32, 32, 32, 1], requires_grad=0, device=cpu) = aten::add(%343, %233, %242), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:408:0
  %345 : int[] = prim::ListConstruct(), scope: __module.model
  %result.5 : Bool(requires_grad=0, device=cpu) = aten::new_ones(%q_idx, %345, %238, %241, %241, %239), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:52:0
  %347 : int[] = prim::ListConstruct(), scope: __module.model
  %result.1 : Bool(requires_grad=0, device=cpu) = aten::new_ones(%q_idx, %347, %238, %241, %241, %239), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:52:0
  %349 : Long(1, 1, 16, 1, strides=[16, 16, 1, 1], requires_grad=0, device=cpu) = aten::sub(%q_idx, %232, %242), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:88:0
  %350 : Bool(1, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::gt(%kv_idx, %349), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:88:0
  %351 : Bool(1, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::to(%350, %238, %237, %240, %241, %239, %239, %241), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:54:0
  %result.3 : Bool(1, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::__and__(%result.1, %351), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:54:0
  %353 : Bool(1, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::le(%kv_idx, %q_idx), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:78:0
  %354 : Bool(1, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::to(%353, %238, %237, %240, %241, %239, %239, %241), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:54:0
  %355 : Bool(1, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::__and__(%result.3, %354), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:54:0
  %356 : Bool(1, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::to(%355, %238, %237, %240, %241, %239, %239, %241), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:54:0
  %result : Bool(1, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::__and__(%result.5, %356), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:54:0
  %358 : Tensor?[] = prim::ListConstruct(%batch_idx, %kv_idx), scope: __module.model
  %359 : Bool(2, 1, 1, 32, strides=[32, 32, 32, 1], requires_grad=0, device=cpu) = aten::index(%attention_mask.3, %358), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:142:0
  %360 : Bool(2, 1, 1, 32, strides=[32, 32, 32, 1], requires_grad=0, device=cpu) = aten::to(%359, %238, %237, %240, %241, %239, %239, %241), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:54:0
  %causal_mask : Bool(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::__and__(%result, %360), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:54:0
  %362 : int[] = prim::ListConstruct(%328, %231, %329, %326), scope: __module.model
  %mask : Bool(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::expand(%causal_mask, %362, %239), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:412:0
  %364 : Float(requires_grad=0, device=cpu) = aten::to(%230, %240, %229, %239, %239, %241), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:207:0
  %365 : Float(requires_grad=0, device=cpu) = aten::detach(%364), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:207:0
  %366 : Float(requires_grad=0, device=cpu) = aten::to(%228, %240, %229, %239, %239, %241), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:208:0
  %367 : Float(requires_grad=0, device=cpu) = aten::detach(%366), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:208:0
  %attention_mask.5 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::where(%mask, %365, %367), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:205:0
  %inv_freq : Tensor = prim::GetAttr[name="inv_freq"](%rotary_emb)
  %370 : Float(1, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%inv_freq, %237), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:293:0
  %371 : Float(1, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%370, %242, %237, %236, %242), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:293:0
  %372 : Float(1, 64, 1, strides=[64, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%371, %243), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:293:0
  %373 : Float(1, 64, 1, strides=[64, 1, 1], requires_grad=0, device=cpu) = aten::to(%372, %229, %239, %239, %241), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:293:0
  %374 : int = aten::size(%position_ids, %237), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:293:0
  %375 : int[] = prim::ListConstruct(%374, %231, %242), scope: __module.model/__module.model.rotary_emb
  %376 : Float(2, 64, 1, strides=[0, 1, 1], requires_grad=0, device=cpu) = aten::expand(%373, %375, %239), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:293:0
  %inv_freq_expanded.1 : Float(2, 64, 1, strides=[0, 1, 1], requires_grad=0, device=cpu) = aten::to(%376, %229, %237, %240, %241, %239, %239, %241), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:293:0
  %378 : Long(2, 16, strides=[16, 1], requires_grad=0, device=cpu) = aten::slice(%position_ids, %237, %237, %236, %242), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:294:0
  %379 : Long(2, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %242), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:294:0
  %380 : Long(2, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::slice(%379, %243, %237, %236, %242), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:294:0
  %position_ids_expanded.1 : Float(2, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::to(%380, %229, %239, %239, %241), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:294:0
  %inv_freq_expanded : Float(2, 64, 1, strides=[0, 1, 1], requires_grad=0, device=cpu) = aten::to(%inv_freq_expanded.1, %229, %239, %239, %241), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:298:0
  %position_ids_expanded : Float(2, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::to(%position_ids_expanded.1, %229, %239, %239, %241), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:298:0
  %384 : Float(2, 64, 16, strides=[1024, 16, 1], requires_grad=0, device=cpu) = aten::matmul(%inv_freq_expanded, %position_ids_expanded), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:298:0
  %freqs : Float(2, 16, 64, strides=[1024, 1, 16], requires_grad=0, device=cpu) = aten::transpose(%384, %242, %243), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:298:0
  %386 : Tensor[] = prim::ListConstruct(%freqs, %freqs), scope: __module.model/__module.model.rotary_emb
  %emb : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%386, %231), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:299:0
  %388 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::cos(%emb), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:300:0
  %cos.1 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%388, %227), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:300:0
  %390 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::sin(%emb), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:301:0
  %sin.1 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%390, %227), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:301:0
  %cos.3 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::to(%cos.1, %229, %239, %239, %241), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:303:0
  %sin.3 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::to(%sin.1, %229, %239, %239, %241), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:303:0
  %394 : (Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu), Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%cos.3, %sin.3)
  %395 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu), %396 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%394)
  %mlp.1 : __torch__.transformers.models.mistral.modeling_mistral.MistralMLP = prim::GetAttr[name="mlp"](%_0)
  %post_attention_layernorm.1 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_6.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_0)
  %self_attn.1 : __torch__.transformers.models.mistral.modeling_mistral.MistralAttention = prim::GetAttr[name="self_attn"](%_0)
  %input_layernorm.1 : __torch__.transformers.models.mistral.modeling_mistral.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_0)
  %weight.3 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.1)
  %hidden_states.1 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%inputs_embeds, %229, %239, %239, %241), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %403 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.1, %243), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %404 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm
  %variance.1 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%403, %404, %220, %241), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %406 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.1, %219, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %407 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%406), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.3 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.1, %407), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.5 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.3, %229, %239, %239, %241), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.7 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.3, %hidden_states.5), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %411 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.7, %hidden_states.1)
  %412 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %413 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%411)
  %o_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_2.Linear = prim::GetAttr[name="o_proj"](%self_attn.1)
  %v_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_1.Linear = prim::GetAttr[name="v_proj"](%self_attn.1)
  %k_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_0.Linear = prim::GetAttr[name="k_proj"](%self_attn.1)
  %q_proj.1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self_attn.1)
  %418 : int = aten::size(%412, %237), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %419 : int = aten::size(%412, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.5 : Tensor = prim::GetAttr[name="weight"](%q_proj.1)
  %421 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%412, %weight.5), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn/__module.model.layers.0.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %422 : int[] = prim::ListConstruct(%418, %419, %231, %226), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %423 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%421, %422), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.1 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%423, %242, %243), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.7 : Tensor = prim::GetAttr[name="weight"](%k_proj.1)
  %426 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%412, %weight.7), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn/__module.model.layers.0.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %427 : int[] = prim::ListConstruct(%418, %419, %231, %226), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %428 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%426, %427), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.1 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%428, %242, %243), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.9 : Tensor = prim::GetAttr[name="weight"](%v_proj.1)
  %431 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%412, %weight.9), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn/__module.model.layers.0.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %432 : int[] = prim::ListConstruct(%418, %419, %231, %226), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %433 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%431, %432), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.1 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%433, %242, %243), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.5 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.5 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %437 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.1, %cos.5), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %438 : int = aten::size(%q.1, %235), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %439 : Long(device=cpu) = prim::NumToTensor(%438), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %440 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%439, %225), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %441 : int = aten::Int(%440), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %x1.1 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.1, %235, %237, %441, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %443 : int = aten::size(%q.1, %235), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %444 : Long(device=cpu) = prim::NumToTensor(%443), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %445 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%444, %225), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %446 : int = aten::Int(%445), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %x2.1 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.1, %235, %446, %236, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %448 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.1), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %449 : Tensor[] = prim::ListConstruct(%448, %x1.1), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %450 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%449, %231), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %451 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%450, %sin.5), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.1 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%437, %451, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %453 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.1, %cos.5), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %454 : int = aten::size(%k.1, %235), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %455 : Long(device=cpu) = prim::NumToTensor(%454), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %456 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%455, %225), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %457 : int = aten::Int(%456), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %x1.3 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.1, %235, %237, %457, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %459 : int = aten::size(%k.1, %235), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %460 : Long(device=cpu) = prim::NumToTensor(%459), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %461 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%460, %225), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %462 : int = aten::Int(%461), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %x2.3 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.1, %235, %462, %236, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %464 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.3), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %465 : Tensor[] = prim::ListConstruct(%464, %x1.3), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %466 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%465, %231), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %467 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%466, %sin.5), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.1 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%453, %467, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %469 : Tensor[] = prim::ListConstruct(%39, %key_states.1), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %hidden_states.9 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%469, %224), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %471 : Tensor[] = prim::ListConstruct(%40, %value_states.1), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %hidden_states.13 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%471, %224), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %473 : int = aten::size(%hidden_states.9, %237), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %474 : int = aten::size(%hidden_states.9, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.1 : Long(device=cpu) = prim::NumToTensor(%474), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %476 : int = aten::size(%hidden_states.9, %243), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %477 : int = aten::size(%hidden_states.9, %235), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %478 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.9, %237, %237, %236, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %479 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%478, %242, %237, %236, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %480 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%479, %243), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %481 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%480, %235, %237, %236, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %482 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%481, %234, %237, %236, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %483 : int[] = prim::ListConstruct(%473, %474, %234, %476, %477), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %hidden_states.11 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%482, %483, %239), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %485 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.1, %223), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %486 : int = aten::Int(%485), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %487 : int[] = prim::ListConstruct(%473, %486, %476, %477), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %key.1 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.11, %487), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %489 : int = aten::size(%hidden_states.13, %237), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %490 : int = aten::size(%hidden_states.13, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.3 : Long(device=cpu) = prim::NumToTensor(%490), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %492 : int = aten::size(%hidden_states.13, %243), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %493 : int = aten::size(%hidden_states.13, %235), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %494 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.13, %237, %237, %236, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %495 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%494, %242, %237, %236, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %496 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%495, %243), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %497 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%496, %235, %237, %236, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %498 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%497, %234, %237, %236, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %499 : int[] = prim::ListConstruct(%489, %490, %234, %492, %493), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %hidden_states.15 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%498, %499, %239), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %501 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.3, %223), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %502 : int = aten::Int(%501), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %503 : int[] = prim::ListConstruct(%489, %502, %492, %493), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %value.1 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.15, %503), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %505 : int = aten::size(%key.1, %243), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %506 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %507 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%506, %242, %237, %236, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %508 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%507, %243, %237, %236, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.7 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%508, %235, %237, %505, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.1 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.1, %key.1, %value.1, %attention_mask.7, %222, %239, %221, %239), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %511 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.1, %242, %243), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.3 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%511, %237), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %513 : int[] = prim::ListConstruct(%418, %419, %231), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %514 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.3, %513), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %515 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%514, %237), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.11 : Tensor = prim::GetAttr[name="weight"](%o_proj.1)
  %hidden_states.17 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%515, %weight.11), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn/__module.model.layers.0.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %518 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.17, %hidden_states.9, %hidden_states.13)
  %519 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %520 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %521 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%518)
  %hidden_states.19 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%413, %519, %242), scope: __module.model/__module.model.layers.0 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.13 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.1)
  %hidden_states.21 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.19, %229, %239, %239, %241), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %525 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.21, %243), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %526 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm
  %variance.3 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%525, %526, %220, %241), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %528 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.3, %219, %242), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %529 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%528), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.23 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.21, %529), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.25 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.23, %229, %239, %239, %241), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %532 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.13, %hidden_states.25), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %533 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%532, %hidden_states.21)
  %534 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %535 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%533)
  %down_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name="down_proj"](%mlp.1)
  %up_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name="up_proj"](%mlp.1)
  %gate_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name="gate_proj"](%mlp.1)
  %weight.15 : Tensor = prim::GetAttr[name="weight"](%gate_proj.1)
  %input.1 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%534, %weight.15), scope: __module.model/__module.model.layers.0/__module.model.layers.0.mlp/__module.model.layers.0.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %541 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.1), scope: __module.model/__module.model.layers.0/__module.model.layers.0.mlp/__module.model.layers.0.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.17 : Tensor = prim::GetAttr[name="weight"](%up_proj.1)
  %543 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%534, %weight.17), scope: __module.model/__module.model.layers.0/__module.model.layers.0.mlp/__module.model.layers.0.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %544 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%541, %543), scope: __module.model/__module.model.layers.0/__module.model.layers.0.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.19 : Tensor = prim::GetAttr[name="weight"](%down_proj.1)
  %hidden_states.27 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%544, %weight.19), scope: __module.model/__module.model.layers.0/__module.model.layers.0.mlp/__module.model.layers.0.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.29 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%535, %hidden_states.27, %242), scope: __module.model/__module.model.layers.0 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %548 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.29, %520, %521)
  %549 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %550 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %551 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%548)
  %mlp.3 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_16.MistralMLP = prim::GetAttr[name="mlp"](%_1)
  %post_attention_layernorm.3 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_18.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_1)
  %self_attn.3 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_11.MistralAttention = prim::GetAttr[name="self_attn"](%_1)
  %input_layernorm.3 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_17.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_1)
  %weight.21 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.3)
  %hidden_states.31 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%549, %229, %239, %239, %241), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %558 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.31, %243), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %559 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm
  %variance.5 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%558, %559, %220, %241), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %561 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.5, %219, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %562 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%561), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.33 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.31, %562), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.35 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.33, %229, %239, %239, %241), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.37 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.21, %hidden_states.35), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %566 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.37, %hidden_states.31)
  %567 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %568 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%566)
  %o_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name="o_proj"](%self_attn.3)
  %v_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_9.Linear = prim::GetAttr[name="v_proj"](%self_attn.3)
  %k_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name="k_proj"](%self_attn.3)
  %q_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_7.Linear = prim::GetAttr[name="q_proj"](%self_attn.3)
  %573 : int = aten::size(%567, %237), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %574 : int = aten::size(%567, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.23 : Tensor = prim::GetAttr[name="weight"](%q_proj.3)
  %576 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%567, %weight.23), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn/__module.model.layers.1.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %577 : int[] = prim::ListConstruct(%573, %574, %231, %226), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %578 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%576, %577), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.3 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%578, %242, %243), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.25 : Tensor = prim::GetAttr[name="weight"](%k_proj.3)
  %581 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%567, %weight.25), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn/__module.model.layers.1.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %582 : int[] = prim::ListConstruct(%573, %574, %231, %226), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %583 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%581, %582), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.3 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%583, %242, %243), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.27 : Tensor = prim::GetAttr[name="weight"](%v_proj.3)
  %586 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%567, %weight.27), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn/__module.model.layers.1.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %587 : int[] = prim::ListConstruct(%573, %574, %231, %226), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %588 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%586, %587), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.3 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%588, %242, %243), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.7 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.7 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %592 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.3, %cos.7), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %593 : int = aten::size(%q.3, %235), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %594 : Long(device=cpu) = prim::NumToTensor(%593), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %595 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%594, %225), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %596 : int = aten::Int(%595), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %x1.5 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.3, %235, %237, %596, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %598 : int = aten::size(%q.3, %235), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %599 : Long(device=cpu) = prim::NumToTensor(%598), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %600 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%599, %225), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %601 : int = aten::Int(%600), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %x2.5 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.3, %235, %601, %236, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %603 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.5), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %604 : Tensor[] = prim::ListConstruct(%603, %x1.5), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %605 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%604, %231), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %606 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%605, %sin.7), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.3 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%592, %606, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %608 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.3, %cos.7), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %609 : int = aten::size(%k.3, %235), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %610 : Long(device=cpu) = prim::NumToTensor(%609), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %611 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%610, %225), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %612 : int = aten::Int(%611), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %x1.7 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.3, %235, %237, %612, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %614 : int = aten::size(%k.3, %235), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %615 : Long(device=cpu) = prim::NumToTensor(%614), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %616 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%615, %225), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %617 : int = aten::Int(%616), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %x2.7 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.3, %235, %617, %236, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %619 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.7), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %620 : Tensor[] = prim::ListConstruct(%619, %x1.7), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %621 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%620, %231), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %622 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%621, %sin.7), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.3 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%608, %622, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %624 : Tensor[] = prim::ListConstruct(%41, %key_states.3), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %hidden_states.39 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%624, %224), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %626 : Tensor[] = prim::ListConstruct(%42, %value_states.3), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %hidden_states.43 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%626, %224), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %628 : int = aten::size(%hidden_states.39, %237), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %629 : int = aten::size(%hidden_states.39, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.5 : Long(device=cpu) = prim::NumToTensor(%629), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %631 : int = aten::size(%hidden_states.39, %243), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %632 : int = aten::size(%hidden_states.39, %235), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %633 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.39, %237, %237, %236, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %634 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%633, %242, %237, %236, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %635 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%634, %243), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %636 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%635, %235, %237, %236, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %637 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%636, %234, %237, %236, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %638 : int[] = prim::ListConstruct(%628, %629, %234, %631, %632), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %hidden_states.41 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%637, %638, %239), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %640 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.5, %223), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %641 : int = aten::Int(%640), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %642 : int[] = prim::ListConstruct(%628, %641, %631, %632), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %key.3 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.41, %642), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %644 : int = aten::size(%hidden_states.43, %237), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %645 : int = aten::size(%hidden_states.43, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.7 : Long(device=cpu) = prim::NumToTensor(%645), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %647 : int = aten::size(%hidden_states.43, %243), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %648 : int = aten::size(%hidden_states.43, %235), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %649 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.43, %237, %237, %236, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %650 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%649, %242, %237, %236, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %651 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%650, %243), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %652 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%651, %235, %237, %236, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %653 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%652, %234, %237, %236, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %654 : int[] = prim::ListConstruct(%644, %645, %234, %647, %648), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %hidden_states.45 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%653, %654, %239), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %656 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.7, %223), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %657 : int = aten::Int(%656), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %658 : int[] = prim::ListConstruct(%644, %657, %647, %648), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %value.3 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.45, %658), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %660 : int = aten::size(%key.3, %243), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %661 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %662 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%661, %242, %237, %236, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %663 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%662, %243, %237, %236, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.9 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%663, %235, %237, %660, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.5 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.3, %key.3, %value.3, %attention_mask.9, %222, %239, %221, %239), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %666 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.5, %242, %243), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.7 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%666, %237), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %668 : int[] = prim::ListConstruct(%573, %574, %231), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %669 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.7, %668), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %670 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%669, %237), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.29 : Tensor = prim::GetAttr[name="weight"](%o_proj.3)
  %hidden_states.47 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%670, %weight.29), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn/__module.model.layers.1.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %673 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.47, %hidden_states.39, %hidden_states.43)
  %674 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %675 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %676 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%673)
  %hidden_states.49 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%568, %674, %242), scope: __module.model/__module.model.layers.1 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.31 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.3)
  %hidden_states.51 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.49, %229, %239, %239, %241), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %680 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.51, %243), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %681 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm
  %variance.7 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%680, %681, %220, %241), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %683 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.7, %219, %242), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %684 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%683), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.53 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.51, %684), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.55 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.53, %229, %239, %239, %241), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %687 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.31, %hidden_states.55), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %688 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%687, %hidden_states.51)
  %689 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %690 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%688)
  %down_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_14.Linear = prim::GetAttr[name="down_proj"](%mlp.3)
  %up_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_13.Linear = prim::GetAttr[name="up_proj"](%mlp.3)
  %gate_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_12.Linear = prim::GetAttr[name="gate_proj"](%mlp.3)
  %weight.33 : Tensor = prim::GetAttr[name="weight"](%gate_proj.3)
  %input.3 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%689, %weight.33), scope: __module.model/__module.model.layers.1/__module.model.layers.1.mlp/__module.model.layers.1.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %696 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.3), scope: __module.model/__module.model.layers.1/__module.model.layers.1.mlp/__module.model.layers.1.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.35 : Tensor = prim::GetAttr[name="weight"](%up_proj.3)
  %698 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%689, %weight.35), scope: __module.model/__module.model.layers.1/__module.model.layers.1.mlp/__module.model.layers.1.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %699 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%696, %698), scope: __module.model/__module.model.layers.1/__module.model.layers.1.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.37 : Tensor = prim::GetAttr[name="weight"](%down_proj.3)
  %hidden_states.57 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%699, %weight.37), scope: __module.model/__module.model.layers.1/__module.model.layers.1.mlp/__module.model.layers.1.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.59 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%690, %hidden_states.57, %242), scope: __module.model/__module.model.layers.1 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %703 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.59, %675, %676)
  %704 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %705 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %706 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%703)
  %mlp.5 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_29.MistralMLP = prim::GetAttr[name="mlp"](%_2)
  %post_attention_layernorm.5 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_31.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_2)
  %self_attn.5 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_24.MistralAttention = prim::GetAttr[name="self_attn"](%_2)
  %input_layernorm.5 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_30.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_2)
  %weight.39 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.5)
  %hidden_states.61 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%704, %229, %239, %239, %241), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %713 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.61, %243), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %714 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm
  %variance.9 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%713, %714, %220, %241), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %716 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.9, %219, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %717 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%716), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.63 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.61, %717), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.65 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.63, %229, %239, %239, %241), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.67 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.39, %hidden_states.65), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %721 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.67, %hidden_states.61)
  %722 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %723 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%721)
  %o_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_23.Linear = prim::GetAttr[name="o_proj"](%self_attn.5)
  %v_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_22.Linear = prim::GetAttr[name="v_proj"](%self_attn.5)
  %k_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_21.Linear = prim::GetAttr[name="k_proj"](%self_attn.5)
  %q_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_20.Linear = prim::GetAttr[name="q_proj"](%self_attn.5)
  %728 : int = aten::size(%722, %237), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %729 : int = aten::size(%722, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.41 : Tensor = prim::GetAttr[name="weight"](%q_proj.5)
  %731 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%722, %weight.41), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn/__module.model.layers.2.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %732 : int[] = prim::ListConstruct(%728, %729, %231, %226), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %733 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%731, %732), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.5 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%733, %242, %243), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.43 : Tensor = prim::GetAttr[name="weight"](%k_proj.5)
  %736 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%722, %weight.43), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn/__module.model.layers.2.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %737 : int[] = prim::ListConstruct(%728, %729, %231, %226), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %738 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%736, %737), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.5 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%738, %242, %243), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.45 : Tensor = prim::GetAttr[name="weight"](%v_proj.5)
  %741 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%722, %weight.45), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn/__module.model.layers.2.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %742 : int[] = prim::ListConstruct(%728, %729, %231, %226), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %743 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%741, %742), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.5 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%743, %242, %243), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.9 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.9 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %747 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.5, %cos.9), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %748 : int = aten::size(%q.5, %235), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %749 : Long(device=cpu) = prim::NumToTensor(%748), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %750 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%749, %225), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %751 : int = aten::Int(%750), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %x1.9 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.5, %235, %237, %751, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %753 : int = aten::size(%q.5, %235), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %754 : Long(device=cpu) = prim::NumToTensor(%753), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %755 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%754, %225), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %756 : int = aten::Int(%755), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %x2.9 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.5, %235, %756, %236, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %758 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.9), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %759 : Tensor[] = prim::ListConstruct(%758, %x1.9), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %760 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%759, %231), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %761 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%760, %sin.9), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.5 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%747, %761, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %763 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.5, %cos.9), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %764 : int = aten::size(%k.5, %235), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %765 : Long(device=cpu) = prim::NumToTensor(%764), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %766 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%765, %225), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %767 : int = aten::Int(%766), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %x1.11 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.5, %235, %237, %767, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %769 : int = aten::size(%k.5, %235), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %770 : Long(device=cpu) = prim::NumToTensor(%769), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %771 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%770, %225), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %772 : int = aten::Int(%771), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %x2.11 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.5, %235, %772, %236, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %774 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.11), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %775 : Tensor[] = prim::ListConstruct(%774, %x1.11), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %776 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%775, %231), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %777 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%776, %sin.9), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.5 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%763, %777, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %779 : Tensor[] = prim::ListConstruct(%43, %key_states.5), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %hidden_states.69 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%779, %224), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %781 : Tensor[] = prim::ListConstruct(%44, %value_states.5), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %hidden_states.73 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%781, %224), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %783 : int = aten::size(%hidden_states.69, %237), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %784 : int = aten::size(%hidden_states.69, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.9 : Long(device=cpu) = prim::NumToTensor(%784), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %786 : int = aten::size(%hidden_states.69, %243), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %787 : int = aten::size(%hidden_states.69, %235), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %788 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.69, %237, %237, %236, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %789 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%788, %242, %237, %236, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %790 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%789, %243), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %791 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%790, %235, %237, %236, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %792 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%791, %234, %237, %236, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %793 : int[] = prim::ListConstruct(%783, %784, %234, %786, %787), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %hidden_states.71 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%792, %793, %239), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %795 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.9, %223), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %796 : int = aten::Int(%795), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %797 : int[] = prim::ListConstruct(%783, %796, %786, %787), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %key.5 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.71, %797), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %799 : int = aten::size(%hidden_states.73, %237), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %800 : int = aten::size(%hidden_states.73, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.11 : Long(device=cpu) = prim::NumToTensor(%800), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %802 : int = aten::size(%hidden_states.73, %243), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %803 : int = aten::size(%hidden_states.73, %235), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %804 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.73, %237, %237, %236, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %805 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%804, %242, %237, %236, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %806 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%805, %243), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %807 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%806, %235, %237, %236, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %808 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%807, %234, %237, %236, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %809 : int[] = prim::ListConstruct(%799, %800, %234, %802, %803), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %hidden_states.75 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%808, %809, %239), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %811 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.11, %223), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %812 : int = aten::Int(%811), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %813 : int[] = prim::ListConstruct(%799, %812, %802, %803), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %value.5 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.75, %813), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %815 : int = aten::size(%key.5, %243), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %816 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %817 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%816, %242, %237, %236, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %818 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%817, %243, %237, %236, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.11 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%818, %235, %237, %815, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.9 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.5, %key.5, %value.5, %attention_mask.11, %222, %239, %221, %239), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %821 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.9, %242, %243), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.11 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%821, %237), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %823 : int[] = prim::ListConstruct(%728, %729, %231), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %824 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.11, %823), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %825 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%824, %237), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.47 : Tensor = prim::GetAttr[name="weight"](%o_proj.5)
  %hidden_states.77 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%825, %weight.47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn/__module.model.layers.2.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %828 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.77, %hidden_states.69, %hidden_states.73)
  %829 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %830 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %831 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%828)
  %hidden_states.79 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%723, %829, %242), scope: __module.model/__module.model.layers.2 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.49 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.5)
  %hidden_states.81 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.79, %229, %239, %239, %241), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %835 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.81, %243), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %836 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm
  %variance.11 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%835, %836, %220, %241), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %838 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.11, %219, %242), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %839 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%838), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.83 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.81, %839), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.85 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.83, %229, %239, %239, %241), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %842 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.49, %hidden_states.85), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %843 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%842, %hidden_states.81)
  %844 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %845 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%843)
  %down_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_27.Linear = prim::GetAttr[name="down_proj"](%mlp.5)
  %up_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_26.Linear = prim::GetAttr[name="up_proj"](%mlp.5)
  %gate_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_25.Linear = prim::GetAttr[name="gate_proj"](%mlp.5)
  %weight.51 : Tensor = prim::GetAttr[name="weight"](%gate_proj.5)
  %input.5 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%844, %weight.51), scope: __module.model/__module.model.layers.2/__module.model.layers.2.mlp/__module.model.layers.2.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %851 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.5), scope: __module.model/__module.model.layers.2/__module.model.layers.2.mlp/__module.model.layers.2.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.53 : Tensor = prim::GetAttr[name="weight"](%up_proj.5)
  %853 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%844, %weight.53), scope: __module.model/__module.model.layers.2/__module.model.layers.2.mlp/__module.model.layers.2.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %854 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%851, %853), scope: __module.model/__module.model.layers.2/__module.model.layers.2.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.55 : Tensor = prim::GetAttr[name="weight"](%down_proj.5)
  %hidden_states.87 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%854, %weight.55), scope: __module.model/__module.model.layers.2/__module.model.layers.2.mlp/__module.model.layers.2.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.89 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%845, %hidden_states.87, %242), scope: __module.model/__module.model.layers.2 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %858 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.89, %830, %831)
  %859 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %860 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %861 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%858)
  %mlp.7 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_42.MistralMLP = prim::GetAttr[name="mlp"](%_3)
  %post_attention_layernorm.7 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_44.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_3)
  %self_attn.7 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_37.MistralAttention = prim::GetAttr[name="self_attn"](%_3)
  %input_layernorm.7 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_43.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_3)
  %weight.57 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.7)
  %hidden_states.91 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%859, %229, %239, %239, %241), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %868 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.91, %243), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %869 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm
  %variance.13 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%868, %869, %220, %241), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %871 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.13, %219, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %872 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%871), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.93 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.91, %872), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.95 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.93, %229, %239, %239, %241), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.97 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.57, %hidden_states.95), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %876 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.97, %hidden_states.91)
  %877 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %878 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%876)
  %o_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_36.Linear = prim::GetAttr[name="o_proj"](%self_attn.7)
  %v_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_35.Linear = prim::GetAttr[name="v_proj"](%self_attn.7)
  %k_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_34.Linear = prim::GetAttr[name="k_proj"](%self_attn.7)
  %q_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_33.Linear = prim::GetAttr[name="q_proj"](%self_attn.7)
  %883 : int = aten::size(%877, %237), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %884 : int = aten::size(%877, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.59 : Tensor = prim::GetAttr[name="weight"](%q_proj.7)
  %886 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%877, %weight.59), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn/__module.model.layers.3.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %887 : int[] = prim::ListConstruct(%883, %884, %231, %226), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %888 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%886, %887), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.7 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%888, %242, %243), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.61 : Tensor = prim::GetAttr[name="weight"](%k_proj.7)
  %891 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%877, %weight.61), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn/__module.model.layers.3.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %892 : int[] = prim::ListConstruct(%883, %884, %231, %226), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %893 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%891, %892), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.7 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%893, %242, %243), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.63 : Tensor = prim::GetAttr[name="weight"](%v_proj.7)
  %896 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%877, %weight.63), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn/__module.model.layers.3.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %897 : int[] = prim::ListConstruct(%883, %884, %231, %226), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %898 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%896, %897), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.7 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%898, %242, %243), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.11 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.11 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %902 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.7, %cos.11), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %903 : int = aten::size(%q.7, %235), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %904 : Long(device=cpu) = prim::NumToTensor(%903), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %905 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%904, %225), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %906 : int = aten::Int(%905), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %x1.13 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.7, %235, %237, %906, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %908 : int = aten::size(%q.7, %235), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %909 : Long(device=cpu) = prim::NumToTensor(%908), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %910 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%909, %225), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %911 : int = aten::Int(%910), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %x2.13 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.7, %235, %911, %236, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %913 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.13), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %914 : Tensor[] = prim::ListConstruct(%913, %x1.13), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %915 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%914, %231), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %916 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%915, %sin.11), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.7 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%902, %916, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %918 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.7, %cos.11), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %919 : int = aten::size(%k.7, %235), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %920 : Long(device=cpu) = prim::NumToTensor(%919), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %921 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%920, %225), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %922 : int = aten::Int(%921), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %x1.15 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.7, %235, %237, %922, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %924 : int = aten::size(%k.7, %235), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %925 : Long(device=cpu) = prim::NumToTensor(%924), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %926 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%925, %225), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %927 : int = aten::Int(%926), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %x2.15 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.7, %235, %927, %236, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %929 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.15), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %930 : Tensor[] = prim::ListConstruct(%929, %x1.15), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %931 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%930, %231), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %932 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%931, %sin.11), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.7 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%918, %932, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %934 : Tensor[] = prim::ListConstruct(%45, %key_states.7), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %hidden_states.99 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%934, %224), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %936 : Tensor[] = prim::ListConstruct(%46, %value_states.7), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %hidden_states.103 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%936, %224), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %938 : int = aten::size(%hidden_states.99, %237), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %939 : int = aten::size(%hidden_states.99, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.13 : Long(device=cpu) = prim::NumToTensor(%939), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %941 : int = aten::size(%hidden_states.99, %243), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %942 : int = aten::size(%hidden_states.99, %235), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %943 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.99, %237, %237, %236, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %944 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%943, %242, %237, %236, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %945 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%944, %243), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %946 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%945, %235, %237, %236, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %947 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%946, %234, %237, %236, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %948 : int[] = prim::ListConstruct(%938, %939, %234, %941, %942), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %hidden_states.101 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%947, %948, %239), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %950 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.13, %223), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %951 : int = aten::Int(%950), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %952 : int[] = prim::ListConstruct(%938, %951, %941, %942), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %key.7 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.101, %952), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %954 : int = aten::size(%hidden_states.103, %237), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %955 : int = aten::size(%hidden_states.103, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.15 : Long(device=cpu) = prim::NumToTensor(%955), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %957 : int = aten::size(%hidden_states.103, %243), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %958 : int = aten::size(%hidden_states.103, %235), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %959 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.103, %237, %237, %236, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %960 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%959, %242, %237, %236, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %961 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%960, %243), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %962 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%961, %235, %237, %236, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %963 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%962, %234, %237, %236, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %964 : int[] = prim::ListConstruct(%954, %955, %234, %957, %958), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %hidden_states.105 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%963, %964, %239), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %966 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.15, %223), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %967 : int = aten::Int(%966), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %968 : int[] = prim::ListConstruct(%954, %967, %957, %958), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %value.7 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.105, %968), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %970 : int = aten::size(%key.7, %243), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %971 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %972 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%971, %242, %237, %236, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %973 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%972, %243, %237, %236, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.13 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%973, %235, %237, %970, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.13 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.7, %key.7, %value.7, %attention_mask.13, %222, %239, %221, %239), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %976 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.13, %242, %243), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.15 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%976, %237), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %978 : int[] = prim::ListConstruct(%883, %884, %231), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %979 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.15, %978), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %980 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%979, %237), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.65 : Tensor = prim::GetAttr[name="weight"](%o_proj.7)
  %hidden_states.107 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%980, %weight.65), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn/__module.model.layers.3.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %983 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.107, %hidden_states.99, %hidden_states.103)
  %984 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %985 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %986 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%983)
  %hidden_states.109 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%878, %984, %242), scope: __module.model/__module.model.layers.3 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.67 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.7)
  %hidden_states.111 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.109, %229, %239, %239, %241), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %990 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.111, %243), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %991 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm
  %variance.15 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%990, %991, %220, %241), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %993 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.15, %219, %242), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %994 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%993), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.113 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.111, %994), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.115 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.113, %229, %239, %239, %241), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %997 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.67, %hidden_states.115), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %998 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%997, %hidden_states.111)
  %999 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1000 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%998)
  %down_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_40.Linear = prim::GetAttr[name="down_proj"](%mlp.7)
  %up_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_39.Linear = prim::GetAttr[name="up_proj"](%mlp.7)
  %gate_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_38.Linear = prim::GetAttr[name="gate_proj"](%mlp.7)
  %weight.69 : Tensor = prim::GetAttr[name="weight"](%gate_proj.7)
  %input.7 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%999, %weight.69), scope: __module.model/__module.model.layers.3/__module.model.layers.3.mlp/__module.model.layers.3.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1006 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.7), scope: __module.model/__module.model.layers.3/__module.model.layers.3.mlp/__module.model.layers.3.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.71 : Tensor = prim::GetAttr[name="weight"](%up_proj.7)
  %1008 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%999, %weight.71), scope: __module.model/__module.model.layers.3/__module.model.layers.3.mlp/__module.model.layers.3.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1009 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%1006, %1008), scope: __module.model/__module.model.layers.3/__module.model.layers.3.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.73 : Tensor = prim::GetAttr[name="weight"](%down_proj.7)
  %hidden_states.117 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1009, %weight.73), scope: __module.model/__module.model.layers.3/__module.model.layers.3.mlp/__module.model.layers.3.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.119 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1000, %hidden_states.117, %242), scope: __module.model/__module.model.layers.3 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %1013 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.119, %985, %986)
  %1014 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1015 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1016 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1013)
  %mlp.9 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_55.MistralMLP = prim::GetAttr[name="mlp"](%_4)
  %post_attention_layernorm.9 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_57.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_4)
  %self_attn.9 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_50.MistralAttention = prim::GetAttr[name="self_attn"](%_4)
  %input_layernorm.9 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_56.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_4)
  %weight.75 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.9)
  %hidden_states.121 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%1014, %229, %239, %239, %241), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1023 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.121, %243), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1024 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm
  %variance.17 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1023, %1024, %220, %241), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1026 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.17, %219, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1027 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1026), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.123 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.121, %1027), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.125 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.123, %229, %239, %239, %241), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.127 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.75, %hidden_states.125), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1031 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.127, %hidden_states.121)
  %1032 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1033 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1031)
  %o_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_49.Linear = prim::GetAttr[name="o_proj"](%self_attn.9)
  %v_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_48.Linear = prim::GetAttr[name="v_proj"](%self_attn.9)
  %k_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_47.Linear = prim::GetAttr[name="k_proj"](%self_attn.9)
  %q_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_46.Linear = prim::GetAttr[name="q_proj"](%self_attn.9)
  %1038 : int = aten::size(%1032, %237), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %1039 : int = aten::size(%1032, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.77 : Tensor = prim::GetAttr[name="weight"](%q_proj.9)
  %1041 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1032, %weight.77), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn/__module.model.layers.4.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1042 : int[] = prim::ListConstruct(%1038, %1039, %231, %226), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %1043 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%1041, %1042), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.9 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1043, %242, %243), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.79 : Tensor = prim::GetAttr[name="weight"](%k_proj.9)
  %1046 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1032, %weight.79), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn/__module.model.layers.4.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1047 : int[] = prim::ListConstruct(%1038, %1039, %231, %226), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %1048 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1046, %1047), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.9 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1048, %242, %243), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.81 : Tensor = prim::GetAttr[name="weight"](%v_proj.9)
  %1051 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1032, %weight.81), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn/__module.model.layers.4.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1052 : int[] = prim::ListConstruct(%1038, %1039, %231, %226), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %1053 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1051, %1052), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.9 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1053, %242, %243), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.13 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.13 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %1057 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.9, %cos.13), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1058 : int = aten::size(%q.9, %235), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1059 : Long(device=cpu) = prim::NumToTensor(%1058), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %1060 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1059, %225), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1061 : int = aten::Int(%1060), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %x1.17 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.9, %235, %237, %1061, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1063 : int = aten::size(%q.9, %235), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1064 : Long(device=cpu) = prim::NumToTensor(%1063), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %1065 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1064, %225), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1066 : int = aten::Int(%1065), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %x2.17 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.9, %235, %1066, %236, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1068 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.17), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1069 : Tensor[] = prim::ListConstruct(%1068, %x1.17), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %1070 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1069, %231), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1071 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1070, %sin.13), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.9 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1057, %1071, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1073 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.9, %cos.13), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1074 : int = aten::size(%k.9, %235), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1075 : Long(device=cpu) = prim::NumToTensor(%1074), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %1076 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1075, %225), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1077 : int = aten::Int(%1076), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %x1.19 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.9, %235, %237, %1077, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1079 : int = aten::size(%k.9, %235), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1080 : Long(device=cpu) = prim::NumToTensor(%1079), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %1081 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1080, %225), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1082 : int = aten::Int(%1081), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %x2.19 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.9, %235, %1082, %236, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1084 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.19), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1085 : Tensor[] = prim::ListConstruct(%1084, %x1.19), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %1086 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1085, %231), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1087 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1086, %sin.13), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.9 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1073, %1087, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1089 : Tensor[] = prim::ListConstruct(%47, %key_states.9), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %hidden_states.129 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1089, %224), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %1091 : Tensor[] = prim::ListConstruct(%48, %value_states.9), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %hidden_states.133 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1091, %224), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %1093 : int = aten::size(%hidden_states.129, %237), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1094 : int = aten::size(%hidden_states.129, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.17 : Long(device=cpu) = prim::NumToTensor(%1094), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %1096 : int = aten::size(%hidden_states.129, %243), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1097 : int = aten::size(%hidden_states.129, %235), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1098 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.129, %237, %237, %236, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1099 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1098, %242, %237, %236, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1100 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1099, %243), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1101 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1100, %235, %237, %236, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1102 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1101, %234, %237, %236, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1103 : int[] = prim::ListConstruct(%1093, %1094, %234, %1096, %1097), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %hidden_states.131 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1102, %1103, %239), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1105 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.17, %223), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1106 : int = aten::Int(%1105), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %1107 : int[] = prim::ListConstruct(%1093, %1106, %1096, %1097), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %key.9 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.131, %1107), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1109 : int = aten::size(%hidden_states.133, %237), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1110 : int = aten::size(%hidden_states.133, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.19 : Long(device=cpu) = prim::NumToTensor(%1110), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %1112 : int = aten::size(%hidden_states.133, %243), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1113 : int = aten::size(%hidden_states.133, %235), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1114 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.133, %237, %237, %236, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1115 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1114, %242, %237, %236, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1116 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1115, %243), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1117 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1116, %235, %237, %236, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1118 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1117, %234, %237, %236, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1119 : int[] = prim::ListConstruct(%1109, %1110, %234, %1112, %1113), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %hidden_states.135 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1118, %1119, %239), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1121 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.19, %223), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1122 : int = aten::Int(%1121), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %1123 : int[] = prim::ListConstruct(%1109, %1122, %1112, %1113), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %value.9 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.135, %1123), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1125 : int = aten::size(%key.9, %243), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1126 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1127 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1126, %242, %237, %236, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1128 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1127, %243, %237, %236, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.15 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1128, %235, %237, %1125, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.17 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.9, %key.9, %value.9, %attention_mask.15, %222, %239, %221, %239), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %1131 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.17, %242, %243), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.19 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1131, %237), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %1133 : int[] = prim::ListConstruct(%1038, %1039, %231), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %1134 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.19, %1133), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %1135 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%1134, %237), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.83 : Tensor = prim::GetAttr[name="weight"](%o_proj.9)
  %hidden_states.137 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1135, %weight.83), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn/__module.model.layers.4.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1138 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.137, %hidden_states.129, %hidden_states.133)
  %1139 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1140 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1141 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1138)
  %hidden_states.139 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1033, %1139, %242), scope: __module.model/__module.model.layers.4 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.85 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.9)
  %hidden_states.141 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.139, %229, %239, %239, %241), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1145 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.141, %243), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1146 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm
  %variance.19 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1145, %1146, %220, %241), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1148 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.19, %219, %242), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1149 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1148), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.143 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.141, %1149), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.145 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.143, %229, %239, %239, %241), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1152 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.85, %hidden_states.145), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1153 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%1152, %hidden_states.141)
  %1154 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1155 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1153)
  %down_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_53.Linear = prim::GetAttr[name="down_proj"](%mlp.9)
  %up_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_52.Linear = prim::GetAttr[name="up_proj"](%mlp.9)
  %gate_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_51.Linear = prim::GetAttr[name="gate_proj"](%mlp.9)
  %weight.87 : Tensor = prim::GetAttr[name="weight"](%gate_proj.9)
  %input.9 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1154, %weight.87), scope: __module.model/__module.model.layers.4/__module.model.layers.4.mlp/__module.model.layers.4.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1161 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.9), scope: __module.model/__module.model.layers.4/__module.model.layers.4.mlp/__module.model.layers.4.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.89 : Tensor = prim::GetAttr[name="weight"](%up_proj.9)
  %1163 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1154, %weight.89), scope: __module.model/__module.model.layers.4/__module.model.layers.4.mlp/__module.model.layers.4.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1164 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%1161, %1163), scope: __module.model/__module.model.layers.4/__module.model.layers.4.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.91 : Tensor = prim::GetAttr[name="weight"](%down_proj.9)
  %hidden_states.147 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1164, %weight.91), scope: __module.model/__module.model.layers.4/__module.model.layers.4.mlp/__module.model.layers.4.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.149 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1155, %hidden_states.147, %242), scope: __module.model/__module.model.layers.4 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %1168 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.149, %1140, %1141)
  %1169 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1170 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1171 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1168)
  %mlp.11 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_68.MistralMLP = prim::GetAttr[name="mlp"](%_5)
  %post_attention_layernorm.11 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_70.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_5)
  %self_attn.11 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_63.MistralAttention = prim::GetAttr[name="self_attn"](%_5)
  %input_layernorm.11 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_69.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_5)
  %weight.93 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.11)
  %hidden_states.151 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%1169, %229, %239, %239, %241), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1178 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.151, %243), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1179 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm
  %variance.21 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1178, %1179, %220, %241), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1181 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.21, %219, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1182 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1181), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.153 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.151, %1182), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.155 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.153, %229, %239, %239, %241), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.157 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.93, %hidden_states.155), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1186 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.157, %hidden_states.151)
  %1187 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1188 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1186)
  %o_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_62.Linear = prim::GetAttr[name="o_proj"](%self_attn.11)
  %v_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_61.Linear = prim::GetAttr[name="v_proj"](%self_attn.11)
  %k_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_60.Linear = prim::GetAttr[name="k_proj"](%self_attn.11)
  %q_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_59.Linear = prim::GetAttr[name="q_proj"](%self_attn.11)
  %1193 : int = aten::size(%1187, %237), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %1194 : int = aten::size(%1187, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.95 : Tensor = prim::GetAttr[name="weight"](%q_proj.11)
  %1196 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1187, %weight.95), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn/__module.model.layers.5.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1197 : int[] = prim::ListConstruct(%1193, %1194, %231, %226), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %1198 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%1196, %1197), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.11 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1198, %242, %243), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.97 : Tensor = prim::GetAttr[name="weight"](%k_proj.11)
  %1201 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1187, %weight.97), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn/__module.model.layers.5.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1202 : int[] = prim::ListConstruct(%1193, %1194, %231, %226), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %1203 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1201, %1202), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.11 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1203, %242, %243), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.99 : Tensor = prim::GetAttr[name="weight"](%v_proj.11)
  %1206 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1187, %weight.99), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn/__module.model.layers.5.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1207 : int[] = prim::ListConstruct(%1193, %1194, %231, %226), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %1208 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1206, %1207), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.11 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1208, %242, %243), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.15 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.15 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %1212 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.11, %cos.15), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1213 : int = aten::size(%q.11, %235), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1214 : Long(device=cpu) = prim::NumToTensor(%1213), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %1215 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1214, %225), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1216 : int = aten::Int(%1215), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %x1.21 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.11, %235, %237, %1216, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1218 : int = aten::size(%q.11, %235), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1219 : Long(device=cpu) = prim::NumToTensor(%1218), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %1220 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1219, %225), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1221 : int = aten::Int(%1220), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %x2.21 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.11, %235, %1221, %236, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1223 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.21), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1224 : Tensor[] = prim::ListConstruct(%1223, %x1.21), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %1225 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1224, %231), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1226 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1225, %sin.15), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.11 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1212, %1226, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1228 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.11, %cos.15), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1229 : int = aten::size(%k.11, %235), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1230 : Long(device=cpu) = prim::NumToTensor(%1229), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %1231 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1230, %225), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1232 : int = aten::Int(%1231), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %x1.23 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.11, %235, %237, %1232, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1234 : int = aten::size(%k.11, %235), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1235 : Long(device=cpu) = prim::NumToTensor(%1234), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %1236 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1235, %225), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1237 : int = aten::Int(%1236), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %x2.23 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.11, %235, %1237, %236, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1239 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.23), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1240 : Tensor[] = prim::ListConstruct(%1239, %x1.23), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %1241 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1240, %231), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1242 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1241, %sin.15), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.11 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1228, %1242, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1244 : Tensor[] = prim::ListConstruct(%49, %key_states.11), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %hidden_states.159 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1244, %224), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %1246 : Tensor[] = prim::ListConstruct(%50, %value_states.11), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %hidden_states.163 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1246, %224), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %1248 : int = aten::size(%hidden_states.159, %237), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1249 : int = aten::size(%hidden_states.159, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.21 : Long(device=cpu) = prim::NumToTensor(%1249), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %1251 : int = aten::size(%hidden_states.159, %243), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1252 : int = aten::size(%hidden_states.159, %235), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1253 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.159, %237, %237, %236, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1254 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1253, %242, %237, %236, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1255 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1254, %243), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1256 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1255, %235, %237, %236, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1257 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1256, %234, %237, %236, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1258 : int[] = prim::ListConstruct(%1248, %1249, %234, %1251, %1252), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %hidden_states.161 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1257, %1258, %239), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1260 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.21, %223), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1261 : int = aten::Int(%1260), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %1262 : int[] = prim::ListConstruct(%1248, %1261, %1251, %1252), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %key.11 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.161, %1262), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1264 : int = aten::size(%hidden_states.163, %237), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1265 : int = aten::size(%hidden_states.163, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.23 : Long(device=cpu) = prim::NumToTensor(%1265), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %1267 : int = aten::size(%hidden_states.163, %243), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1268 : int = aten::size(%hidden_states.163, %235), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1269 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.163, %237, %237, %236, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1270 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1269, %242, %237, %236, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1271 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1270, %243), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1272 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1271, %235, %237, %236, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1273 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1272, %234, %237, %236, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1274 : int[] = prim::ListConstruct(%1264, %1265, %234, %1267, %1268), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %hidden_states.165 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1273, %1274, %239), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1276 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.23, %223), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1277 : int = aten::Int(%1276), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %1278 : int[] = prim::ListConstruct(%1264, %1277, %1267, %1268), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %value.11 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.165, %1278), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1280 : int = aten::size(%key.11, %243), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1281 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1282 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1281, %242, %237, %236, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1283 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1282, %243, %237, %236, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.17 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1283, %235, %237, %1280, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.21 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.11, %key.11, %value.11, %attention_mask.17, %222, %239, %221, %239), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %1286 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.21, %242, %243), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.23 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1286, %237), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %1288 : int[] = prim::ListConstruct(%1193, %1194, %231), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %1289 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.23, %1288), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %1290 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%1289, %237), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.101 : Tensor = prim::GetAttr[name="weight"](%o_proj.11)
  %hidden_states.167 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1290, %weight.101), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn/__module.model.layers.5.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1293 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.167, %hidden_states.159, %hidden_states.163)
  %1294 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1295 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1296 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1293)
  %hidden_states.169 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1188, %1294, %242), scope: __module.model/__module.model.layers.5 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.103 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.11)
  %hidden_states.171 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.169, %229, %239, %239, %241), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1300 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.171, %243), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1301 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm
  %variance.23 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1300, %1301, %220, %241), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1303 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.23, %219, %242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1304 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1303), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.173 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.171, %1304), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.175 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.173, %229, %239, %239, %241), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1307 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.103, %hidden_states.175), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1308 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%1307, %hidden_states.171)
  %1309 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1310 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1308)
  %down_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_66.Linear = prim::GetAttr[name="down_proj"](%mlp.11)
  %up_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_65.Linear = prim::GetAttr[name="up_proj"](%mlp.11)
  %gate_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_64.Linear = prim::GetAttr[name="gate_proj"](%mlp.11)
  %weight.105 : Tensor = prim::GetAttr[name="weight"](%gate_proj.11)
  %input.11 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1309, %weight.105), scope: __module.model/__module.model.layers.5/__module.model.layers.5.mlp/__module.model.layers.5.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1316 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.11), scope: __module.model/__module.model.layers.5/__module.model.layers.5.mlp/__module.model.layers.5.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.107 : Tensor = prim::GetAttr[name="weight"](%up_proj.11)
  %1318 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1309, %weight.107), scope: __module.model/__module.model.layers.5/__module.model.layers.5.mlp/__module.model.layers.5.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1319 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%1316, %1318), scope: __module.model/__module.model.layers.5/__module.model.layers.5.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.109 : Tensor = prim::GetAttr[name="weight"](%down_proj.11)
  %hidden_states.177 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1319, %weight.109), scope: __module.model/__module.model.layers.5/__module.model.layers.5.mlp/__module.model.layers.5.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.179 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1310, %hidden_states.177, %242), scope: __module.model/__module.model.layers.5 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %1323 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.179, %1295, %1296)
  %1324 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1325 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1326 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1323)
  %mlp.13 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_81.MistralMLP = prim::GetAttr[name="mlp"](%_6)
  %post_attention_layernorm.13 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_83.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_6)
  %self_attn.13 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_76.MistralAttention = prim::GetAttr[name="self_attn"](%_6)
  %input_layernorm.13 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_82.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_6)
  %weight.111 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.13)
  %hidden_states.181 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%1324, %229, %239, %239, %241), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1333 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.181, %243), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1334 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm
  %variance.25 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1333, %1334, %220, %241), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1336 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.25, %219, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1337 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1336), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.183 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.181, %1337), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.185 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.183, %229, %239, %239, %241), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.187 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.111, %hidden_states.185), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1341 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.187, %hidden_states.181)
  %1342 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1343 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1341)
  %o_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_75.Linear = prim::GetAttr[name="o_proj"](%self_attn.13)
  %v_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_74.Linear = prim::GetAttr[name="v_proj"](%self_attn.13)
  %k_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_73.Linear = prim::GetAttr[name="k_proj"](%self_attn.13)
  %q_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_72.Linear = prim::GetAttr[name="q_proj"](%self_attn.13)
  %1348 : int = aten::size(%1342, %237), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %1349 : int = aten::size(%1342, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.113 : Tensor = prim::GetAttr[name="weight"](%q_proj.13)
  %1351 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1342, %weight.113), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn/__module.model.layers.6.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1352 : int[] = prim::ListConstruct(%1348, %1349, %231, %226), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1353 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%1351, %1352), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.13 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1353, %242, %243), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.115 : Tensor = prim::GetAttr[name="weight"](%k_proj.13)
  %1356 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1342, %weight.115), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn/__module.model.layers.6.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1357 : int[] = prim::ListConstruct(%1348, %1349, %231, %226), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1358 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1356, %1357), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.13 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1358, %242, %243), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.117 : Tensor = prim::GetAttr[name="weight"](%v_proj.13)
  %1361 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1342, %weight.117), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn/__module.model.layers.6.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1362 : int[] = prim::ListConstruct(%1348, %1349, %231, %226), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1363 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1361, %1362), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.13 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1363, %242, %243), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.17 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.17 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %1367 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.13, %cos.17), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1368 : int = aten::size(%q.13, %235), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1369 : Long(device=cpu) = prim::NumToTensor(%1368), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1370 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1369, %225), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1371 : int = aten::Int(%1370), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %x1.25 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.13, %235, %237, %1371, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1373 : int = aten::size(%q.13, %235), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1374 : Long(device=cpu) = prim::NumToTensor(%1373), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1375 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1374, %225), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1376 : int = aten::Int(%1375), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %x2.25 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.13, %235, %1376, %236, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1378 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.25), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1379 : Tensor[] = prim::ListConstruct(%1378, %x1.25), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1380 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1379, %231), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1381 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1380, %sin.17), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.13 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1367, %1381, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1383 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.13, %cos.17), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1384 : int = aten::size(%k.13, %235), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1385 : Long(device=cpu) = prim::NumToTensor(%1384), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1386 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1385, %225), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1387 : int = aten::Int(%1386), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %x1.27 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.13, %235, %237, %1387, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1389 : int = aten::size(%k.13, %235), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1390 : Long(device=cpu) = prim::NumToTensor(%1389), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1391 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1390, %225), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1392 : int = aten::Int(%1391), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %x2.27 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.13, %235, %1392, %236, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1394 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.27), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1395 : Tensor[] = prim::ListConstruct(%1394, %x1.27), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1396 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1395, %231), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1397 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1396, %sin.17), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.13 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1383, %1397, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1399 : Tensor[] = prim::ListConstruct(%51, %key_states.13), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %hidden_states.189 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1399, %224), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %1401 : Tensor[] = prim::ListConstruct(%52, %value_states.13), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %hidden_states.193 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1401, %224), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %1403 : int = aten::size(%hidden_states.189, %237), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1404 : int = aten::size(%hidden_states.189, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.25 : Long(device=cpu) = prim::NumToTensor(%1404), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1406 : int = aten::size(%hidden_states.189, %243), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1407 : int = aten::size(%hidden_states.189, %235), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1408 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.189, %237, %237, %236, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1409 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1408, %242, %237, %236, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1410 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1409, %243), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1411 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1410, %235, %237, %236, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1412 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1411, %234, %237, %236, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1413 : int[] = prim::ListConstruct(%1403, %1404, %234, %1406, %1407), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %hidden_states.191 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1412, %1413, %239), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1415 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.25, %223), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1416 : int = aten::Int(%1415), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1417 : int[] = prim::ListConstruct(%1403, %1416, %1406, %1407), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %key.13 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.191, %1417), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1419 : int = aten::size(%hidden_states.193, %237), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1420 : int = aten::size(%hidden_states.193, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.27 : Long(device=cpu) = prim::NumToTensor(%1420), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1422 : int = aten::size(%hidden_states.193, %243), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1423 : int = aten::size(%hidden_states.193, %235), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1424 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.193, %237, %237, %236, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1425 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1424, %242, %237, %236, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1426 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1425, %243), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1427 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1426, %235, %237, %236, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1428 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1427, %234, %237, %236, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1429 : int[] = prim::ListConstruct(%1419, %1420, %234, %1422, %1423), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %hidden_states.195 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1428, %1429, %239), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1431 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.27, %223), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1432 : int = aten::Int(%1431), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1433 : int[] = prim::ListConstruct(%1419, %1432, %1422, %1423), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %value.13 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.195, %1433), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1435 : int = aten::size(%key.13, %243), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1436 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1437 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1436, %242, %237, %236, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1438 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1437, %243, %237, %236, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.19 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1438, %235, %237, %1435, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.25 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.13, %key.13, %value.13, %attention_mask.19, %222, %239, %221, %239), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %1441 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.25, %242, %243), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.27 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1441, %237), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %1443 : int[] = prim::ListConstruct(%1348, %1349, %231), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1444 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.27, %1443), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %1445 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%1444, %237), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.119 : Tensor = prim::GetAttr[name="weight"](%o_proj.13)
  %hidden_states.197 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1445, %weight.119), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn/__module.model.layers.6.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1448 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.197, %hidden_states.189, %hidden_states.193)
  %1449 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1450 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1451 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1448)
  %hidden_states.199 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1343, %1449, %242), scope: __module.model/__module.model.layers.6 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.121 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.13)
  %hidden_states.201 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.199, %229, %239, %239, %241), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1455 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.201, %243), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1456 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm
  %variance.27 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1455, %1456, %220, %241), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1458 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.27, %219, %242), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1459 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1458), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.203 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.201, %1459), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.205 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.203, %229, %239, %239, %241), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1462 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.121, %hidden_states.205), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1463 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%1462, %hidden_states.201)
  %1464 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1465 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1463)
  %down_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_79.Linear = prim::GetAttr[name="down_proj"](%mlp.13)
  %up_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_78.Linear = prim::GetAttr[name="up_proj"](%mlp.13)
  %gate_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_77.Linear = prim::GetAttr[name="gate_proj"](%mlp.13)
  %weight.123 : Tensor = prim::GetAttr[name="weight"](%gate_proj.13)
  %input.13 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1464, %weight.123), scope: __module.model/__module.model.layers.6/__module.model.layers.6.mlp/__module.model.layers.6.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1471 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.13), scope: __module.model/__module.model.layers.6/__module.model.layers.6.mlp/__module.model.layers.6.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.125 : Tensor = prim::GetAttr[name="weight"](%up_proj.13)
  %1473 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1464, %weight.125), scope: __module.model/__module.model.layers.6/__module.model.layers.6.mlp/__module.model.layers.6.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1474 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%1471, %1473), scope: __module.model/__module.model.layers.6/__module.model.layers.6.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.127 : Tensor = prim::GetAttr[name="weight"](%down_proj.13)
  %hidden_states.207 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1474, %weight.127), scope: __module.model/__module.model.layers.6/__module.model.layers.6.mlp/__module.model.layers.6.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.209 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1465, %hidden_states.207, %242), scope: __module.model/__module.model.layers.6 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %1478 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.209, %1450, %1451)
  %1479 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1480 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1481 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1478)
  %mlp.15 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_94.MistralMLP = prim::GetAttr[name="mlp"](%_7)
  %post_attention_layernorm.15 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_96.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_7)
  %self_attn.15 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_89.MistralAttention = prim::GetAttr[name="self_attn"](%_7)
  %input_layernorm.15 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_95.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_7)
  %weight.129 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.15)
  %hidden_states.211 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%1479, %229, %239, %239, %241), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1488 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.211, %243), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1489 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm
  %variance.29 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1488, %1489, %220, %241), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1491 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.29, %219, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1492 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1491), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.213 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.211, %1492), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.215 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.213, %229, %239, %239, %241), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.217 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.129, %hidden_states.215), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1496 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.217, %hidden_states.211)
  %1497 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1498 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1496)
  %o_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_88.Linear = prim::GetAttr[name="o_proj"](%self_attn.15)
  %v_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_87.Linear = prim::GetAttr[name="v_proj"](%self_attn.15)
  %k_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_86.Linear = prim::GetAttr[name="k_proj"](%self_attn.15)
  %q_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_85.Linear = prim::GetAttr[name="q_proj"](%self_attn.15)
  %1503 : int = aten::size(%1497, %237), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %1504 : int = aten::size(%1497, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.131 : Tensor = prim::GetAttr[name="weight"](%q_proj.15)
  %1506 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1497, %weight.131), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn/__module.model.layers.7.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1507 : int[] = prim::ListConstruct(%1503, %1504, %231, %226), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1508 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%1506, %1507), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.15 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1508, %242, %243), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.133 : Tensor = prim::GetAttr[name="weight"](%k_proj.15)
  %1511 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1497, %weight.133), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn/__module.model.layers.7.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1512 : int[] = prim::ListConstruct(%1503, %1504, %231, %226), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1513 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1511, %1512), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.15 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1513, %242, %243), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.135 : Tensor = prim::GetAttr[name="weight"](%v_proj.15)
  %1516 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1497, %weight.135), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn/__module.model.layers.7.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1517 : int[] = prim::ListConstruct(%1503, %1504, %231, %226), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1518 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1516, %1517), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.15 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1518, %242, %243), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.19 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.19 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %1522 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.15, %cos.19), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1523 : int = aten::size(%q.15, %235), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1524 : Long(device=cpu) = prim::NumToTensor(%1523), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1525 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1524, %225), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1526 : int = aten::Int(%1525), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %x1.29 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.15, %235, %237, %1526, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1528 : int = aten::size(%q.15, %235), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1529 : Long(device=cpu) = prim::NumToTensor(%1528), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1530 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1529, %225), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1531 : int = aten::Int(%1530), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %x2.29 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.15, %235, %1531, %236, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1533 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.29), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1534 : Tensor[] = prim::ListConstruct(%1533, %x1.29), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1535 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1534, %231), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1536 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1535, %sin.19), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.15 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1522, %1536, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1538 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.15, %cos.19), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1539 : int = aten::size(%k.15, %235), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1540 : Long(device=cpu) = prim::NumToTensor(%1539), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1541 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1540, %225), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1542 : int = aten::Int(%1541), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %x1.31 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.15, %235, %237, %1542, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1544 : int = aten::size(%k.15, %235), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1545 : Long(device=cpu) = prim::NumToTensor(%1544), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1546 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1545, %225), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1547 : int = aten::Int(%1546), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %x2.31 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.15, %235, %1547, %236, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1549 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.31), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1550 : Tensor[] = prim::ListConstruct(%1549, %x1.31), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1551 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1550, %231), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1552 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1551, %sin.19), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.15 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1538, %1552, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1554 : Tensor[] = prim::ListConstruct(%53, %key_states.15), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %hidden_states.219 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1554, %224), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %1556 : Tensor[] = prim::ListConstruct(%54, %value_states.15), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %hidden_states.223 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1556, %224), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %1558 : int = aten::size(%hidden_states.219, %237), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1559 : int = aten::size(%hidden_states.219, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.29 : Long(device=cpu) = prim::NumToTensor(%1559), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1561 : int = aten::size(%hidden_states.219, %243), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1562 : int = aten::size(%hidden_states.219, %235), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1563 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.219, %237, %237, %236, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1564 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1563, %242, %237, %236, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1565 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1564, %243), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1566 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1565, %235, %237, %236, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1567 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1566, %234, %237, %236, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1568 : int[] = prim::ListConstruct(%1558, %1559, %234, %1561, %1562), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %hidden_states.221 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1567, %1568, %239), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1570 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.29, %223), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1571 : int = aten::Int(%1570), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1572 : int[] = prim::ListConstruct(%1558, %1571, %1561, %1562), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %key.15 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.221, %1572), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1574 : int = aten::size(%hidden_states.223, %237), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1575 : int = aten::size(%hidden_states.223, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.31 : Long(device=cpu) = prim::NumToTensor(%1575), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1577 : int = aten::size(%hidden_states.223, %243), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1578 : int = aten::size(%hidden_states.223, %235), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1579 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.223, %237, %237, %236, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1580 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1579, %242, %237, %236, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1581 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1580, %243), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1582 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1581, %235, %237, %236, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1583 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1582, %234, %237, %236, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1584 : int[] = prim::ListConstruct(%1574, %1575, %234, %1577, %1578), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %hidden_states.225 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1583, %1584, %239), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1586 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.31, %223), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1587 : int = aten::Int(%1586), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1588 : int[] = prim::ListConstruct(%1574, %1587, %1577, %1578), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %value.15 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.225, %1588), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1590 : int = aten::size(%key.15, %243), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1591 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1592 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1591, %242, %237, %236, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1593 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1592, %243, %237, %236, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.21 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1593, %235, %237, %1590, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.29 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.15, %key.15, %value.15, %attention_mask.21, %222, %239, %221, %239), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %1596 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.29, %242, %243), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.31 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1596, %237), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %1598 : int[] = prim::ListConstruct(%1503, %1504, %231), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1599 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.31, %1598), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %1600 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%1599, %237), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.137 : Tensor = prim::GetAttr[name="weight"](%o_proj.15)
  %hidden_states.227 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1600, %weight.137), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn/__module.model.layers.7.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1603 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.227, %hidden_states.219, %hidden_states.223)
  %1604 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1605 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1606 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1603)
  %hidden_states.229 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1498, %1604, %242), scope: __module.model/__module.model.layers.7 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.139 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.15)
  %hidden_states.231 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.229, %229, %239, %239, %241), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1610 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.231, %243), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1611 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm
  %variance.31 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1610, %1611, %220, %241), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1613 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.31, %219, %242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1614 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1613), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.233 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.231, %1614), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.235 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.233, %229, %239, %239, %241), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1617 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.139, %hidden_states.235), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1618 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%1617, %hidden_states.231)
  %1619 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1620 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1618)
  %down_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_92.Linear = prim::GetAttr[name="down_proj"](%mlp.15)
  %up_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_91.Linear = prim::GetAttr[name="up_proj"](%mlp.15)
  %gate_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_90.Linear = prim::GetAttr[name="gate_proj"](%mlp.15)
  %weight.141 : Tensor = prim::GetAttr[name="weight"](%gate_proj.15)
  %input.15 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1619, %weight.141), scope: __module.model/__module.model.layers.7/__module.model.layers.7.mlp/__module.model.layers.7.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1626 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.15), scope: __module.model/__module.model.layers.7/__module.model.layers.7.mlp/__module.model.layers.7.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.143 : Tensor = prim::GetAttr[name="weight"](%up_proj.15)
  %1628 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1619, %weight.143), scope: __module.model/__module.model.layers.7/__module.model.layers.7.mlp/__module.model.layers.7.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1629 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%1626, %1628), scope: __module.model/__module.model.layers.7/__module.model.layers.7.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.145 : Tensor = prim::GetAttr[name="weight"](%down_proj.15)
  %hidden_states.237 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1629, %weight.145), scope: __module.model/__module.model.layers.7/__module.model.layers.7.mlp/__module.model.layers.7.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.239 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1620, %hidden_states.237, %242), scope: __module.model/__module.model.layers.7 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %1633 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.239, %1605, %1606)
  %1634 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1635 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1636 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1633)
  %mlp.17 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_107.MistralMLP = prim::GetAttr[name="mlp"](%_8)
  %post_attention_layernorm.17 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_109.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_8)
  %self_attn.17 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_102.MistralAttention = prim::GetAttr[name="self_attn"](%_8)
  %input_layernorm.17 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_108.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_8)
  %weight.147 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.17)
  %hidden_states.241 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%1634, %229, %239, %239, %241), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1643 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.241, %243), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1644 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm
  %variance.33 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1643, %1644, %220, %241), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1646 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.33, %219, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1647 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1646), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.243 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.241, %1647), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.245 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.243, %229, %239, %239, %241), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.247 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.147, %hidden_states.245), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1651 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.247, %hidden_states.241)
  %1652 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1653 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1651)
  %o_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_101.Linear = prim::GetAttr[name="o_proj"](%self_attn.17)
  %v_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_100.Linear = prim::GetAttr[name="v_proj"](%self_attn.17)
  %k_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_99.Linear = prim::GetAttr[name="k_proj"](%self_attn.17)
  %q_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_98.Linear = prim::GetAttr[name="q_proj"](%self_attn.17)
  %1658 : int = aten::size(%1652, %237), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %1659 : int = aten::size(%1652, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.149 : Tensor = prim::GetAttr[name="weight"](%q_proj.17)
  %1661 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1652, %weight.149), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn/__module.model.layers.8.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1662 : int[] = prim::ListConstruct(%1658, %1659, %231, %226), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1663 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%1661, %1662), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.17 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1663, %242, %243), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.151 : Tensor = prim::GetAttr[name="weight"](%k_proj.17)
  %1666 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1652, %weight.151), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn/__module.model.layers.8.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1667 : int[] = prim::ListConstruct(%1658, %1659, %231, %226), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1668 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1666, %1667), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.17 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1668, %242, %243), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.153 : Tensor = prim::GetAttr[name="weight"](%v_proj.17)
  %1671 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1652, %weight.153), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn/__module.model.layers.8.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1672 : int[] = prim::ListConstruct(%1658, %1659, %231, %226), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1673 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1671, %1672), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.17 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1673, %242, %243), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.21 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.21 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %1677 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.17, %cos.21), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1678 : int = aten::size(%q.17, %235), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1679 : Long(device=cpu) = prim::NumToTensor(%1678), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1680 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1679, %225), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1681 : int = aten::Int(%1680), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %x1.33 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.17, %235, %237, %1681, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1683 : int = aten::size(%q.17, %235), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1684 : Long(device=cpu) = prim::NumToTensor(%1683), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1685 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1684, %225), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1686 : int = aten::Int(%1685), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %x2.33 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.17, %235, %1686, %236, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1688 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.33), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1689 : Tensor[] = prim::ListConstruct(%1688, %x1.33), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1690 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1689, %231), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1691 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1690, %sin.21), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.17 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1677, %1691, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1693 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.17, %cos.21), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1694 : int = aten::size(%k.17, %235), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1695 : Long(device=cpu) = prim::NumToTensor(%1694), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1696 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1695, %225), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1697 : int = aten::Int(%1696), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %x1.35 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.17, %235, %237, %1697, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1699 : int = aten::size(%k.17, %235), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1700 : Long(device=cpu) = prim::NumToTensor(%1699), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1701 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1700, %225), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1702 : int = aten::Int(%1701), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %x2.35 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.17, %235, %1702, %236, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1704 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.35), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1705 : Tensor[] = prim::ListConstruct(%1704, %x1.35), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1706 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1705, %231), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1707 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1706, %sin.21), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.17 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1693, %1707, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1709 : Tensor[] = prim::ListConstruct(%55, %key_states.17), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %hidden_states.249 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1709, %224), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %1711 : Tensor[] = prim::ListConstruct(%56, %value_states.17), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %hidden_states.253 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1711, %224), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %1713 : int = aten::size(%hidden_states.249, %237), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1714 : int = aten::size(%hidden_states.249, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.33 : Long(device=cpu) = prim::NumToTensor(%1714), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1716 : int = aten::size(%hidden_states.249, %243), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1717 : int = aten::size(%hidden_states.249, %235), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1718 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.249, %237, %237, %236, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1719 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1718, %242, %237, %236, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1720 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1719, %243), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1721 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1720, %235, %237, %236, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1722 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1721, %234, %237, %236, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1723 : int[] = prim::ListConstruct(%1713, %1714, %234, %1716, %1717), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %hidden_states.251 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1722, %1723, %239), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1725 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.33, %223), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1726 : int = aten::Int(%1725), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1727 : int[] = prim::ListConstruct(%1713, %1726, %1716, %1717), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %key.17 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.251, %1727), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1729 : int = aten::size(%hidden_states.253, %237), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1730 : int = aten::size(%hidden_states.253, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.35 : Long(device=cpu) = prim::NumToTensor(%1730), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1732 : int = aten::size(%hidden_states.253, %243), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1733 : int = aten::size(%hidden_states.253, %235), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1734 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.253, %237, %237, %236, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1735 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1734, %242, %237, %236, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1736 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1735, %243), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1737 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1736, %235, %237, %236, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1738 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1737, %234, %237, %236, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1739 : int[] = prim::ListConstruct(%1729, %1730, %234, %1732, %1733), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %hidden_states.255 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1738, %1739, %239), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1741 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.35, %223), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1742 : int = aten::Int(%1741), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1743 : int[] = prim::ListConstruct(%1729, %1742, %1732, %1733), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %value.17 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.255, %1743), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1745 : int = aten::size(%key.17, %243), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1746 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1747 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1746, %242, %237, %236, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1748 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1747, %243, %237, %236, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.23 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1748, %235, %237, %1745, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.33 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.17, %key.17, %value.17, %attention_mask.23, %222, %239, %221, %239), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %1751 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.33, %242, %243), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.35 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1751, %237), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %1753 : int[] = prim::ListConstruct(%1658, %1659, %231), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1754 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.35, %1753), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %1755 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%1754, %237), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.155 : Tensor = prim::GetAttr[name="weight"](%o_proj.17)
  %hidden_states.257 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1755, %weight.155), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn/__module.model.layers.8.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1758 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.257, %hidden_states.249, %hidden_states.253)
  %1759 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1760 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1761 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1758)
  %hidden_states.259 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1653, %1759, %242), scope: __module.model/__module.model.layers.8 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.157 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.17)
  %hidden_states.261 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.259, %229, %239, %239, %241), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1765 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.261, %243), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1766 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm
  %variance.35 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1765, %1766, %220, %241), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1768 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.35, %219, %242), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1769 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1768), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.263 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.261, %1769), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.265 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.263, %229, %239, %239, %241), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1772 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.157, %hidden_states.265), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1773 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%1772, %hidden_states.261)
  %1774 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1775 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1773)
  %down_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_105.Linear = prim::GetAttr[name="down_proj"](%mlp.17)
  %up_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_104.Linear = prim::GetAttr[name="up_proj"](%mlp.17)
  %gate_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_103.Linear = prim::GetAttr[name="gate_proj"](%mlp.17)
  %weight.159 : Tensor = prim::GetAttr[name="weight"](%gate_proj.17)
  %input.17 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1774, %weight.159), scope: __module.model/__module.model.layers.8/__module.model.layers.8.mlp/__module.model.layers.8.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1781 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.17), scope: __module.model/__module.model.layers.8/__module.model.layers.8.mlp/__module.model.layers.8.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.161 : Tensor = prim::GetAttr[name="weight"](%up_proj.17)
  %1783 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1774, %weight.161), scope: __module.model/__module.model.layers.8/__module.model.layers.8.mlp/__module.model.layers.8.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1784 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%1781, %1783), scope: __module.model/__module.model.layers.8/__module.model.layers.8.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.163 : Tensor = prim::GetAttr[name="weight"](%down_proj.17)
  %hidden_states.267 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1784, %weight.163), scope: __module.model/__module.model.layers.8/__module.model.layers.8.mlp/__module.model.layers.8.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.269 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1775, %hidden_states.267, %242), scope: __module.model/__module.model.layers.8 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %1788 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.269, %1760, %1761)
  %1789 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1790 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1791 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1788)
  %mlp.19 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_120.MistralMLP = prim::GetAttr[name="mlp"](%_9)
  %post_attention_layernorm.19 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_122.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_9)
  %self_attn.19 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_115.MistralAttention = prim::GetAttr[name="self_attn"](%_9)
  %input_layernorm.19 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_121.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_9)
  %weight.165 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.19)
  %hidden_states.271 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%1789, %229, %239, %239, %241), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1798 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.271, %243), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1799 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm
  %variance.37 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1798, %1799, %220, %241), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1801 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.37, %219, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1802 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1801), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.273 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.271, %1802), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.275 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.273, %229, %239, %239, %241), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.277 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.165, %hidden_states.275), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1806 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.277, %hidden_states.271)
  %1807 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1808 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1806)
  %o_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_114.Linear = prim::GetAttr[name="o_proj"](%self_attn.19)
  %v_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_113.Linear = prim::GetAttr[name="v_proj"](%self_attn.19)
  %k_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_112.Linear = prim::GetAttr[name="k_proj"](%self_attn.19)
  %q_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_111.Linear = prim::GetAttr[name="q_proj"](%self_attn.19)
  %1813 : int = aten::size(%1807, %237), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %1814 : int = aten::size(%1807, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.167 : Tensor = prim::GetAttr[name="weight"](%q_proj.19)
  %1816 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1807, %weight.167), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn/__module.model.layers.9.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1817 : int[] = prim::ListConstruct(%1813, %1814, %231, %226), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1818 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%1816, %1817), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.19 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1818, %242, %243), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.169 : Tensor = prim::GetAttr[name="weight"](%k_proj.19)
  %1821 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1807, %weight.169), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn/__module.model.layers.9.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1822 : int[] = prim::ListConstruct(%1813, %1814, %231, %226), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1823 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1821, %1822), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.19 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1823, %242, %243), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.171 : Tensor = prim::GetAttr[name="weight"](%v_proj.19)
  %1826 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1807, %weight.171), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn/__module.model.layers.9.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1827 : int[] = prim::ListConstruct(%1813, %1814, %231, %226), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1828 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1826, %1827), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.19 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1828, %242, %243), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.23 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.23 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %1832 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.19, %cos.23), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1833 : int = aten::size(%q.19, %235), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1834 : Long(device=cpu) = prim::NumToTensor(%1833), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1835 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1834, %225), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1836 : int = aten::Int(%1835), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %x1.37 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.19, %235, %237, %1836, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1838 : int = aten::size(%q.19, %235), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1839 : Long(device=cpu) = prim::NumToTensor(%1838), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1840 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1839, %225), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1841 : int = aten::Int(%1840), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %x2.37 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.19, %235, %1841, %236, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1843 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.37), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1844 : Tensor[] = prim::ListConstruct(%1843, %x1.37), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1845 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1844, %231), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1846 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1845, %sin.23), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.19 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1832, %1846, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1848 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.19, %cos.23), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1849 : int = aten::size(%k.19, %235), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1850 : Long(device=cpu) = prim::NumToTensor(%1849), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1851 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1850, %225), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1852 : int = aten::Int(%1851), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %x1.39 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.19, %235, %237, %1852, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1854 : int = aten::size(%k.19, %235), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1855 : Long(device=cpu) = prim::NumToTensor(%1854), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1856 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1855, %225), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1857 : int = aten::Int(%1856), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %x2.39 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.19, %235, %1857, %236, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1859 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.39), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1860 : Tensor[] = prim::ListConstruct(%1859, %x1.39), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1861 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1860, %231), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1862 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1861, %sin.23), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.19 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1848, %1862, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1864 : Tensor[] = prim::ListConstruct(%57, %key_states.19), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %hidden_states.279 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1864, %224), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %1866 : Tensor[] = prim::ListConstruct(%58, %value_states.19), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %hidden_states.283 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1866, %224), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %1868 : int = aten::size(%hidden_states.279, %237), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1869 : int = aten::size(%hidden_states.279, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.37 : Long(device=cpu) = prim::NumToTensor(%1869), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1871 : int = aten::size(%hidden_states.279, %243), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1872 : int = aten::size(%hidden_states.279, %235), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1873 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.279, %237, %237, %236, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1874 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1873, %242, %237, %236, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1875 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1874, %243), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1876 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1875, %235, %237, %236, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1877 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1876, %234, %237, %236, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1878 : int[] = prim::ListConstruct(%1868, %1869, %234, %1871, %1872), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %hidden_states.281 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1877, %1878, %239), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1880 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.37, %223), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1881 : int = aten::Int(%1880), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1882 : int[] = prim::ListConstruct(%1868, %1881, %1871, %1872), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %key.19 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.281, %1882), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1884 : int = aten::size(%hidden_states.283, %237), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1885 : int = aten::size(%hidden_states.283, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.39 : Long(device=cpu) = prim::NumToTensor(%1885), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1887 : int = aten::size(%hidden_states.283, %243), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1888 : int = aten::size(%hidden_states.283, %235), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1889 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.283, %237, %237, %236, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1890 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1889, %242, %237, %236, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1891 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1890, %243), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1892 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1891, %235, %237, %236, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1893 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1892, %234, %237, %236, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1894 : int[] = prim::ListConstruct(%1884, %1885, %234, %1887, %1888), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %hidden_states.285 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1893, %1894, %239), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1896 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.39, %223), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1897 : int = aten::Int(%1896), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1898 : int[] = prim::ListConstruct(%1884, %1897, %1887, %1888), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %value.19 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.285, %1898), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1900 : int = aten::size(%key.19, %243), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1901 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1902 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1901, %242, %237, %236, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1903 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1902, %243, %237, %236, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.25 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1903, %235, %237, %1900, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.37 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.19, %key.19, %value.19, %attention_mask.25, %222, %239, %221, %239), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %1906 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.37, %242, %243), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.39 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1906, %237), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %1908 : int[] = prim::ListConstruct(%1813, %1814, %231), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1909 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.39, %1908), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %1910 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%1909, %237), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.173 : Tensor = prim::GetAttr[name="weight"](%o_proj.19)
  %hidden_states.287 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1910, %weight.173), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn/__module.model.layers.9.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1913 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.287, %hidden_states.279, %hidden_states.283)
  %1914 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1915 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1916 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1913)
  %hidden_states.289 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1808, %1914, %242), scope: __module.model/__module.model.layers.9 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.175 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.19)
  %hidden_states.291 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.289, %229, %239, %239, %241), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1920 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.291, %243), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1921 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm
  %variance.39 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1920, %1921, %220, %241), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1923 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.39, %219, %242), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1924 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1923), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.293 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.291, %1924), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.295 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.293, %229, %239, %239, %241), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1927 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.175, %hidden_states.295), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1928 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%1927, %hidden_states.291)
  %1929 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1930 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1928)
  %down_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_118.Linear = prim::GetAttr[name="down_proj"](%mlp.19)
  %up_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_117.Linear = prim::GetAttr[name="up_proj"](%mlp.19)
  %gate_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_116.Linear = prim::GetAttr[name="gate_proj"](%mlp.19)
  %weight.177 : Tensor = prim::GetAttr[name="weight"](%gate_proj.19)
  %input.19 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1929, %weight.177), scope: __module.model/__module.model.layers.9/__module.model.layers.9.mlp/__module.model.layers.9.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1936 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.19), scope: __module.model/__module.model.layers.9/__module.model.layers.9.mlp/__module.model.layers.9.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.179 : Tensor = prim::GetAttr[name="weight"](%up_proj.19)
  %1938 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1929, %weight.179), scope: __module.model/__module.model.layers.9/__module.model.layers.9.mlp/__module.model.layers.9.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1939 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%1936, %1938), scope: __module.model/__module.model.layers.9/__module.model.layers.9.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.181 : Tensor = prim::GetAttr[name="weight"](%down_proj.19)
  %hidden_states.297 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1939, %weight.181), scope: __module.model/__module.model.layers.9/__module.model.layers.9.mlp/__module.model.layers.9.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.299 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1930, %hidden_states.297, %242), scope: __module.model/__module.model.layers.9 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %1943 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.299, %1915, %1916)
  %1944 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1945 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1946 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1943)
  %mlp.21 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_133.MistralMLP = prim::GetAttr[name="mlp"](%_10)
  %post_attention_layernorm.21 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_135.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_10)
  %self_attn.21 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_128.MistralAttention = prim::GetAttr[name="self_attn"](%_10)
  %input_layernorm.21 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_134.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_10)
  %weight.183 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.21)
  %hidden_states.301 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%1944, %229, %239, %239, %241), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1953 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.301, %243), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1954 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm
  %variance.41 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1953, %1954, %220, %241), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1956 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.41, %219, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1957 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1956), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.303 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.301, %1957), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.305 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.303, %229, %239, %239, %241), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.307 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.183, %hidden_states.305), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1961 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.307, %hidden_states.301)
  %1962 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1963 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1961)
  %o_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_127.Linear = prim::GetAttr[name="o_proj"](%self_attn.21)
  %v_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_126.Linear = prim::GetAttr[name="v_proj"](%self_attn.21)
  %k_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_125.Linear = prim::GetAttr[name="k_proj"](%self_attn.21)
  %q_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_124.Linear = prim::GetAttr[name="q_proj"](%self_attn.21)
  %1968 : int = aten::size(%1962, %237), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %1969 : int = aten::size(%1962, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.185 : Tensor = prim::GetAttr[name="weight"](%q_proj.21)
  %1971 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1962, %weight.185), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn/__module.model.layers.10.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1972 : int[] = prim::ListConstruct(%1968, %1969, %231, %226), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1973 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%1971, %1972), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.21 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1973, %242, %243), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.187 : Tensor = prim::GetAttr[name="weight"](%k_proj.21)
  %1976 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1962, %weight.187), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn/__module.model.layers.10.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1977 : int[] = prim::ListConstruct(%1968, %1969, %231, %226), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1978 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1976, %1977), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.21 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1978, %242, %243), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.189 : Tensor = prim::GetAttr[name="weight"](%v_proj.21)
  %1981 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1962, %weight.189), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn/__module.model.layers.10.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1982 : int[] = prim::ListConstruct(%1968, %1969, %231, %226), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1983 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1981, %1982), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.21 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1983, %242, %243), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.25 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.25 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %1987 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.21, %cos.25), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1988 : int = aten::size(%q.21, %235), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1989 : Long(device=cpu) = prim::NumToTensor(%1988), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1990 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1989, %225), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1991 : int = aten::Int(%1990), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %x1.41 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.21, %235, %237, %1991, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1993 : int = aten::size(%q.21, %235), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1994 : Long(device=cpu) = prim::NumToTensor(%1993), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1995 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1994, %225), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1996 : int = aten::Int(%1995), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %x2.41 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.21, %235, %1996, %236, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1998 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.41), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1999 : Tensor[] = prim::ListConstruct(%1998, %x1.41), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %2000 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1999, %231), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2001 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2000, %sin.25), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.21 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1987, %2001, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2003 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.21, %cos.25), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2004 : int = aten::size(%k.21, %235), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2005 : Long(device=cpu) = prim::NumToTensor(%2004), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %2006 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2005, %225), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2007 : int = aten::Int(%2006), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %x1.43 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.21, %235, %237, %2007, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2009 : int = aten::size(%k.21, %235), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2010 : Long(device=cpu) = prim::NumToTensor(%2009), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %2011 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2010, %225), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2012 : int = aten::Int(%2011), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %x2.43 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.21, %235, %2012, %236, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2014 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.43), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2015 : Tensor[] = prim::ListConstruct(%2014, %x1.43), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %2016 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2015, %231), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2017 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2016, %sin.25), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.21 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2003, %2017, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2019 : Tensor[] = prim::ListConstruct(%59, %key_states.21), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %hidden_states.309 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2019, %224), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %2021 : Tensor[] = prim::ListConstruct(%60, %value_states.21), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %hidden_states.313 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2021, %224), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %2023 : int = aten::size(%hidden_states.309, %237), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2024 : int = aten::size(%hidden_states.309, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.41 : Long(device=cpu) = prim::NumToTensor(%2024), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %2026 : int = aten::size(%hidden_states.309, %243), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2027 : int = aten::size(%hidden_states.309, %235), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2028 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.309, %237, %237, %236, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2029 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2028, %242, %237, %236, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2030 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2029, %243), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2031 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2030, %235, %237, %236, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2032 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2031, %234, %237, %236, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2033 : int[] = prim::ListConstruct(%2023, %2024, %234, %2026, %2027), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %hidden_states.311 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2032, %2033, %239), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2035 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.41, %223), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2036 : int = aten::Int(%2035), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %2037 : int[] = prim::ListConstruct(%2023, %2036, %2026, %2027), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %key.21 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.311, %2037), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2039 : int = aten::size(%hidden_states.313, %237), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2040 : int = aten::size(%hidden_states.313, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.43 : Long(device=cpu) = prim::NumToTensor(%2040), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %2042 : int = aten::size(%hidden_states.313, %243), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2043 : int = aten::size(%hidden_states.313, %235), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2044 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.313, %237, %237, %236, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2045 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2044, %242, %237, %236, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2046 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2045, %243), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2047 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2046, %235, %237, %236, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2048 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2047, %234, %237, %236, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2049 : int[] = prim::ListConstruct(%2039, %2040, %234, %2042, %2043), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %hidden_states.315 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2048, %2049, %239), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2051 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.43, %223), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2052 : int = aten::Int(%2051), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %2053 : int[] = prim::ListConstruct(%2039, %2052, %2042, %2043), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %value.21 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.315, %2053), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2055 : int = aten::size(%key.21, %243), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2056 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2057 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2056, %242, %237, %236, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2058 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2057, %243, %237, %236, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.27 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2058, %235, %237, %2055, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.41 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.21, %key.21, %value.21, %attention_mask.27, %222, %239, %221, %239), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %2061 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.41, %242, %243), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.43 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2061, %237), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %2063 : int[] = prim::ListConstruct(%1968, %1969, %231), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %2064 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.43, %2063), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %2065 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%2064, %237), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.191 : Tensor = prim::GetAttr[name="weight"](%o_proj.21)
  %hidden_states.317 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2065, %weight.191), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn/__module.model.layers.10.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2068 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.317, %hidden_states.309, %hidden_states.313)
  %2069 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2070 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2071 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2068)
  %hidden_states.319 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1963, %2069, %242), scope: __module.model/__module.model.layers.10 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.193 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.21)
  %hidden_states.321 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.319, %229, %239, %239, %241), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2075 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.321, %243), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2076 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm
  %variance.43 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2075, %2076, %220, %241), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2078 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.43, %219, %242), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2079 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2078), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.323 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.321, %2079), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.325 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.323, %229, %239, %239, %241), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2082 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.193, %hidden_states.325), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2083 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%2082, %hidden_states.321)
  %2084 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2085 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2083)
  %down_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_131.Linear = prim::GetAttr[name="down_proj"](%mlp.21)
  %up_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_130.Linear = prim::GetAttr[name="up_proj"](%mlp.21)
  %gate_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_129.Linear = prim::GetAttr[name="gate_proj"](%mlp.21)
  %weight.195 : Tensor = prim::GetAttr[name="weight"](%gate_proj.21)
  %input.21 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2084, %weight.195), scope: __module.model/__module.model.layers.10/__module.model.layers.10.mlp/__module.model.layers.10.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2091 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.21), scope: __module.model/__module.model.layers.10/__module.model.layers.10.mlp/__module.model.layers.10.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.197 : Tensor = prim::GetAttr[name="weight"](%up_proj.21)
  %2093 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2084, %weight.197), scope: __module.model/__module.model.layers.10/__module.model.layers.10.mlp/__module.model.layers.10.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2094 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%2091, %2093), scope: __module.model/__module.model.layers.10/__module.model.layers.10.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.199 : Tensor = prim::GetAttr[name="weight"](%down_proj.21)
  %hidden_states.327 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2094, %weight.199), scope: __module.model/__module.model.layers.10/__module.model.layers.10.mlp/__module.model.layers.10.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.329 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2085, %hidden_states.327, %242), scope: __module.model/__module.model.layers.10 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %2098 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.329, %2070, %2071)
  %2099 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2100 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2101 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2098)
  %mlp.23 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_146.MistralMLP = prim::GetAttr[name="mlp"](%_11)
  %post_attention_layernorm.23 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_148.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_11)
  %self_attn.23 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_141.MistralAttention = prim::GetAttr[name="self_attn"](%_11)
  %input_layernorm.23 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_147.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_11)
  %weight.201 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.23)
  %hidden_states.331 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%2099, %229, %239, %239, %241), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2108 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.331, %243), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2109 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm
  %variance.45 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2108, %2109, %220, %241), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2111 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.45, %219, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2112 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2111), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.333 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.331, %2112), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.335 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.333, %229, %239, %239, %241), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.337 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.201, %hidden_states.335), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2116 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.337, %hidden_states.331)
  %2117 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2118 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2116)
  %o_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_140.Linear = prim::GetAttr[name="o_proj"](%self_attn.23)
  %v_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_139.Linear = prim::GetAttr[name="v_proj"](%self_attn.23)
  %k_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_138.Linear = prim::GetAttr[name="k_proj"](%self_attn.23)
  %q_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_137.Linear = prim::GetAttr[name="q_proj"](%self_attn.23)
  %2123 : int = aten::size(%2117, %237), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %2124 : int = aten::size(%2117, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.203 : Tensor = prim::GetAttr[name="weight"](%q_proj.23)
  %2126 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2117, %weight.203), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn/__module.model.layers.11.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2127 : int[] = prim::ListConstruct(%2123, %2124, %231, %226), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %2128 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%2126, %2127), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.23 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2128, %242, %243), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.205 : Tensor = prim::GetAttr[name="weight"](%k_proj.23)
  %2131 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2117, %weight.205), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn/__module.model.layers.11.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2132 : int[] = prim::ListConstruct(%2123, %2124, %231, %226), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %2133 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2131, %2132), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.23 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2133, %242, %243), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.207 : Tensor = prim::GetAttr[name="weight"](%v_proj.23)
  %2136 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2117, %weight.207), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn/__module.model.layers.11.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2137 : int[] = prim::ListConstruct(%2123, %2124, %231, %226), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %2138 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2136, %2137), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.23 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2138, %242, %243), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.27 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.27 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %2142 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.23, %cos.27), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2143 : int = aten::size(%q.23, %235), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2144 : Long(device=cpu) = prim::NumToTensor(%2143), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %2145 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2144, %225), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2146 : int = aten::Int(%2145), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %x1.45 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.23, %235, %237, %2146, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2148 : int = aten::size(%q.23, %235), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2149 : Long(device=cpu) = prim::NumToTensor(%2148), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %2150 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2149, %225), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2151 : int = aten::Int(%2150), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %x2.45 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.23, %235, %2151, %236, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2153 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.45), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2154 : Tensor[] = prim::ListConstruct(%2153, %x1.45), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %2155 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2154, %231), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2156 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2155, %sin.27), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.23 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2142, %2156, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2158 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.23, %cos.27), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2159 : int = aten::size(%k.23, %235), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2160 : Long(device=cpu) = prim::NumToTensor(%2159), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %2161 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2160, %225), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2162 : int = aten::Int(%2161), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %x1.47 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.23, %235, %237, %2162, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2164 : int = aten::size(%k.23, %235), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2165 : Long(device=cpu) = prim::NumToTensor(%2164), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %2166 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2165, %225), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2167 : int = aten::Int(%2166), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %x2.47 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.23, %235, %2167, %236, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2169 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2170 : Tensor[] = prim::ListConstruct(%2169, %x1.47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %2171 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2170, %231), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2172 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2171, %sin.27), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.23 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2158, %2172, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2174 : Tensor[] = prim::ListConstruct(%61, %key_states.23), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %hidden_states.339 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2174, %224), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %2176 : Tensor[] = prim::ListConstruct(%62, %value_states.23), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %hidden_states.343 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2176, %224), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %2178 : int = aten::size(%hidden_states.339, %237), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2179 : int = aten::size(%hidden_states.339, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.45 : Long(device=cpu) = prim::NumToTensor(%2179), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %2181 : int = aten::size(%hidden_states.339, %243), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2182 : int = aten::size(%hidden_states.339, %235), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2183 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.339, %237, %237, %236, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2184 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2183, %242, %237, %236, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2185 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2184, %243), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2186 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2185, %235, %237, %236, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2187 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2186, %234, %237, %236, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2188 : int[] = prim::ListConstruct(%2178, %2179, %234, %2181, %2182), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %hidden_states.341 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2187, %2188, %239), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2190 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.45, %223), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2191 : int = aten::Int(%2190), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %2192 : int[] = prim::ListConstruct(%2178, %2191, %2181, %2182), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %key.23 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.341, %2192), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2194 : int = aten::size(%hidden_states.343, %237), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2195 : int = aten::size(%hidden_states.343, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.47 : Long(device=cpu) = prim::NumToTensor(%2195), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %2197 : int = aten::size(%hidden_states.343, %243), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2198 : int = aten::size(%hidden_states.343, %235), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2199 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.343, %237, %237, %236, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2200 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2199, %242, %237, %236, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2201 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2200, %243), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2202 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2201, %235, %237, %236, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2203 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2202, %234, %237, %236, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2204 : int[] = prim::ListConstruct(%2194, %2195, %234, %2197, %2198), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %hidden_states.345 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2203, %2204, %239), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2206 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.47, %223), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2207 : int = aten::Int(%2206), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %2208 : int[] = prim::ListConstruct(%2194, %2207, %2197, %2198), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %value.23 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.345, %2208), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2210 : int = aten::size(%key.23, %243), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2211 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2212 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2211, %242, %237, %236, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2213 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2212, %243, %237, %236, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.29 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2213, %235, %237, %2210, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.45 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.23, %key.23, %value.23, %attention_mask.29, %222, %239, %221, %239), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %2216 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.45, %242, %243), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.47 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2216, %237), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %2218 : int[] = prim::ListConstruct(%2123, %2124, %231), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %2219 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.47, %2218), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %2220 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%2219, %237), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.209 : Tensor = prim::GetAttr[name="weight"](%o_proj.23)
  %hidden_states.347 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2220, %weight.209), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn/__module.model.layers.11.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2223 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.347, %hidden_states.339, %hidden_states.343)
  %2224 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2225 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2226 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2223)
  %hidden_states.349 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2118, %2224, %242), scope: __module.model/__module.model.layers.11 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.211 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.23)
  %hidden_states.351 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.349, %229, %239, %239, %241), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2230 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.351, %243), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2231 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm
  %variance.47 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2230, %2231, %220, %241), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2233 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.47, %219, %242), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2234 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2233), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.353 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.351, %2234), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.355 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.353, %229, %239, %239, %241), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2237 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.211, %hidden_states.355), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2238 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%2237, %hidden_states.351)
  %2239 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2240 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2238)
  %down_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_144.Linear = prim::GetAttr[name="down_proj"](%mlp.23)
  %up_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_143.Linear = prim::GetAttr[name="up_proj"](%mlp.23)
  %gate_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_142.Linear = prim::GetAttr[name="gate_proj"](%mlp.23)
  %weight.213 : Tensor = prim::GetAttr[name="weight"](%gate_proj.23)
  %input.23 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2239, %weight.213), scope: __module.model/__module.model.layers.11/__module.model.layers.11.mlp/__module.model.layers.11.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2246 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.23), scope: __module.model/__module.model.layers.11/__module.model.layers.11.mlp/__module.model.layers.11.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.215 : Tensor = prim::GetAttr[name="weight"](%up_proj.23)
  %2248 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2239, %weight.215), scope: __module.model/__module.model.layers.11/__module.model.layers.11.mlp/__module.model.layers.11.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2249 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%2246, %2248), scope: __module.model/__module.model.layers.11/__module.model.layers.11.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.217 : Tensor = prim::GetAttr[name="weight"](%down_proj.23)
  %hidden_states.357 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2249, %weight.217), scope: __module.model/__module.model.layers.11/__module.model.layers.11.mlp/__module.model.layers.11.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.359 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2240, %hidden_states.357, %242), scope: __module.model/__module.model.layers.11 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %2253 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.359, %2225, %2226)
  %2254 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2255 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2256 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2253)
  %mlp.25 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_159.MistralMLP = prim::GetAttr[name="mlp"](%_12)
  %post_attention_layernorm.25 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_161.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_12)
  %self_attn.25 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_154.MistralAttention = prim::GetAttr[name="self_attn"](%_12)
  %input_layernorm.25 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_160.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_12)
  %weight.219 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.25)
  %hidden_states.361 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%2254, %229, %239, %239, %241), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2263 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.361, %243), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2264 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm
  %variance.49 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2263, %2264, %220, %241), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2266 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.49, %219, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2267 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2266), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.363 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.361, %2267), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.365 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.363, %229, %239, %239, %241), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.367 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.219, %hidden_states.365), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2271 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.367, %hidden_states.361)
  %2272 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2273 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2271)
  %o_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_153.Linear = prim::GetAttr[name="o_proj"](%self_attn.25)
  %v_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_152.Linear = prim::GetAttr[name="v_proj"](%self_attn.25)
  %k_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_151.Linear = prim::GetAttr[name="k_proj"](%self_attn.25)
  %q_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_150.Linear = prim::GetAttr[name="q_proj"](%self_attn.25)
  %2278 : int = aten::size(%2272, %237), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %2279 : int = aten::size(%2272, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.221 : Tensor = prim::GetAttr[name="weight"](%q_proj.25)
  %2281 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2272, %weight.221), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn/__module.model.layers.12.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2282 : int[] = prim::ListConstruct(%2278, %2279, %231, %226), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2283 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%2281, %2282), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.25 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2283, %242, %243), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.223 : Tensor = prim::GetAttr[name="weight"](%k_proj.25)
  %2286 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2272, %weight.223), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn/__module.model.layers.12.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2287 : int[] = prim::ListConstruct(%2278, %2279, %231, %226), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2288 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2286, %2287), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.25 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2288, %242, %243), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.225 : Tensor = prim::GetAttr[name="weight"](%v_proj.25)
  %2291 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2272, %weight.225), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn/__module.model.layers.12.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2292 : int[] = prim::ListConstruct(%2278, %2279, %231, %226), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2293 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2291, %2292), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.25 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2293, %242, %243), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.29 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.29 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %2297 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.25, %cos.29), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2298 : int = aten::size(%q.25, %235), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2299 : Long(device=cpu) = prim::NumToTensor(%2298), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2300 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2299, %225), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2301 : int = aten::Int(%2300), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %x1.49 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.25, %235, %237, %2301, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2303 : int = aten::size(%q.25, %235), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2304 : Long(device=cpu) = prim::NumToTensor(%2303), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2305 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2304, %225), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2306 : int = aten::Int(%2305), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %x2.49 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.25, %235, %2306, %236, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2308 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.49), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2309 : Tensor[] = prim::ListConstruct(%2308, %x1.49), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2310 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2309, %231), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2311 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2310, %sin.29), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.25 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2297, %2311, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2313 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.25, %cos.29), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2314 : int = aten::size(%k.25, %235), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2315 : Long(device=cpu) = prim::NumToTensor(%2314), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2316 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2315, %225), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2317 : int = aten::Int(%2316), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %x1.51 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.25, %235, %237, %2317, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2319 : int = aten::size(%k.25, %235), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2320 : Long(device=cpu) = prim::NumToTensor(%2319), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2321 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2320, %225), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2322 : int = aten::Int(%2321), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %x2.51 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.25, %235, %2322, %236, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2324 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.51), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2325 : Tensor[] = prim::ListConstruct(%2324, %x1.51), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2326 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2325, %231), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2327 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2326, %sin.29), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.25 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2313, %2327, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2329 : Tensor[] = prim::ListConstruct(%63, %key_states.25), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %hidden_states.369 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2329, %224), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %2331 : Tensor[] = prim::ListConstruct(%64, %value_states.25), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %hidden_states.373 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2331, %224), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %2333 : int = aten::size(%hidden_states.369, %237), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2334 : int = aten::size(%hidden_states.369, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.49 : Long(device=cpu) = prim::NumToTensor(%2334), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2336 : int = aten::size(%hidden_states.369, %243), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2337 : int = aten::size(%hidden_states.369, %235), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2338 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.369, %237, %237, %236, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2339 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2338, %242, %237, %236, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2340 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2339, %243), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2341 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2340, %235, %237, %236, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2342 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2341, %234, %237, %236, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2343 : int[] = prim::ListConstruct(%2333, %2334, %234, %2336, %2337), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %hidden_states.371 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2342, %2343, %239), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2345 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.49, %223), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2346 : int = aten::Int(%2345), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2347 : int[] = prim::ListConstruct(%2333, %2346, %2336, %2337), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %key.25 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.371, %2347), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2349 : int = aten::size(%hidden_states.373, %237), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2350 : int = aten::size(%hidden_states.373, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.51 : Long(device=cpu) = prim::NumToTensor(%2350), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2352 : int = aten::size(%hidden_states.373, %243), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2353 : int = aten::size(%hidden_states.373, %235), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2354 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.373, %237, %237, %236, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2355 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2354, %242, %237, %236, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2356 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2355, %243), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2357 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2356, %235, %237, %236, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2358 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2357, %234, %237, %236, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2359 : int[] = prim::ListConstruct(%2349, %2350, %234, %2352, %2353), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %hidden_states.375 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2358, %2359, %239), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2361 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.51, %223), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2362 : int = aten::Int(%2361), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2363 : int[] = prim::ListConstruct(%2349, %2362, %2352, %2353), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %value.25 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.375, %2363), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2365 : int = aten::size(%key.25, %243), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2366 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2367 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2366, %242, %237, %236, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2368 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2367, %243, %237, %236, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.31 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2368, %235, %237, %2365, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.49 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.25, %key.25, %value.25, %attention_mask.31, %222, %239, %221, %239), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %2371 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.49, %242, %243), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.51 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2371, %237), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %2373 : int[] = prim::ListConstruct(%2278, %2279, %231), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2374 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.51, %2373), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %2375 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%2374, %237), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.227 : Tensor = prim::GetAttr[name="weight"](%o_proj.25)
  %hidden_states.377 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2375, %weight.227), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn/__module.model.layers.12.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2378 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.377, %hidden_states.369, %hidden_states.373)
  %2379 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2380 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2381 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2378)
  %hidden_states.379 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2273, %2379, %242), scope: __module.model/__module.model.layers.12 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.229 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.25)
  %hidden_states.381 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.379, %229, %239, %239, %241), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2385 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.381, %243), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2386 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm
  %variance.51 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2385, %2386, %220, %241), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2388 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.51, %219, %242), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2389 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2388), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.383 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.381, %2389), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.385 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.383, %229, %239, %239, %241), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2392 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.229, %hidden_states.385), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2393 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%2392, %hidden_states.381)
  %2394 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2395 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2393)
  %down_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_157.Linear = prim::GetAttr[name="down_proj"](%mlp.25)
  %up_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_156.Linear = prim::GetAttr[name="up_proj"](%mlp.25)
  %gate_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_155.Linear = prim::GetAttr[name="gate_proj"](%mlp.25)
  %weight.231 : Tensor = prim::GetAttr[name="weight"](%gate_proj.25)
  %input.25 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2394, %weight.231), scope: __module.model/__module.model.layers.12/__module.model.layers.12.mlp/__module.model.layers.12.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2401 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.25), scope: __module.model/__module.model.layers.12/__module.model.layers.12.mlp/__module.model.layers.12.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.233 : Tensor = prim::GetAttr[name="weight"](%up_proj.25)
  %2403 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2394, %weight.233), scope: __module.model/__module.model.layers.12/__module.model.layers.12.mlp/__module.model.layers.12.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2404 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%2401, %2403), scope: __module.model/__module.model.layers.12/__module.model.layers.12.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.235 : Tensor = prim::GetAttr[name="weight"](%down_proj.25)
  %hidden_states.387 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2404, %weight.235), scope: __module.model/__module.model.layers.12/__module.model.layers.12.mlp/__module.model.layers.12.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.389 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2395, %hidden_states.387, %242), scope: __module.model/__module.model.layers.12 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %2408 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.389, %2380, %2381)
  %2409 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2410 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2411 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2408)
  %mlp.27 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_172.MistralMLP = prim::GetAttr[name="mlp"](%_13)
  %post_attention_layernorm.27 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_174.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_13)
  %self_attn.27 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_167.MistralAttention = prim::GetAttr[name="self_attn"](%_13)
  %input_layernorm.27 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_173.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_13)
  %weight.237 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.27)
  %hidden_states.391 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%2409, %229, %239, %239, %241), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2418 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.391, %243), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2419 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm
  %variance.53 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2418, %2419, %220, %241), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2421 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.53, %219, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2422 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2421), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.393 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.391, %2422), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.395 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.393, %229, %239, %239, %241), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.397 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.237, %hidden_states.395), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2426 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.397, %hidden_states.391)
  %2427 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2428 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2426)
  %o_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_166.Linear = prim::GetAttr[name="o_proj"](%self_attn.27)
  %v_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_165.Linear = prim::GetAttr[name="v_proj"](%self_attn.27)
  %k_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_164.Linear = prim::GetAttr[name="k_proj"](%self_attn.27)
  %q_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_163.Linear = prim::GetAttr[name="q_proj"](%self_attn.27)
  %2433 : int = aten::size(%2427, %237), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %2434 : int = aten::size(%2427, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.239 : Tensor = prim::GetAttr[name="weight"](%q_proj.27)
  %2436 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2427, %weight.239), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn/__module.model.layers.13.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2437 : int[] = prim::ListConstruct(%2433, %2434, %231, %226), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2438 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%2436, %2437), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.27 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2438, %242, %243), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.241 : Tensor = prim::GetAttr[name="weight"](%k_proj.27)
  %2441 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2427, %weight.241), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn/__module.model.layers.13.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2442 : int[] = prim::ListConstruct(%2433, %2434, %231, %226), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2443 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2441, %2442), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.27 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2443, %242, %243), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.243 : Tensor = prim::GetAttr[name="weight"](%v_proj.27)
  %2446 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2427, %weight.243), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn/__module.model.layers.13.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2447 : int[] = prim::ListConstruct(%2433, %2434, %231, %226), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2448 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2446, %2447), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.27 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2448, %242, %243), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.31 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.31 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %2452 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.27, %cos.31), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2453 : int = aten::size(%q.27, %235), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2454 : Long(device=cpu) = prim::NumToTensor(%2453), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2455 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2454, %225), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2456 : int = aten::Int(%2455), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %x1.53 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.27, %235, %237, %2456, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2458 : int = aten::size(%q.27, %235), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2459 : Long(device=cpu) = prim::NumToTensor(%2458), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2460 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2459, %225), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2461 : int = aten::Int(%2460), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %x2.53 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.27, %235, %2461, %236, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2463 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.53), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2464 : Tensor[] = prim::ListConstruct(%2463, %x1.53), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2465 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2464, %231), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2466 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2465, %sin.31), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.27 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2452, %2466, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2468 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.27, %cos.31), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2469 : int = aten::size(%k.27, %235), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2470 : Long(device=cpu) = prim::NumToTensor(%2469), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2471 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2470, %225), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2472 : int = aten::Int(%2471), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %x1.55 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.27, %235, %237, %2472, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2474 : int = aten::size(%k.27, %235), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2475 : Long(device=cpu) = prim::NumToTensor(%2474), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2476 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2475, %225), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2477 : int = aten::Int(%2476), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %x2.55 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.27, %235, %2477, %236, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2479 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.55), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2480 : Tensor[] = prim::ListConstruct(%2479, %x1.55), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2481 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2480, %231), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2482 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2481, %sin.31), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.27 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2468, %2482, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2484 : Tensor[] = prim::ListConstruct(%65, %key_states.27), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %hidden_states.399 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2484, %224), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %2486 : Tensor[] = prim::ListConstruct(%66, %value_states.27), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %hidden_states.403 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2486, %224), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %2488 : int = aten::size(%hidden_states.399, %237), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2489 : int = aten::size(%hidden_states.399, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.53 : Long(device=cpu) = prim::NumToTensor(%2489), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2491 : int = aten::size(%hidden_states.399, %243), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2492 : int = aten::size(%hidden_states.399, %235), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2493 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.399, %237, %237, %236, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2494 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2493, %242, %237, %236, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2495 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2494, %243), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2496 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2495, %235, %237, %236, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2497 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2496, %234, %237, %236, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2498 : int[] = prim::ListConstruct(%2488, %2489, %234, %2491, %2492), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %hidden_states.401 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2497, %2498, %239), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2500 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.53, %223), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2501 : int = aten::Int(%2500), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2502 : int[] = prim::ListConstruct(%2488, %2501, %2491, %2492), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %key.27 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.401, %2502), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2504 : int = aten::size(%hidden_states.403, %237), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2505 : int = aten::size(%hidden_states.403, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.55 : Long(device=cpu) = prim::NumToTensor(%2505), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2507 : int = aten::size(%hidden_states.403, %243), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2508 : int = aten::size(%hidden_states.403, %235), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2509 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.403, %237, %237, %236, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2510 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2509, %242, %237, %236, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2511 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2510, %243), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2512 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2511, %235, %237, %236, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2513 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2512, %234, %237, %236, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2514 : int[] = prim::ListConstruct(%2504, %2505, %234, %2507, %2508), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %hidden_states.405 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2513, %2514, %239), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2516 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.55, %223), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2517 : int = aten::Int(%2516), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2518 : int[] = prim::ListConstruct(%2504, %2517, %2507, %2508), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %value.27 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.405, %2518), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2520 : int = aten::size(%key.27, %243), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2521 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2522 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2521, %242, %237, %236, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2523 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2522, %243, %237, %236, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.33 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2523, %235, %237, %2520, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.53 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.27, %key.27, %value.27, %attention_mask.33, %222, %239, %221, %239), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %2526 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.53, %242, %243), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.55 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2526, %237), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %2528 : int[] = prim::ListConstruct(%2433, %2434, %231), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2529 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.55, %2528), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %2530 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%2529, %237), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.245 : Tensor = prim::GetAttr[name="weight"](%o_proj.27)
  %hidden_states.407 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2530, %weight.245), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn/__module.model.layers.13.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2533 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.407, %hidden_states.399, %hidden_states.403)
  %2534 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2535 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2536 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2533)
  %hidden_states.409 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2428, %2534, %242), scope: __module.model/__module.model.layers.13 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.247 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.27)
  %hidden_states.411 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.409, %229, %239, %239, %241), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2540 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.411, %243), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2541 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm
  %variance.55 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2540, %2541, %220, %241), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2543 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.55, %219, %242), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2544 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2543), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.413 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.411, %2544), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.415 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.413, %229, %239, %239, %241), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2547 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.247, %hidden_states.415), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2548 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%2547, %hidden_states.411)
  %2549 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2550 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2548)
  %down_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_170.Linear = prim::GetAttr[name="down_proj"](%mlp.27)
  %up_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_169.Linear = prim::GetAttr[name="up_proj"](%mlp.27)
  %gate_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_168.Linear = prim::GetAttr[name="gate_proj"](%mlp.27)
  %weight.249 : Tensor = prim::GetAttr[name="weight"](%gate_proj.27)
  %input.27 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2549, %weight.249), scope: __module.model/__module.model.layers.13/__module.model.layers.13.mlp/__module.model.layers.13.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2556 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.27), scope: __module.model/__module.model.layers.13/__module.model.layers.13.mlp/__module.model.layers.13.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.251 : Tensor = prim::GetAttr[name="weight"](%up_proj.27)
  %2558 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2549, %weight.251), scope: __module.model/__module.model.layers.13/__module.model.layers.13.mlp/__module.model.layers.13.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2559 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%2556, %2558), scope: __module.model/__module.model.layers.13/__module.model.layers.13.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.253 : Tensor = prim::GetAttr[name="weight"](%down_proj.27)
  %hidden_states.417 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2559, %weight.253), scope: __module.model/__module.model.layers.13/__module.model.layers.13.mlp/__module.model.layers.13.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.419 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2550, %hidden_states.417, %242), scope: __module.model/__module.model.layers.13 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %2563 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.419, %2535, %2536)
  %2564 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2565 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2566 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2563)
  %mlp.29 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_185.MistralMLP = prim::GetAttr[name="mlp"](%_14)
  %post_attention_layernorm.29 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_187.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_14)
  %self_attn.29 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_180.MistralAttention = prim::GetAttr[name="self_attn"](%_14)
  %input_layernorm.29 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_186.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_14)
  %weight.255 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.29)
  %hidden_states.421 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%2564, %229, %239, %239, %241), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2573 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.421, %243), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2574 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm
  %variance.57 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2573, %2574, %220, %241), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2576 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.57, %219, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2577 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2576), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.423 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.421, %2577), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.425 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.423, %229, %239, %239, %241), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.427 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.255, %hidden_states.425), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2581 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.427, %hidden_states.421)
  %2582 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2583 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2581)
  %o_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_179.Linear = prim::GetAttr[name="o_proj"](%self_attn.29)
  %v_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_178.Linear = prim::GetAttr[name="v_proj"](%self_attn.29)
  %k_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_177.Linear = prim::GetAttr[name="k_proj"](%self_attn.29)
  %q_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_176.Linear = prim::GetAttr[name="q_proj"](%self_attn.29)
  %2588 : int = aten::size(%2582, %237), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %2589 : int = aten::size(%2582, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.257 : Tensor = prim::GetAttr[name="weight"](%q_proj.29)
  %2591 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2582, %weight.257), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn/__module.model.layers.14.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2592 : int[] = prim::ListConstruct(%2588, %2589, %231, %226), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2593 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%2591, %2592), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.29 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2593, %242, %243), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.259 : Tensor = prim::GetAttr[name="weight"](%k_proj.29)
  %2596 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2582, %weight.259), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn/__module.model.layers.14.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2597 : int[] = prim::ListConstruct(%2588, %2589, %231, %226), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2598 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2596, %2597), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.29 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2598, %242, %243), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.261 : Tensor = prim::GetAttr[name="weight"](%v_proj.29)
  %2601 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2582, %weight.261), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn/__module.model.layers.14.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2602 : int[] = prim::ListConstruct(%2588, %2589, %231, %226), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2603 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2601, %2602), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.29 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2603, %242, %243), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.33 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.33 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %2607 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.29, %cos.33), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2608 : int = aten::size(%q.29, %235), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2609 : Long(device=cpu) = prim::NumToTensor(%2608), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2610 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2609, %225), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2611 : int = aten::Int(%2610), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %x1.57 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.29, %235, %237, %2611, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2613 : int = aten::size(%q.29, %235), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2614 : Long(device=cpu) = prim::NumToTensor(%2613), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2615 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2614, %225), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2616 : int = aten::Int(%2615), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %x2.57 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.29, %235, %2616, %236, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2618 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.57), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2619 : Tensor[] = prim::ListConstruct(%2618, %x1.57), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2620 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2619, %231), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2621 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2620, %sin.33), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.29 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2607, %2621, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2623 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.29, %cos.33), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2624 : int = aten::size(%k.29, %235), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2625 : Long(device=cpu) = prim::NumToTensor(%2624), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2626 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2625, %225), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2627 : int = aten::Int(%2626), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %x1.59 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.29, %235, %237, %2627, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2629 : int = aten::size(%k.29, %235), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2630 : Long(device=cpu) = prim::NumToTensor(%2629), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2631 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2630, %225), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2632 : int = aten::Int(%2631), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %x2.59 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.29, %235, %2632, %236, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2634 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.59), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2635 : Tensor[] = prim::ListConstruct(%2634, %x1.59), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2636 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2635, %231), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2637 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2636, %sin.33), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.29 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2623, %2637, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2639 : Tensor[] = prim::ListConstruct(%67, %key_states.29), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %hidden_states.429 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2639, %224), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %2641 : Tensor[] = prim::ListConstruct(%68, %value_states.29), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %hidden_states.433 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2641, %224), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %2643 : int = aten::size(%hidden_states.429, %237), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2644 : int = aten::size(%hidden_states.429, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.57 : Long(device=cpu) = prim::NumToTensor(%2644), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2646 : int = aten::size(%hidden_states.429, %243), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2647 : int = aten::size(%hidden_states.429, %235), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2648 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.429, %237, %237, %236, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2649 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2648, %242, %237, %236, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2650 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2649, %243), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2651 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2650, %235, %237, %236, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2652 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2651, %234, %237, %236, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2653 : int[] = prim::ListConstruct(%2643, %2644, %234, %2646, %2647), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %hidden_states.431 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2652, %2653, %239), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2655 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.57, %223), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2656 : int = aten::Int(%2655), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2657 : int[] = prim::ListConstruct(%2643, %2656, %2646, %2647), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %key.29 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.431, %2657), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2659 : int = aten::size(%hidden_states.433, %237), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2660 : int = aten::size(%hidden_states.433, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.59 : Long(device=cpu) = prim::NumToTensor(%2660), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2662 : int = aten::size(%hidden_states.433, %243), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2663 : int = aten::size(%hidden_states.433, %235), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2664 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.433, %237, %237, %236, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2665 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2664, %242, %237, %236, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2666 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2665, %243), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2667 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2666, %235, %237, %236, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2668 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2667, %234, %237, %236, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2669 : int[] = prim::ListConstruct(%2659, %2660, %234, %2662, %2663), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %hidden_states.435 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2668, %2669, %239), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2671 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.59, %223), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2672 : int = aten::Int(%2671), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2673 : int[] = prim::ListConstruct(%2659, %2672, %2662, %2663), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %value.29 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.435, %2673), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2675 : int = aten::size(%key.29, %243), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2676 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2677 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2676, %242, %237, %236, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2678 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2677, %243, %237, %236, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.35 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2678, %235, %237, %2675, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.57 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.29, %key.29, %value.29, %attention_mask.35, %222, %239, %221, %239), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %2681 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.57, %242, %243), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.59 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2681, %237), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %2683 : int[] = prim::ListConstruct(%2588, %2589, %231), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2684 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.59, %2683), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %2685 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%2684, %237), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.263 : Tensor = prim::GetAttr[name="weight"](%o_proj.29)
  %hidden_states.437 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2685, %weight.263), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn/__module.model.layers.14.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2688 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.437, %hidden_states.429, %hidden_states.433)
  %2689 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2690 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2691 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2688)
  %hidden_states.439 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2583, %2689, %242), scope: __module.model/__module.model.layers.14 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.265 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.29)
  %hidden_states.441 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.439, %229, %239, %239, %241), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2695 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.441, %243), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2696 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm
  %variance.59 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2695, %2696, %220, %241), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2698 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.59, %219, %242), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2699 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2698), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.443 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.441, %2699), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.445 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.443, %229, %239, %239, %241), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2702 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.265, %hidden_states.445), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2703 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%2702, %hidden_states.441)
  %2704 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2705 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2703)
  %down_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_183.Linear = prim::GetAttr[name="down_proj"](%mlp.29)
  %up_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_182.Linear = prim::GetAttr[name="up_proj"](%mlp.29)
  %gate_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_181.Linear = prim::GetAttr[name="gate_proj"](%mlp.29)
  %weight.267 : Tensor = prim::GetAttr[name="weight"](%gate_proj.29)
  %input.29 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2704, %weight.267), scope: __module.model/__module.model.layers.14/__module.model.layers.14.mlp/__module.model.layers.14.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2711 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.29), scope: __module.model/__module.model.layers.14/__module.model.layers.14.mlp/__module.model.layers.14.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.269 : Tensor = prim::GetAttr[name="weight"](%up_proj.29)
  %2713 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2704, %weight.269), scope: __module.model/__module.model.layers.14/__module.model.layers.14.mlp/__module.model.layers.14.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2714 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%2711, %2713), scope: __module.model/__module.model.layers.14/__module.model.layers.14.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.271 : Tensor = prim::GetAttr[name="weight"](%down_proj.29)
  %hidden_states.447 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2714, %weight.271), scope: __module.model/__module.model.layers.14/__module.model.layers.14.mlp/__module.model.layers.14.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.449 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2705, %hidden_states.447, %242), scope: __module.model/__module.model.layers.14 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %2718 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.449, %2690, %2691)
  %2719 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2720 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2721 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2718)
  %mlp.31 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_198.MistralMLP = prim::GetAttr[name="mlp"](%_15)
  %post_attention_layernorm.31 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_200.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_15)
  %self_attn.31 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_193.MistralAttention = prim::GetAttr[name="self_attn"](%_15)
  %input_layernorm.31 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_199.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_15)
  %weight.273 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.31)
  %hidden_states.451 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%2719, %229, %239, %239, %241), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2728 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.451, %243), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2729 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm
  %variance.61 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2728, %2729, %220, %241), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2731 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.61, %219, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2732 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2731), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.453 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.451, %2732), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.455 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.453, %229, %239, %239, %241), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.457 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.273, %hidden_states.455), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2736 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.457, %hidden_states.451)
  %2737 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2738 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2736)
  %o_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_192.Linear = prim::GetAttr[name="o_proj"](%self_attn.31)
  %v_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_191.Linear = prim::GetAttr[name="v_proj"](%self_attn.31)
  %k_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_190.Linear = prim::GetAttr[name="k_proj"](%self_attn.31)
  %q_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_189.Linear = prim::GetAttr[name="q_proj"](%self_attn.31)
  %2743 : int = aten::size(%2737, %237), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %2744 : int = aten::size(%2737, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.275 : Tensor = prim::GetAttr[name="weight"](%q_proj.31)
  %2746 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2737, %weight.275), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn/__module.model.layers.15.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2747 : int[] = prim::ListConstruct(%2743, %2744, %231, %226), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2748 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%2746, %2747), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.31 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2748, %242, %243), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.277 : Tensor = prim::GetAttr[name="weight"](%k_proj.31)
  %2751 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2737, %weight.277), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn/__module.model.layers.15.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2752 : int[] = prim::ListConstruct(%2743, %2744, %231, %226), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2753 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2751, %2752), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.31 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2753, %242, %243), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.279 : Tensor = prim::GetAttr[name="weight"](%v_proj.31)
  %2756 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2737, %weight.279), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn/__module.model.layers.15.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2757 : int[] = prim::ListConstruct(%2743, %2744, %231, %226), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2758 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2756, %2757), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.31 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2758, %242, %243), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.35 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.35 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %2762 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.31, %cos.35), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2763 : int = aten::size(%q.31, %235), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2764 : Long(device=cpu) = prim::NumToTensor(%2763), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2765 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2764, %225), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2766 : int = aten::Int(%2765), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %x1.61 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.31, %235, %237, %2766, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2768 : int = aten::size(%q.31, %235), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2769 : Long(device=cpu) = prim::NumToTensor(%2768), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2770 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2769, %225), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2771 : int = aten::Int(%2770), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %x2.61 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.31, %235, %2771, %236, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2773 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.61), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2774 : Tensor[] = prim::ListConstruct(%2773, %x1.61), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2775 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2774, %231), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2776 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2775, %sin.35), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.31 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2762, %2776, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2778 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.31, %cos.35), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2779 : int = aten::size(%k.31, %235), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2780 : Long(device=cpu) = prim::NumToTensor(%2779), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2781 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2780, %225), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2782 : int = aten::Int(%2781), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %x1.63 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.31, %235, %237, %2782, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2784 : int = aten::size(%k.31, %235), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2785 : Long(device=cpu) = prim::NumToTensor(%2784), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2786 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2785, %225), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2787 : int = aten::Int(%2786), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %x2.63 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.31, %235, %2787, %236, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2789 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.63), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2790 : Tensor[] = prim::ListConstruct(%2789, %x1.63), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2791 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2790, %231), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2792 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2791, %sin.35), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.31 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2778, %2792, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2794 : Tensor[] = prim::ListConstruct(%69, %key_states.31), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %hidden_states.459 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2794, %224), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %2796 : Tensor[] = prim::ListConstruct(%70, %value_states.31), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %hidden_states.463 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2796, %224), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %2798 : int = aten::size(%hidden_states.459, %237), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2799 : int = aten::size(%hidden_states.459, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.61 : Long(device=cpu) = prim::NumToTensor(%2799), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2801 : int = aten::size(%hidden_states.459, %243), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2802 : int = aten::size(%hidden_states.459, %235), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2803 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.459, %237, %237, %236, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2804 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2803, %242, %237, %236, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2805 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2804, %243), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2806 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2805, %235, %237, %236, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2807 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2806, %234, %237, %236, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2808 : int[] = prim::ListConstruct(%2798, %2799, %234, %2801, %2802), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %hidden_states.461 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2807, %2808, %239), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2810 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.61, %223), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2811 : int = aten::Int(%2810), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2812 : int[] = prim::ListConstruct(%2798, %2811, %2801, %2802), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %key.31 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.461, %2812), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2814 : int = aten::size(%hidden_states.463, %237), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2815 : int = aten::size(%hidden_states.463, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.63 : Long(device=cpu) = prim::NumToTensor(%2815), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2817 : int = aten::size(%hidden_states.463, %243), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2818 : int = aten::size(%hidden_states.463, %235), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2819 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.463, %237, %237, %236, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2820 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2819, %242, %237, %236, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2821 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2820, %243), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2822 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2821, %235, %237, %236, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2823 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2822, %234, %237, %236, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2824 : int[] = prim::ListConstruct(%2814, %2815, %234, %2817, %2818), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %hidden_states.465 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2823, %2824, %239), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2826 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.63, %223), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2827 : int = aten::Int(%2826), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2828 : int[] = prim::ListConstruct(%2814, %2827, %2817, %2818), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %value.31 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.465, %2828), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2830 : int = aten::size(%key.31, %243), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2831 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2832 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2831, %242, %237, %236, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2833 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2832, %243, %237, %236, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.37 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2833, %235, %237, %2830, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.61 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.31, %key.31, %value.31, %attention_mask.37, %222, %239, %221, %239), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %2836 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.61, %242, %243), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.63 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2836, %237), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %2838 : int[] = prim::ListConstruct(%2743, %2744, %231), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2839 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.63, %2838), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %2840 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%2839, %237), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.281 : Tensor = prim::GetAttr[name="weight"](%o_proj.31)
  %hidden_states.467 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2840, %weight.281), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn/__module.model.layers.15.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2843 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.467, %hidden_states.459, %hidden_states.463)
  %2844 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2845 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2846 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2843)
  %hidden_states.469 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2738, %2844, %242), scope: __module.model/__module.model.layers.15 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.283 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.31)
  %hidden_states.471 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.469, %229, %239, %239, %241), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2850 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.471, %243), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2851 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm
  %variance.63 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2850, %2851, %220, %241), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2853 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.63, %219, %242), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2854 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2853), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.473 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.471, %2854), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.475 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.473, %229, %239, %239, %241), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2857 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.283, %hidden_states.475), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2858 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%2857, %hidden_states.471)
  %2859 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2860 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2858)
  %down_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_196.Linear = prim::GetAttr[name="down_proj"](%mlp.31)
  %up_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_195.Linear = prim::GetAttr[name="up_proj"](%mlp.31)
  %gate_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_194.Linear = prim::GetAttr[name="gate_proj"](%mlp.31)
  %weight.285 : Tensor = prim::GetAttr[name="weight"](%gate_proj.31)
  %input.31 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2859, %weight.285), scope: __module.model/__module.model.layers.15/__module.model.layers.15.mlp/__module.model.layers.15.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2866 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.31), scope: __module.model/__module.model.layers.15/__module.model.layers.15.mlp/__module.model.layers.15.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.287 : Tensor = prim::GetAttr[name="weight"](%up_proj.31)
  %2868 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2859, %weight.287), scope: __module.model/__module.model.layers.15/__module.model.layers.15.mlp/__module.model.layers.15.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2869 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%2866, %2868), scope: __module.model/__module.model.layers.15/__module.model.layers.15.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.289 : Tensor = prim::GetAttr[name="weight"](%down_proj.31)
  %hidden_states.477 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2869, %weight.289), scope: __module.model/__module.model.layers.15/__module.model.layers.15.mlp/__module.model.layers.15.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.479 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2860, %hidden_states.477, %242), scope: __module.model/__module.model.layers.15 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %2873 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.479, %2845, %2846)
  %2874 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2875 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2876 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2873)
  %mlp.33 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_211.MistralMLP = prim::GetAttr[name="mlp"](%_16)
  %post_attention_layernorm.33 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_213.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_16)
  %self_attn.33 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_206.MistralAttention = prim::GetAttr[name="self_attn"](%_16)
  %input_layernorm.33 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_212.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_16)
  %weight.291 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.33)
  %hidden_states.481 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%2874, %229, %239, %239, %241), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2883 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.481, %243), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2884 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm
  %variance.65 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2883, %2884, %220, %241), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2886 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.65, %219, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2887 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2886), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.483 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.481, %2887), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.485 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.483, %229, %239, %239, %241), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.487 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.291, %hidden_states.485), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2891 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.487, %hidden_states.481)
  %2892 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2893 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2891)
  %o_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_205.Linear = prim::GetAttr[name="o_proj"](%self_attn.33)
  %v_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_204.Linear = prim::GetAttr[name="v_proj"](%self_attn.33)
  %k_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_203.Linear = prim::GetAttr[name="k_proj"](%self_attn.33)
  %q_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_202.Linear = prim::GetAttr[name="q_proj"](%self_attn.33)
  %2898 : int = aten::size(%2892, %237), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %2899 : int = aten::size(%2892, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.293 : Tensor = prim::GetAttr[name="weight"](%q_proj.33)
  %2901 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2892, %weight.293), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn/__module.model.layers.16.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2902 : int[] = prim::ListConstruct(%2898, %2899, %231, %226), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2903 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%2901, %2902), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.33 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2903, %242, %243), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.295 : Tensor = prim::GetAttr[name="weight"](%k_proj.33)
  %2906 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2892, %weight.295), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn/__module.model.layers.16.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2907 : int[] = prim::ListConstruct(%2898, %2899, %231, %226), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2908 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2906, %2907), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.33 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2908, %242, %243), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.297 : Tensor = prim::GetAttr[name="weight"](%v_proj.33)
  %2911 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2892, %weight.297), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn/__module.model.layers.16.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2912 : int[] = prim::ListConstruct(%2898, %2899, %231, %226), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2913 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2911, %2912), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.33 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2913, %242, %243), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.37 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.37 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %2917 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.33, %cos.37), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2918 : int = aten::size(%q.33, %235), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2919 : Long(device=cpu) = prim::NumToTensor(%2918), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2920 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2919, %225), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2921 : int = aten::Int(%2920), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %x1.65 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.33, %235, %237, %2921, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2923 : int = aten::size(%q.33, %235), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2924 : Long(device=cpu) = prim::NumToTensor(%2923), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2925 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2924, %225), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2926 : int = aten::Int(%2925), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %x2.65 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.33, %235, %2926, %236, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2928 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.65), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2929 : Tensor[] = prim::ListConstruct(%2928, %x1.65), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2930 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2929, %231), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2931 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2930, %sin.37), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.33 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2917, %2931, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2933 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.33, %cos.37), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2934 : int = aten::size(%k.33, %235), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2935 : Long(device=cpu) = prim::NumToTensor(%2934), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2936 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2935, %225), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2937 : int = aten::Int(%2936), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %x1.67 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.33, %235, %237, %2937, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2939 : int = aten::size(%k.33, %235), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2940 : Long(device=cpu) = prim::NumToTensor(%2939), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2941 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2940, %225), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2942 : int = aten::Int(%2941), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %x2.67 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.33, %235, %2942, %236, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2944 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.67), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2945 : Tensor[] = prim::ListConstruct(%2944, %x1.67), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2946 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2945, %231), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2947 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2946, %sin.37), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.33 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2933, %2947, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2949 : Tensor[] = prim::ListConstruct(%71, %key_states.33), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %hidden_states.489 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2949, %224), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %2951 : Tensor[] = prim::ListConstruct(%72, %value_states.33), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %hidden_states.493 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2951, %224), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %2953 : int = aten::size(%hidden_states.489, %237), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2954 : int = aten::size(%hidden_states.489, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.65 : Long(device=cpu) = prim::NumToTensor(%2954), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2956 : int = aten::size(%hidden_states.489, %243), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2957 : int = aten::size(%hidden_states.489, %235), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2958 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.489, %237, %237, %236, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2959 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2958, %242, %237, %236, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2960 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2959, %243), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2961 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2960, %235, %237, %236, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2962 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2961, %234, %237, %236, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2963 : int[] = prim::ListConstruct(%2953, %2954, %234, %2956, %2957), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %hidden_states.491 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2962, %2963, %239), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2965 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.65, %223), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2966 : int = aten::Int(%2965), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2967 : int[] = prim::ListConstruct(%2953, %2966, %2956, %2957), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %key.33 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.491, %2967), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2969 : int = aten::size(%hidden_states.493, %237), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2970 : int = aten::size(%hidden_states.493, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.67 : Long(device=cpu) = prim::NumToTensor(%2970), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2972 : int = aten::size(%hidden_states.493, %243), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2973 : int = aten::size(%hidden_states.493, %235), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2974 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.493, %237, %237, %236, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2975 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2974, %242, %237, %236, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2976 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2975, %243), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2977 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2976, %235, %237, %236, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2978 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2977, %234, %237, %236, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2979 : int[] = prim::ListConstruct(%2969, %2970, %234, %2972, %2973), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %hidden_states.495 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2978, %2979, %239), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2981 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.67, %223), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2982 : int = aten::Int(%2981), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2983 : int[] = prim::ListConstruct(%2969, %2982, %2972, %2973), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %value.33 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.495, %2983), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2985 : int = aten::size(%key.33, %243), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2986 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2987 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2986, %242, %237, %236, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2988 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2987, %243, %237, %236, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.39 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2988, %235, %237, %2985, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.65 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.33, %key.33, %value.33, %attention_mask.39, %222, %239, %221, %239), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %2991 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.65, %242, %243), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.67 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2991, %237), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %2993 : int[] = prim::ListConstruct(%2898, %2899, %231), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2994 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.67, %2993), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %2995 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%2994, %237), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.299 : Tensor = prim::GetAttr[name="weight"](%o_proj.33)
  %hidden_states.497 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2995, %weight.299), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn/__module.model.layers.16.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2998 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.497, %hidden_states.489, %hidden_states.493)
  %2999 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3000 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3001 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2998)
  %hidden_states.499 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2893, %2999, %242), scope: __module.model/__module.model.layers.16 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.301 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.33)
  %hidden_states.501 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.499, %229, %239, %239, %241), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3005 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.501, %243), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3006 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm
  %variance.67 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3005, %3006, %220, %241), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3008 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.67, %219, %242), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3009 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3008), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.503 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.501, %3009), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.505 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.503, %229, %239, %239, %241), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3012 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.301, %hidden_states.505), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3013 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%3012, %hidden_states.501)
  %3014 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3015 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3013)
  %down_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_209.Linear = prim::GetAttr[name="down_proj"](%mlp.33)
  %up_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_208.Linear = prim::GetAttr[name="up_proj"](%mlp.33)
  %gate_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_207.Linear = prim::GetAttr[name="gate_proj"](%mlp.33)
  %weight.303 : Tensor = prim::GetAttr[name="weight"](%gate_proj.33)
  %input.33 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3014, %weight.303), scope: __module.model/__module.model.layers.16/__module.model.layers.16.mlp/__module.model.layers.16.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3021 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.33), scope: __module.model/__module.model.layers.16/__module.model.layers.16.mlp/__module.model.layers.16.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.305 : Tensor = prim::GetAttr[name="weight"](%up_proj.33)
  %3023 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3014, %weight.305), scope: __module.model/__module.model.layers.16/__module.model.layers.16.mlp/__module.model.layers.16.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3024 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%3021, %3023), scope: __module.model/__module.model.layers.16/__module.model.layers.16.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.307 : Tensor = prim::GetAttr[name="weight"](%down_proj.33)
  %hidden_states.507 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3024, %weight.307), scope: __module.model/__module.model.layers.16/__module.model.layers.16.mlp/__module.model.layers.16.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.509 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3015, %hidden_states.507, %242), scope: __module.model/__module.model.layers.16 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %3028 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.509, %3000, %3001)
  %3029 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3030 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3031 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3028)
  %mlp.35 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_224.MistralMLP = prim::GetAttr[name="mlp"](%_17)
  %post_attention_layernorm.35 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_226.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_17)
  %self_attn.35 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_219.MistralAttention = prim::GetAttr[name="self_attn"](%_17)
  %input_layernorm.35 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_225.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_17)
  %weight.309 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.35)
  %hidden_states.511 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%3029, %229, %239, %239, %241), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3038 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.511, %243), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3039 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm
  %variance.69 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3038, %3039, %220, %241), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3041 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.69, %219, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3042 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3041), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.513 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.511, %3042), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.515 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.513, %229, %239, %239, %241), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.517 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.309, %hidden_states.515), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3046 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.517, %hidden_states.511)
  %3047 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3048 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3046)
  %o_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_218.Linear = prim::GetAttr[name="o_proj"](%self_attn.35)
  %v_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_217.Linear = prim::GetAttr[name="v_proj"](%self_attn.35)
  %k_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_216.Linear = prim::GetAttr[name="k_proj"](%self_attn.35)
  %q_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_215.Linear = prim::GetAttr[name="q_proj"](%self_attn.35)
  %3053 : int = aten::size(%3047, %237), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %3054 : int = aten::size(%3047, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.311 : Tensor = prim::GetAttr[name="weight"](%q_proj.35)
  %3056 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3047, %weight.311), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn/__module.model.layers.17.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3057 : int[] = prim::ListConstruct(%3053, %3054, %231, %226), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %3058 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%3056, %3057), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.35 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3058, %242, %243), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.313 : Tensor = prim::GetAttr[name="weight"](%k_proj.35)
  %3061 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3047, %weight.313), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn/__module.model.layers.17.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3062 : int[] = prim::ListConstruct(%3053, %3054, %231, %226), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %3063 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3061, %3062), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.35 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3063, %242, %243), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.315 : Tensor = prim::GetAttr[name="weight"](%v_proj.35)
  %3066 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3047, %weight.315), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn/__module.model.layers.17.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3067 : int[] = prim::ListConstruct(%3053, %3054, %231, %226), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %3068 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3066, %3067), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.35 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3068, %242, %243), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.39 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.39 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %3072 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.35, %cos.39), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3073 : int = aten::size(%q.35, %235), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3074 : Long(device=cpu) = prim::NumToTensor(%3073), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %3075 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3074, %225), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3076 : int = aten::Int(%3075), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %x1.69 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.35, %235, %237, %3076, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3078 : int = aten::size(%q.35, %235), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3079 : Long(device=cpu) = prim::NumToTensor(%3078), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %3080 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3079, %225), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3081 : int = aten::Int(%3080), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %x2.69 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.35, %235, %3081, %236, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3083 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.69), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3084 : Tensor[] = prim::ListConstruct(%3083, %x1.69), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %3085 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3084, %231), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3086 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3085, %sin.39), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.35 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3072, %3086, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3088 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.35, %cos.39), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3089 : int = aten::size(%k.35, %235), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3090 : Long(device=cpu) = prim::NumToTensor(%3089), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %3091 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3090, %225), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3092 : int = aten::Int(%3091), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %x1.71 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.35, %235, %237, %3092, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3094 : int = aten::size(%k.35, %235), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3095 : Long(device=cpu) = prim::NumToTensor(%3094), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %3096 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3095, %225), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3097 : int = aten::Int(%3096), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %x2.71 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.35, %235, %3097, %236, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3099 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.71), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3100 : Tensor[] = prim::ListConstruct(%3099, %x1.71), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %3101 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3100, %231), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3102 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3101, %sin.39), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.35 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3088, %3102, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3104 : Tensor[] = prim::ListConstruct(%73, %key_states.35), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %hidden_states.519 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3104, %224), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %3106 : Tensor[] = prim::ListConstruct(%74, %value_states.35), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %hidden_states.523 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3106, %224), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %3108 : int = aten::size(%hidden_states.519, %237), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3109 : int = aten::size(%hidden_states.519, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.69 : Long(device=cpu) = prim::NumToTensor(%3109), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %3111 : int = aten::size(%hidden_states.519, %243), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3112 : int = aten::size(%hidden_states.519, %235), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3113 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.519, %237, %237, %236, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3114 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3113, %242, %237, %236, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3115 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3114, %243), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3116 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3115, %235, %237, %236, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3117 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3116, %234, %237, %236, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3118 : int[] = prim::ListConstruct(%3108, %3109, %234, %3111, %3112), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %hidden_states.521 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3117, %3118, %239), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3120 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.69, %223), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3121 : int = aten::Int(%3120), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %3122 : int[] = prim::ListConstruct(%3108, %3121, %3111, %3112), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %key.35 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.521, %3122), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3124 : int = aten::size(%hidden_states.523, %237), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3125 : int = aten::size(%hidden_states.523, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.71 : Long(device=cpu) = prim::NumToTensor(%3125), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %3127 : int = aten::size(%hidden_states.523, %243), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3128 : int = aten::size(%hidden_states.523, %235), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3129 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.523, %237, %237, %236, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3130 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3129, %242, %237, %236, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3131 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3130, %243), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3132 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3131, %235, %237, %236, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3133 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3132, %234, %237, %236, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3134 : int[] = prim::ListConstruct(%3124, %3125, %234, %3127, %3128), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %hidden_states.525 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3133, %3134, %239), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3136 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.71, %223), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3137 : int = aten::Int(%3136), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %3138 : int[] = prim::ListConstruct(%3124, %3137, %3127, %3128), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %value.35 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.525, %3138), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3140 : int = aten::size(%key.35, %243), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3141 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3142 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3141, %242, %237, %236, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3143 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3142, %243, %237, %236, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.41 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3143, %235, %237, %3140, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.69 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.35, %key.35, %value.35, %attention_mask.41, %222, %239, %221, %239), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %3146 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.69, %242, %243), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.71 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3146, %237), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %3148 : int[] = prim::ListConstruct(%3053, %3054, %231), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %3149 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.71, %3148), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %3150 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%3149, %237), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.317 : Tensor = prim::GetAttr[name="weight"](%o_proj.35)
  %hidden_states.527 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3150, %weight.317), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn/__module.model.layers.17.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3153 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.527, %hidden_states.519, %hidden_states.523)
  %3154 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3155 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3156 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3153)
  %hidden_states.529 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3048, %3154, %242), scope: __module.model/__module.model.layers.17 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.319 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.35)
  %hidden_states.531 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.529, %229, %239, %239, %241), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3160 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.531, %243), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3161 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm
  %variance.71 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3160, %3161, %220, %241), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3163 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.71, %219, %242), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3164 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3163), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.533 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.531, %3164), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.535 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.533, %229, %239, %239, %241), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3167 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.319, %hidden_states.535), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3168 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%3167, %hidden_states.531)
  %3169 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3170 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3168)
  %down_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_222.Linear = prim::GetAttr[name="down_proj"](%mlp.35)
  %up_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_221.Linear = prim::GetAttr[name="up_proj"](%mlp.35)
  %gate_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_220.Linear = prim::GetAttr[name="gate_proj"](%mlp.35)
  %weight.321 : Tensor = prim::GetAttr[name="weight"](%gate_proj.35)
  %input.35 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3169, %weight.321), scope: __module.model/__module.model.layers.17/__module.model.layers.17.mlp/__module.model.layers.17.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3176 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.35), scope: __module.model/__module.model.layers.17/__module.model.layers.17.mlp/__module.model.layers.17.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.323 : Tensor = prim::GetAttr[name="weight"](%up_proj.35)
  %3178 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3169, %weight.323), scope: __module.model/__module.model.layers.17/__module.model.layers.17.mlp/__module.model.layers.17.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3179 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%3176, %3178), scope: __module.model/__module.model.layers.17/__module.model.layers.17.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.325 : Tensor = prim::GetAttr[name="weight"](%down_proj.35)
  %hidden_states.537 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3179, %weight.325), scope: __module.model/__module.model.layers.17/__module.model.layers.17.mlp/__module.model.layers.17.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.539 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3170, %hidden_states.537, %242), scope: __module.model/__module.model.layers.17 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %3183 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.539, %3155, %3156)
  %3184 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3185 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3186 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3183)
  %mlp.37 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_237.MistralMLP = prim::GetAttr[name="mlp"](%_18)
  %post_attention_layernorm.37 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_239.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_18)
  %self_attn.37 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_232.MistralAttention = prim::GetAttr[name="self_attn"](%_18)
  %input_layernorm.37 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_238.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_18)
  %weight.327 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.37)
  %hidden_states.541 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%3184, %229, %239, %239, %241), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3193 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.541, %243), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3194 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm
  %variance.73 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3193, %3194, %220, %241), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3196 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.73, %219, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3197 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3196), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.543 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.541, %3197), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.545 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.543, %229, %239, %239, %241), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.547 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.327, %hidden_states.545), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3201 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.547, %hidden_states.541)
  %3202 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3203 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3201)
  %o_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_231.Linear = prim::GetAttr[name="o_proj"](%self_attn.37)
  %v_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_230.Linear = prim::GetAttr[name="v_proj"](%self_attn.37)
  %k_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_229.Linear = prim::GetAttr[name="k_proj"](%self_attn.37)
  %q_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_228.Linear = prim::GetAttr[name="q_proj"](%self_attn.37)
  %3208 : int = aten::size(%3202, %237), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %3209 : int = aten::size(%3202, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.329 : Tensor = prim::GetAttr[name="weight"](%q_proj.37)
  %3211 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3202, %weight.329), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn/__module.model.layers.18.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3212 : int[] = prim::ListConstruct(%3208, %3209, %231, %226), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %3213 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%3211, %3212), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.37 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3213, %242, %243), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.331 : Tensor = prim::GetAttr[name="weight"](%k_proj.37)
  %3216 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3202, %weight.331), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn/__module.model.layers.18.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3217 : int[] = prim::ListConstruct(%3208, %3209, %231, %226), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %3218 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3216, %3217), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.37 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3218, %242, %243), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.333 : Tensor = prim::GetAttr[name="weight"](%v_proj.37)
  %3221 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3202, %weight.333), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn/__module.model.layers.18.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3222 : int[] = prim::ListConstruct(%3208, %3209, %231, %226), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %3223 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3221, %3222), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.37 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3223, %242, %243), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.41 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.41 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %3227 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.37, %cos.41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3228 : int = aten::size(%q.37, %235), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3229 : Long(device=cpu) = prim::NumToTensor(%3228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %3230 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3229, %225), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3231 : int = aten::Int(%3230), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %x1.73 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.37, %235, %237, %3231, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3233 : int = aten::size(%q.37, %235), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3234 : Long(device=cpu) = prim::NumToTensor(%3233), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %3235 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3234, %225), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3236 : int = aten::Int(%3235), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %x2.73 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.37, %235, %3236, %236, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3238 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.73), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3239 : Tensor[] = prim::ListConstruct(%3238, %x1.73), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %3240 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3239, %231), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3241 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3240, %sin.41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.37 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3227, %3241, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3243 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.37, %cos.41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3244 : int = aten::size(%k.37, %235), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3245 : Long(device=cpu) = prim::NumToTensor(%3244), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %3246 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3245, %225), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3247 : int = aten::Int(%3246), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %x1.75 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.37, %235, %237, %3247, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3249 : int = aten::size(%k.37, %235), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3250 : Long(device=cpu) = prim::NumToTensor(%3249), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %3251 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3250, %225), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3252 : int = aten::Int(%3251), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %x2.75 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.37, %235, %3252, %236, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3254 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.75), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3255 : Tensor[] = prim::ListConstruct(%3254, %x1.75), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %3256 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3255, %231), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3257 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3256, %sin.41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.37 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3243, %3257, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3259 : Tensor[] = prim::ListConstruct(%75, %key_states.37), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %hidden_states.549 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3259, %224), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %3261 : Tensor[] = prim::ListConstruct(%76, %value_states.37), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %hidden_states.553 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3261, %224), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %3263 : int = aten::size(%hidden_states.549, %237), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3264 : int = aten::size(%hidden_states.549, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.73 : Long(device=cpu) = prim::NumToTensor(%3264), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %3266 : int = aten::size(%hidden_states.549, %243), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3267 : int = aten::size(%hidden_states.549, %235), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3268 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.549, %237, %237, %236, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3269 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3268, %242, %237, %236, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3270 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3269, %243), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3271 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3270, %235, %237, %236, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3272 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3271, %234, %237, %236, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3273 : int[] = prim::ListConstruct(%3263, %3264, %234, %3266, %3267), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %hidden_states.551 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3272, %3273, %239), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3275 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.73, %223), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3276 : int = aten::Int(%3275), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %3277 : int[] = prim::ListConstruct(%3263, %3276, %3266, %3267), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %key.37 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.551, %3277), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3279 : int = aten::size(%hidden_states.553, %237), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3280 : int = aten::size(%hidden_states.553, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.75 : Long(device=cpu) = prim::NumToTensor(%3280), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %3282 : int = aten::size(%hidden_states.553, %243), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3283 : int = aten::size(%hidden_states.553, %235), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3284 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.553, %237, %237, %236, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3285 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3284, %242, %237, %236, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3286 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3285, %243), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3287 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3286, %235, %237, %236, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3288 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3287, %234, %237, %236, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3289 : int[] = prim::ListConstruct(%3279, %3280, %234, %3282, %3283), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %hidden_states.555 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3288, %3289, %239), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3291 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.75, %223), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3292 : int = aten::Int(%3291), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %3293 : int[] = prim::ListConstruct(%3279, %3292, %3282, %3283), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %value.37 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.555, %3293), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3295 : int = aten::size(%key.37, %243), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3296 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3297 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3296, %242, %237, %236, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3298 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3297, %243, %237, %236, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.43 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3298, %235, %237, %3295, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.73 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.37, %key.37, %value.37, %attention_mask.43, %222, %239, %221, %239), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %3301 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.73, %242, %243), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.75 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3301, %237), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %3303 : int[] = prim::ListConstruct(%3208, %3209, %231), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %3304 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.75, %3303), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %3305 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%3304, %237), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.335 : Tensor = prim::GetAttr[name="weight"](%o_proj.37)
  %hidden_states.557 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3305, %weight.335), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn/__module.model.layers.18.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3308 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.557, %hidden_states.549, %hidden_states.553)
  %3309 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3310 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3311 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3308)
  %hidden_states.559 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3203, %3309, %242), scope: __module.model/__module.model.layers.18 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.337 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.37)
  %hidden_states.561 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.559, %229, %239, %239, %241), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3315 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.561, %243), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3316 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm
  %variance.75 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3315, %3316, %220, %241), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3318 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.75, %219, %242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3319 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3318), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.563 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.561, %3319), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.565 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.563, %229, %239, %239, %241), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3322 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.337, %hidden_states.565), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3323 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%3322, %hidden_states.561)
  %3324 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3325 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3323)
  %down_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_235.Linear = prim::GetAttr[name="down_proj"](%mlp.37)
  %up_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_234.Linear = prim::GetAttr[name="up_proj"](%mlp.37)
  %gate_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_233.Linear = prim::GetAttr[name="gate_proj"](%mlp.37)
  %weight.339 : Tensor = prim::GetAttr[name="weight"](%gate_proj.37)
  %input.37 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3324, %weight.339), scope: __module.model/__module.model.layers.18/__module.model.layers.18.mlp/__module.model.layers.18.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3331 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.37), scope: __module.model/__module.model.layers.18/__module.model.layers.18.mlp/__module.model.layers.18.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.341 : Tensor = prim::GetAttr[name="weight"](%up_proj.37)
  %3333 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3324, %weight.341), scope: __module.model/__module.model.layers.18/__module.model.layers.18.mlp/__module.model.layers.18.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3334 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%3331, %3333), scope: __module.model/__module.model.layers.18/__module.model.layers.18.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.343 : Tensor = prim::GetAttr[name="weight"](%down_proj.37)
  %hidden_states.567 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3334, %weight.343), scope: __module.model/__module.model.layers.18/__module.model.layers.18.mlp/__module.model.layers.18.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.569 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3325, %hidden_states.567, %242), scope: __module.model/__module.model.layers.18 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %3338 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.569, %3310, %3311)
  %3339 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3340 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3341 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3338)
  %mlp.39 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_250.MistralMLP = prim::GetAttr[name="mlp"](%_19)
  %post_attention_layernorm.39 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_252.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_19)
  %self_attn.39 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_245.MistralAttention = prim::GetAttr[name="self_attn"](%_19)
  %input_layernorm.39 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_251.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_19)
  %weight.345 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.39)
  %hidden_states.571 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%3339, %229, %239, %239, %241), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3348 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.571, %243), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3349 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm
  %variance.77 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3348, %3349, %220, %241), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3351 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.77, %219, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3352 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3351), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.573 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.571, %3352), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.575 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.573, %229, %239, %239, %241), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.577 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.345, %hidden_states.575), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3356 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.577, %hidden_states.571)
  %3357 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3358 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3356)
  %o_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_244.Linear = prim::GetAttr[name="o_proj"](%self_attn.39)
  %v_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_243.Linear = prim::GetAttr[name="v_proj"](%self_attn.39)
  %k_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_242.Linear = prim::GetAttr[name="k_proj"](%self_attn.39)
  %q_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_241.Linear = prim::GetAttr[name="q_proj"](%self_attn.39)
  %3363 : int = aten::size(%3357, %237), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %3364 : int = aten::size(%3357, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.347 : Tensor = prim::GetAttr[name="weight"](%q_proj.39)
  %3366 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3357, %weight.347), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn/__module.model.layers.19.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3367 : int[] = prim::ListConstruct(%3363, %3364, %231, %226), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3368 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%3366, %3367), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.39 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3368, %242, %243), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.349 : Tensor = prim::GetAttr[name="weight"](%k_proj.39)
  %3371 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3357, %weight.349), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn/__module.model.layers.19.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3372 : int[] = prim::ListConstruct(%3363, %3364, %231, %226), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3373 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3371, %3372), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.39 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3373, %242, %243), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.351 : Tensor = prim::GetAttr[name="weight"](%v_proj.39)
  %3376 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3357, %weight.351), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn/__module.model.layers.19.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3377 : int[] = prim::ListConstruct(%3363, %3364, %231, %226), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3378 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3376, %3377), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.39 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3378, %242, %243), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.43 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.43 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %3382 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.39, %cos.43), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3383 : int = aten::size(%q.39, %235), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3384 : Long(device=cpu) = prim::NumToTensor(%3383), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3385 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3384, %225), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3386 : int = aten::Int(%3385), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %x1.77 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.39, %235, %237, %3386, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3388 : int = aten::size(%q.39, %235), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3389 : Long(device=cpu) = prim::NumToTensor(%3388), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3390 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3389, %225), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3391 : int = aten::Int(%3390), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %x2.77 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.39, %235, %3391, %236, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3393 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.77), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3394 : Tensor[] = prim::ListConstruct(%3393, %x1.77), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3395 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3394, %231), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3396 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3395, %sin.43), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.39 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3382, %3396, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3398 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.39, %cos.43), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3399 : int = aten::size(%k.39, %235), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3400 : Long(device=cpu) = prim::NumToTensor(%3399), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3401 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3400, %225), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3402 : int = aten::Int(%3401), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %x1.79 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.39, %235, %237, %3402, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3404 : int = aten::size(%k.39, %235), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3405 : Long(device=cpu) = prim::NumToTensor(%3404), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3406 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3405, %225), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3407 : int = aten::Int(%3406), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %x2.79 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.39, %235, %3407, %236, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3409 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.79), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3410 : Tensor[] = prim::ListConstruct(%3409, %x1.79), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3411 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3410, %231), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3412 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3411, %sin.43), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.39 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3398, %3412, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3414 : Tensor[] = prim::ListConstruct(%77, %key_states.39), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %hidden_states.579 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3414, %224), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %3416 : Tensor[] = prim::ListConstruct(%78, %value_states.39), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %hidden_states.583 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3416, %224), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %3418 : int = aten::size(%hidden_states.579, %237), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3419 : int = aten::size(%hidden_states.579, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.77 : Long(device=cpu) = prim::NumToTensor(%3419), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3421 : int = aten::size(%hidden_states.579, %243), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3422 : int = aten::size(%hidden_states.579, %235), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3423 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.579, %237, %237, %236, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3424 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3423, %242, %237, %236, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3425 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3424, %243), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3426 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3425, %235, %237, %236, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3427 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3426, %234, %237, %236, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3428 : int[] = prim::ListConstruct(%3418, %3419, %234, %3421, %3422), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %hidden_states.581 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3427, %3428, %239), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3430 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.77, %223), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3431 : int = aten::Int(%3430), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3432 : int[] = prim::ListConstruct(%3418, %3431, %3421, %3422), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %key.39 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.581, %3432), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3434 : int = aten::size(%hidden_states.583, %237), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3435 : int = aten::size(%hidden_states.583, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.79 : Long(device=cpu) = prim::NumToTensor(%3435), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3437 : int = aten::size(%hidden_states.583, %243), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3438 : int = aten::size(%hidden_states.583, %235), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3439 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.583, %237, %237, %236, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3440 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3439, %242, %237, %236, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3441 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3440, %243), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3442 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3441, %235, %237, %236, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3443 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3442, %234, %237, %236, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3444 : int[] = prim::ListConstruct(%3434, %3435, %234, %3437, %3438), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %hidden_states.585 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3443, %3444, %239), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3446 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.79, %223), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3447 : int = aten::Int(%3446), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3448 : int[] = prim::ListConstruct(%3434, %3447, %3437, %3438), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %value.39 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.585, %3448), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3450 : int = aten::size(%key.39, %243), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3451 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3452 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3451, %242, %237, %236, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3453 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3452, %243, %237, %236, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.45 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3453, %235, %237, %3450, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.77 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.39, %key.39, %value.39, %attention_mask.45, %222, %239, %221, %239), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %3456 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.77, %242, %243), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.79 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3456, %237), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %3458 : int[] = prim::ListConstruct(%3363, %3364, %231), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3459 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.79, %3458), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %3460 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%3459, %237), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.353 : Tensor = prim::GetAttr[name="weight"](%o_proj.39)
  %hidden_states.587 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3460, %weight.353), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn/__module.model.layers.19.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3463 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.587, %hidden_states.579, %hidden_states.583)
  %3464 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3465 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3466 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3463)
  %hidden_states.589 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3358, %3464, %242), scope: __module.model/__module.model.layers.19 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.355 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.39)
  %hidden_states.591 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.589, %229, %239, %239, %241), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3470 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.591, %243), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3471 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm
  %variance.79 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3470, %3471, %220, %241), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3473 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.79, %219, %242), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3474 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3473), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.593 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.591, %3474), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.595 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.593, %229, %239, %239, %241), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3477 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.355, %hidden_states.595), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3478 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%3477, %hidden_states.591)
  %3479 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3480 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3478)
  %down_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_248.Linear = prim::GetAttr[name="down_proj"](%mlp.39)
  %up_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_247.Linear = prim::GetAttr[name="up_proj"](%mlp.39)
  %gate_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_246.Linear = prim::GetAttr[name="gate_proj"](%mlp.39)
  %weight.357 : Tensor = prim::GetAttr[name="weight"](%gate_proj.39)
  %input.39 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3479, %weight.357), scope: __module.model/__module.model.layers.19/__module.model.layers.19.mlp/__module.model.layers.19.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3486 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.39), scope: __module.model/__module.model.layers.19/__module.model.layers.19.mlp/__module.model.layers.19.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.359 : Tensor = prim::GetAttr[name="weight"](%up_proj.39)
  %3488 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3479, %weight.359), scope: __module.model/__module.model.layers.19/__module.model.layers.19.mlp/__module.model.layers.19.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3489 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%3486, %3488), scope: __module.model/__module.model.layers.19/__module.model.layers.19.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.361 : Tensor = prim::GetAttr[name="weight"](%down_proj.39)
  %hidden_states.597 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3489, %weight.361), scope: __module.model/__module.model.layers.19/__module.model.layers.19.mlp/__module.model.layers.19.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.599 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3480, %hidden_states.597, %242), scope: __module.model/__module.model.layers.19 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %3493 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.599, %3465, %3466)
  %3494 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3495 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3496 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3493)
  %mlp.41 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_263.MistralMLP = prim::GetAttr[name="mlp"](%_20)
  %post_attention_layernorm.41 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_265.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_20)
  %self_attn.41 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_258.MistralAttention = prim::GetAttr[name="self_attn"](%_20)
  %input_layernorm.41 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_264.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_20)
  %weight.363 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.41)
  %hidden_states.601 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%3494, %229, %239, %239, %241), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3503 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.601, %243), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3504 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm
  %variance.81 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3503, %3504, %220, %241), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3506 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.81, %219, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3507 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3506), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.603 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.601, %3507), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.605 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.603, %229, %239, %239, %241), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.607 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.363, %hidden_states.605), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3511 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.607, %hidden_states.601)
  %3512 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3513 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3511)
  %o_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_257.Linear = prim::GetAttr[name="o_proj"](%self_attn.41)
  %v_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_256.Linear = prim::GetAttr[name="v_proj"](%self_attn.41)
  %k_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_255.Linear = prim::GetAttr[name="k_proj"](%self_attn.41)
  %q_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_254.Linear = prim::GetAttr[name="q_proj"](%self_attn.41)
  %3518 : int = aten::size(%3512, %237), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %3519 : int = aten::size(%3512, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.365 : Tensor = prim::GetAttr[name="weight"](%q_proj.41)
  %3521 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3512, %weight.365), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn/__module.model.layers.20.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3522 : int[] = prim::ListConstruct(%3518, %3519, %231, %226), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3523 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%3521, %3522), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.41 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3523, %242, %243), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.367 : Tensor = prim::GetAttr[name="weight"](%k_proj.41)
  %3526 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3512, %weight.367), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn/__module.model.layers.20.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3527 : int[] = prim::ListConstruct(%3518, %3519, %231, %226), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3528 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3526, %3527), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.41 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3528, %242, %243), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.369 : Tensor = prim::GetAttr[name="weight"](%v_proj.41)
  %3531 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3512, %weight.369), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn/__module.model.layers.20.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3532 : int[] = prim::ListConstruct(%3518, %3519, %231, %226), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3533 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3531, %3532), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.41 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3533, %242, %243), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.45 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.45 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %3537 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.41, %cos.45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3538 : int = aten::size(%q.41, %235), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3539 : Long(device=cpu) = prim::NumToTensor(%3538), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3540 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3539, %225), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3541 : int = aten::Int(%3540), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %x1.81 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.41, %235, %237, %3541, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3543 : int = aten::size(%q.41, %235), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3544 : Long(device=cpu) = prim::NumToTensor(%3543), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3545 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3544, %225), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3546 : int = aten::Int(%3545), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %x2.81 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.41, %235, %3546, %236, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3548 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.81), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3549 : Tensor[] = prim::ListConstruct(%3548, %x1.81), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3550 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3549, %231), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3551 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3550, %sin.45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.41 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3537, %3551, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3553 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.41, %cos.45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3554 : int = aten::size(%k.41, %235), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3555 : Long(device=cpu) = prim::NumToTensor(%3554), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3556 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3555, %225), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3557 : int = aten::Int(%3556), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %x1.83 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.41, %235, %237, %3557, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3559 : int = aten::size(%k.41, %235), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3560 : Long(device=cpu) = prim::NumToTensor(%3559), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3561 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3560, %225), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3562 : int = aten::Int(%3561), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %x2.83 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.41, %235, %3562, %236, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3564 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.83), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3565 : Tensor[] = prim::ListConstruct(%3564, %x1.83), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3566 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3565, %231), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3567 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3566, %sin.45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.41 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3553, %3567, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3569 : Tensor[] = prim::ListConstruct(%79, %key_states.41), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %hidden_states.609 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3569, %224), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %3571 : Tensor[] = prim::ListConstruct(%80, %value_states.41), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %hidden_states.613 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3571, %224), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %3573 : int = aten::size(%hidden_states.609, %237), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3574 : int = aten::size(%hidden_states.609, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.81 : Long(device=cpu) = prim::NumToTensor(%3574), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3576 : int = aten::size(%hidden_states.609, %243), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3577 : int = aten::size(%hidden_states.609, %235), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3578 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.609, %237, %237, %236, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3579 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3578, %242, %237, %236, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3580 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3579, %243), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3581 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3580, %235, %237, %236, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3582 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3581, %234, %237, %236, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3583 : int[] = prim::ListConstruct(%3573, %3574, %234, %3576, %3577), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %hidden_states.611 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3582, %3583, %239), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3585 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.81, %223), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3586 : int = aten::Int(%3585), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3587 : int[] = prim::ListConstruct(%3573, %3586, %3576, %3577), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %key.41 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.611, %3587), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3589 : int = aten::size(%hidden_states.613, %237), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3590 : int = aten::size(%hidden_states.613, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.83 : Long(device=cpu) = prim::NumToTensor(%3590), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3592 : int = aten::size(%hidden_states.613, %243), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3593 : int = aten::size(%hidden_states.613, %235), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3594 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.613, %237, %237, %236, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3595 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3594, %242, %237, %236, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3596 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3595, %243), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3597 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3596, %235, %237, %236, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3598 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3597, %234, %237, %236, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3599 : int[] = prim::ListConstruct(%3589, %3590, %234, %3592, %3593), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %hidden_states.615 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3598, %3599, %239), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3601 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.83, %223), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3602 : int = aten::Int(%3601), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3603 : int[] = prim::ListConstruct(%3589, %3602, %3592, %3593), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %value.41 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.615, %3603), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3605 : int = aten::size(%key.41, %243), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3606 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3607 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3606, %242, %237, %236, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3608 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3607, %243, %237, %236, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.47 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3608, %235, %237, %3605, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.81 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.41, %key.41, %value.41, %attention_mask.47, %222, %239, %221, %239), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %3611 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.81, %242, %243), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.83 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3611, %237), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %3613 : int[] = prim::ListConstruct(%3518, %3519, %231), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3614 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.83, %3613), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %3615 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%3614, %237), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.371 : Tensor = prim::GetAttr[name="weight"](%o_proj.41)
  %hidden_states.617 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3615, %weight.371), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn/__module.model.layers.20.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3618 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.617, %hidden_states.609, %hidden_states.613)
  %3619 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3620 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3621 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3618)
  %hidden_states.619 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3513, %3619, %242), scope: __module.model/__module.model.layers.20 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.373 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.41)
  %hidden_states.621 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.619, %229, %239, %239, %241), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3625 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.621, %243), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3626 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm
  %variance.83 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3625, %3626, %220, %241), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3628 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.83, %219, %242), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3629 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3628), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.623 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.621, %3629), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.625 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.623, %229, %239, %239, %241), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3632 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.373, %hidden_states.625), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3633 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%3632, %hidden_states.621)
  %3634 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3635 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3633)
  %down_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_261.Linear = prim::GetAttr[name="down_proj"](%mlp.41)
  %up_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_260.Linear = prim::GetAttr[name="up_proj"](%mlp.41)
  %gate_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_259.Linear = prim::GetAttr[name="gate_proj"](%mlp.41)
  %weight.375 : Tensor = prim::GetAttr[name="weight"](%gate_proj.41)
  %input.41 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3634, %weight.375), scope: __module.model/__module.model.layers.20/__module.model.layers.20.mlp/__module.model.layers.20.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3641 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.41), scope: __module.model/__module.model.layers.20/__module.model.layers.20.mlp/__module.model.layers.20.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.377 : Tensor = prim::GetAttr[name="weight"](%up_proj.41)
  %3643 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3634, %weight.377), scope: __module.model/__module.model.layers.20/__module.model.layers.20.mlp/__module.model.layers.20.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3644 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%3641, %3643), scope: __module.model/__module.model.layers.20/__module.model.layers.20.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.379 : Tensor = prim::GetAttr[name="weight"](%down_proj.41)
  %hidden_states.627 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3644, %weight.379), scope: __module.model/__module.model.layers.20/__module.model.layers.20.mlp/__module.model.layers.20.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.629 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3635, %hidden_states.627, %242), scope: __module.model/__module.model.layers.20 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %3648 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.629, %3620, %3621)
  %3649 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3650 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3651 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3648)
  %mlp.43 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_276.MistralMLP = prim::GetAttr[name="mlp"](%_21)
  %post_attention_layernorm.43 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_278.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_21)
  %self_attn.43 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_271.MistralAttention = prim::GetAttr[name="self_attn"](%_21)
  %input_layernorm.43 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_277.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_21)
  %weight.381 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.43)
  %hidden_states.631 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%3649, %229, %239, %239, %241), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3658 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.631, %243), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3659 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm
  %variance.85 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3658, %3659, %220, %241), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3661 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.85, %219, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3662 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3661), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.633 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.631, %3662), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.635 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.633, %229, %239, %239, %241), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.637 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.381, %hidden_states.635), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3666 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.637, %hidden_states.631)
  %3667 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3668 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3666)
  %o_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_270.Linear = prim::GetAttr[name="o_proj"](%self_attn.43)
  %v_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_269.Linear = prim::GetAttr[name="v_proj"](%self_attn.43)
  %k_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_268.Linear = prim::GetAttr[name="k_proj"](%self_attn.43)
  %q_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_267.Linear = prim::GetAttr[name="q_proj"](%self_attn.43)
  %3673 : int = aten::size(%3667, %237), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %3674 : int = aten::size(%3667, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.383 : Tensor = prim::GetAttr[name="weight"](%q_proj.43)
  %3676 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3667, %weight.383), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn/__module.model.layers.21.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3677 : int[] = prim::ListConstruct(%3673, %3674, %231, %226), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3678 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%3676, %3677), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.43 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3678, %242, %243), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.385 : Tensor = prim::GetAttr[name="weight"](%k_proj.43)
  %3681 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3667, %weight.385), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn/__module.model.layers.21.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3682 : int[] = prim::ListConstruct(%3673, %3674, %231, %226), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3683 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3681, %3682), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.43 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3683, %242, %243), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.387 : Tensor = prim::GetAttr[name="weight"](%v_proj.43)
  %3686 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3667, %weight.387), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn/__module.model.layers.21.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3687 : int[] = prim::ListConstruct(%3673, %3674, %231, %226), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3688 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3686, %3687), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.43 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3688, %242, %243), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.47 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.47 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %3692 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.43, %cos.47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3693 : int = aten::size(%q.43, %235), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3694 : Long(device=cpu) = prim::NumToTensor(%3693), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3695 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3694, %225), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3696 : int = aten::Int(%3695), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %x1.85 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.43, %235, %237, %3696, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3698 : int = aten::size(%q.43, %235), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3699 : Long(device=cpu) = prim::NumToTensor(%3698), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3700 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3699, %225), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3701 : int = aten::Int(%3700), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %x2.85 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.43, %235, %3701, %236, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3703 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.85), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3704 : Tensor[] = prim::ListConstruct(%3703, %x1.85), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3705 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3704, %231), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3706 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3705, %sin.47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.43 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3692, %3706, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3708 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.43, %cos.47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3709 : int = aten::size(%k.43, %235), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3710 : Long(device=cpu) = prim::NumToTensor(%3709), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3711 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3710, %225), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3712 : int = aten::Int(%3711), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %x1.87 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.43, %235, %237, %3712, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3714 : int = aten::size(%k.43, %235), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3715 : Long(device=cpu) = prim::NumToTensor(%3714), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3716 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3715, %225), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3717 : int = aten::Int(%3716), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %x2.87 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.43, %235, %3717, %236, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3719 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.87), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3720 : Tensor[] = prim::ListConstruct(%3719, %x1.87), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3721 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3720, %231), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3722 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3721, %sin.47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.43 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3708, %3722, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3724 : Tensor[] = prim::ListConstruct(%81, %key_states.43), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %hidden_states.639 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3724, %224), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %3726 : Tensor[] = prim::ListConstruct(%82, %value_states.43), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %hidden_states.643 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3726, %224), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %3728 : int = aten::size(%hidden_states.639, %237), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3729 : int = aten::size(%hidden_states.639, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.85 : Long(device=cpu) = prim::NumToTensor(%3729), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3731 : int = aten::size(%hidden_states.639, %243), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3732 : int = aten::size(%hidden_states.639, %235), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3733 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.639, %237, %237, %236, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3734 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3733, %242, %237, %236, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3735 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3734, %243), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3736 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3735, %235, %237, %236, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3737 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3736, %234, %237, %236, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3738 : int[] = prim::ListConstruct(%3728, %3729, %234, %3731, %3732), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %hidden_states.641 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3737, %3738, %239), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3740 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.85, %223), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3741 : int = aten::Int(%3740), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3742 : int[] = prim::ListConstruct(%3728, %3741, %3731, %3732), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %key.43 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.641, %3742), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3744 : int = aten::size(%hidden_states.643, %237), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3745 : int = aten::size(%hidden_states.643, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.87 : Long(device=cpu) = prim::NumToTensor(%3745), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3747 : int = aten::size(%hidden_states.643, %243), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3748 : int = aten::size(%hidden_states.643, %235), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3749 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.643, %237, %237, %236, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3750 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3749, %242, %237, %236, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3751 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3750, %243), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3752 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3751, %235, %237, %236, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3753 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3752, %234, %237, %236, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3754 : int[] = prim::ListConstruct(%3744, %3745, %234, %3747, %3748), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %hidden_states.645 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3753, %3754, %239), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3756 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.87, %223), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3757 : int = aten::Int(%3756), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3758 : int[] = prim::ListConstruct(%3744, %3757, %3747, %3748), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %value.43 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.645, %3758), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3760 : int = aten::size(%key.43, %243), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3761 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3762 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3761, %242, %237, %236, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3763 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3762, %243, %237, %236, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.49 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3763, %235, %237, %3760, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.85 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.43, %key.43, %value.43, %attention_mask.49, %222, %239, %221, %239), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %3766 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.85, %242, %243), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.87 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3766, %237), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %3768 : int[] = prim::ListConstruct(%3673, %3674, %231), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3769 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.87, %3768), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %3770 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%3769, %237), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.389 : Tensor = prim::GetAttr[name="weight"](%o_proj.43)
  %hidden_states.647 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3770, %weight.389), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn/__module.model.layers.21.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3773 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.647, %hidden_states.639, %hidden_states.643)
  %3774 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3775 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3776 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3773)
  %hidden_states.649 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3668, %3774, %242), scope: __module.model/__module.model.layers.21 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.391 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.43)
  %hidden_states.651 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.649, %229, %239, %239, %241), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3780 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.651, %243), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3781 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm
  %variance.87 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3780, %3781, %220, %241), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3783 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.87, %219, %242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3784 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3783), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.653 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.651, %3784), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.655 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.653, %229, %239, %239, %241), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3787 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.391, %hidden_states.655), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3788 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%3787, %hidden_states.651)
  %3789 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3790 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3788)
  %down_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_274.Linear = prim::GetAttr[name="down_proj"](%mlp.43)
  %up_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_273.Linear = prim::GetAttr[name="up_proj"](%mlp.43)
  %gate_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_272.Linear = prim::GetAttr[name="gate_proj"](%mlp.43)
  %weight.393 : Tensor = prim::GetAttr[name="weight"](%gate_proj.43)
  %input.43 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3789, %weight.393), scope: __module.model/__module.model.layers.21/__module.model.layers.21.mlp/__module.model.layers.21.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3796 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.43), scope: __module.model/__module.model.layers.21/__module.model.layers.21.mlp/__module.model.layers.21.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.395 : Tensor = prim::GetAttr[name="weight"](%up_proj.43)
  %3798 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3789, %weight.395), scope: __module.model/__module.model.layers.21/__module.model.layers.21.mlp/__module.model.layers.21.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3799 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%3796, %3798), scope: __module.model/__module.model.layers.21/__module.model.layers.21.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.397 : Tensor = prim::GetAttr[name="weight"](%down_proj.43)
  %hidden_states.657 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3799, %weight.397), scope: __module.model/__module.model.layers.21/__module.model.layers.21.mlp/__module.model.layers.21.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.659 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3790, %hidden_states.657, %242), scope: __module.model/__module.model.layers.21 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %3803 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.659, %3775, %3776)
  %3804 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3805 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3806 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3803)
  %mlp.45 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_289.MistralMLP = prim::GetAttr[name="mlp"](%_22)
  %post_attention_layernorm.45 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_291.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_22)
  %self_attn.45 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_284.MistralAttention = prim::GetAttr[name="self_attn"](%_22)
  %input_layernorm.45 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_290.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_22)
  %weight.399 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.45)
  %hidden_states.661 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%3804, %229, %239, %239, %241), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3813 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.661, %243), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3814 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm
  %variance.89 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3813, %3814, %220, %241), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3816 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.89, %219, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3817 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3816), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.663 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.661, %3817), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.665 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.663, %229, %239, %239, %241), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.667 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.399, %hidden_states.665), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3821 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.667, %hidden_states.661)
  %3822 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3823 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3821)
  %o_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_283.Linear = prim::GetAttr[name="o_proj"](%self_attn.45)
  %v_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_282.Linear = prim::GetAttr[name="v_proj"](%self_attn.45)
  %k_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_281.Linear = prim::GetAttr[name="k_proj"](%self_attn.45)
  %q_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_280.Linear = prim::GetAttr[name="q_proj"](%self_attn.45)
  %3828 : int = aten::size(%3822, %237), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %3829 : int = aten::size(%3822, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.401 : Tensor = prim::GetAttr[name="weight"](%q_proj.45)
  %3831 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3822, %weight.401), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn/__module.model.layers.22.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3832 : int[] = prim::ListConstruct(%3828, %3829, %231, %226), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3833 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%3831, %3832), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.45 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3833, %242, %243), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.403 : Tensor = prim::GetAttr[name="weight"](%k_proj.45)
  %3836 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3822, %weight.403), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn/__module.model.layers.22.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3837 : int[] = prim::ListConstruct(%3828, %3829, %231, %226), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3838 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3836, %3837), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.45 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3838, %242, %243), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.405 : Tensor = prim::GetAttr[name="weight"](%v_proj.45)
  %3841 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3822, %weight.405), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn/__module.model.layers.22.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3842 : int[] = prim::ListConstruct(%3828, %3829, %231, %226), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3843 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3841, %3842), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.45 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3843, %242, %243), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.49 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.49 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %3847 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.45, %cos.49), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3848 : int = aten::size(%q.45, %235), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3849 : Long(device=cpu) = prim::NumToTensor(%3848), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3850 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3849, %225), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3851 : int = aten::Int(%3850), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %x1.89 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.45, %235, %237, %3851, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3853 : int = aten::size(%q.45, %235), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3854 : Long(device=cpu) = prim::NumToTensor(%3853), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3855 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3854, %225), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3856 : int = aten::Int(%3855), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %x2.89 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.45, %235, %3856, %236, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3858 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.89), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3859 : Tensor[] = prim::ListConstruct(%3858, %x1.89), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3860 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3859, %231), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3861 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3860, %sin.49), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.45 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3847, %3861, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3863 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.45, %cos.49), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3864 : int = aten::size(%k.45, %235), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3865 : Long(device=cpu) = prim::NumToTensor(%3864), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3866 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3865, %225), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3867 : int = aten::Int(%3866), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %x1.91 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.45, %235, %237, %3867, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3869 : int = aten::size(%k.45, %235), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3870 : Long(device=cpu) = prim::NumToTensor(%3869), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3871 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3870, %225), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3872 : int = aten::Int(%3871), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %x2.91 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.45, %235, %3872, %236, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3874 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.91), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3875 : Tensor[] = prim::ListConstruct(%3874, %x1.91), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3876 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3875, %231), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3877 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3876, %sin.49), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.45 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3863, %3877, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3879 : Tensor[] = prim::ListConstruct(%83, %key_states.45), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %hidden_states.669 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3879, %224), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %3881 : Tensor[] = prim::ListConstruct(%84, %value_states.45), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %hidden_states.673 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3881, %224), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %3883 : int = aten::size(%hidden_states.669, %237), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3884 : int = aten::size(%hidden_states.669, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.89 : Long(device=cpu) = prim::NumToTensor(%3884), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3886 : int = aten::size(%hidden_states.669, %243), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3887 : int = aten::size(%hidden_states.669, %235), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3888 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.669, %237, %237, %236, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3889 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3888, %242, %237, %236, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3890 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3889, %243), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3891 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3890, %235, %237, %236, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3892 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3891, %234, %237, %236, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3893 : int[] = prim::ListConstruct(%3883, %3884, %234, %3886, %3887), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %hidden_states.671 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3892, %3893, %239), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3895 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.89, %223), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3896 : int = aten::Int(%3895), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3897 : int[] = prim::ListConstruct(%3883, %3896, %3886, %3887), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %key.45 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.671, %3897), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3899 : int = aten::size(%hidden_states.673, %237), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3900 : int = aten::size(%hidden_states.673, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.91 : Long(device=cpu) = prim::NumToTensor(%3900), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3902 : int = aten::size(%hidden_states.673, %243), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3903 : int = aten::size(%hidden_states.673, %235), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3904 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.673, %237, %237, %236, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3905 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3904, %242, %237, %236, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3906 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3905, %243), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3907 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3906, %235, %237, %236, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3908 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3907, %234, %237, %236, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3909 : int[] = prim::ListConstruct(%3899, %3900, %234, %3902, %3903), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %hidden_states.675 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3908, %3909, %239), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3911 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.91, %223), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3912 : int = aten::Int(%3911), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3913 : int[] = prim::ListConstruct(%3899, %3912, %3902, %3903), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %value.45 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.675, %3913), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3915 : int = aten::size(%key.45, %243), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3916 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3917 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3916, %242, %237, %236, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3918 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3917, %243, %237, %236, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.51 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3918, %235, %237, %3915, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.89 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.45, %key.45, %value.45, %attention_mask.51, %222, %239, %221, %239), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %3921 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.89, %242, %243), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.91 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3921, %237), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %3923 : int[] = prim::ListConstruct(%3828, %3829, %231), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3924 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.91, %3923), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %3925 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%3924, %237), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.407 : Tensor = prim::GetAttr[name="weight"](%o_proj.45)
  %hidden_states.677 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3925, %weight.407), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn/__module.model.layers.22.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3928 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.677, %hidden_states.669, %hidden_states.673)
  %3929 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3930 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3931 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3928)
  %hidden_states.679 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3823, %3929, %242), scope: __module.model/__module.model.layers.22 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.409 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.45)
  %hidden_states.681 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.679, %229, %239, %239, %241), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3935 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.681, %243), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3936 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm
  %variance.91 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3935, %3936, %220, %241), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3938 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.91, %219, %242), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3939 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3938), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.683 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.681, %3939), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.685 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.683, %229, %239, %239, %241), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3942 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.409, %hidden_states.685), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3943 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%3942, %hidden_states.681)
  %3944 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3945 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3943)
  %down_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_287.Linear = prim::GetAttr[name="down_proj"](%mlp.45)
  %up_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_286.Linear = prim::GetAttr[name="up_proj"](%mlp.45)
  %gate_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_285.Linear = prim::GetAttr[name="gate_proj"](%mlp.45)
  %weight.411 : Tensor = prim::GetAttr[name="weight"](%gate_proj.45)
  %input.45 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3944, %weight.411), scope: __module.model/__module.model.layers.22/__module.model.layers.22.mlp/__module.model.layers.22.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3951 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.45), scope: __module.model/__module.model.layers.22/__module.model.layers.22.mlp/__module.model.layers.22.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.413 : Tensor = prim::GetAttr[name="weight"](%up_proj.45)
  %3953 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3944, %weight.413), scope: __module.model/__module.model.layers.22/__module.model.layers.22.mlp/__module.model.layers.22.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3954 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%3951, %3953), scope: __module.model/__module.model.layers.22/__module.model.layers.22.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.415 : Tensor = prim::GetAttr[name="weight"](%down_proj.45)
  %hidden_states.687 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3954, %weight.415), scope: __module.model/__module.model.layers.22/__module.model.layers.22.mlp/__module.model.layers.22.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.689 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3945, %hidden_states.687, %242), scope: __module.model/__module.model.layers.22 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %3958 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.689, %3930, %3931)
  %3959 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3960 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3961 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3958)
  %mlp.47 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_302.MistralMLP = prim::GetAttr[name="mlp"](%_23)
  %post_attention_layernorm.47 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_304.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_23)
  %self_attn.47 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_297.MistralAttention = prim::GetAttr[name="self_attn"](%_23)
  %input_layernorm.47 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_303.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_23)
  %weight.417 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.47)
  %hidden_states.691 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%3959, %229, %239, %239, %241), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3968 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.691, %243), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3969 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm
  %variance.93 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3968, %3969, %220, %241), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3971 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.93, %219, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3972 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3971), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.693 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.691, %3972), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.695 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.693, %229, %239, %239, %241), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.697 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.417, %hidden_states.695), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3976 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.697, %hidden_states.691)
  %3977 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3978 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3976)
  %o_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_296.Linear = prim::GetAttr[name="o_proj"](%self_attn.47)
  %v_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_295.Linear = prim::GetAttr[name="v_proj"](%self_attn.47)
  %k_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_294.Linear = prim::GetAttr[name="k_proj"](%self_attn.47)
  %q_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_293.Linear = prim::GetAttr[name="q_proj"](%self_attn.47)
  %3983 : int = aten::size(%3977, %237), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %3984 : int = aten::size(%3977, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.419 : Tensor = prim::GetAttr[name="weight"](%q_proj.47)
  %3986 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3977, %weight.419), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn/__module.model.layers.23.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3987 : int[] = prim::ListConstruct(%3983, %3984, %231, %226), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3988 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%3986, %3987), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.47 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3988, %242, %243), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.421 : Tensor = prim::GetAttr[name="weight"](%k_proj.47)
  %3991 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3977, %weight.421), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn/__module.model.layers.23.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3992 : int[] = prim::ListConstruct(%3983, %3984, %231, %226), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3993 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3991, %3992), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.47 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3993, %242, %243), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.423 : Tensor = prim::GetAttr[name="weight"](%v_proj.47)
  %3996 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3977, %weight.423), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn/__module.model.layers.23.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3997 : int[] = prim::ListConstruct(%3983, %3984, %231, %226), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3998 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3996, %3997), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.47 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3998, %242, %243), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.51 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.51 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %4002 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.47, %cos.51), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4003 : int = aten::size(%q.47, %235), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4004 : Long(device=cpu) = prim::NumToTensor(%4003), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %4005 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4004, %225), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4006 : int = aten::Int(%4005), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %x1.93 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.47, %235, %237, %4006, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4008 : int = aten::size(%q.47, %235), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4009 : Long(device=cpu) = prim::NumToTensor(%4008), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %4010 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4009, %225), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4011 : int = aten::Int(%4010), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %x2.93 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.47, %235, %4011, %236, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4013 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.93), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4014 : Tensor[] = prim::ListConstruct(%4013, %x1.93), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %4015 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4014, %231), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4016 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4015, %sin.51), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.47 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4002, %4016, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4018 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.47, %cos.51), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4019 : int = aten::size(%k.47, %235), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4020 : Long(device=cpu) = prim::NumToTensor(%4019), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %4021 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4020, %225), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4022 : int = aten::Int(%4021), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %x1.95 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.47, %235, %237, %4022, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4024 : int = aten::size(%k.47, %235), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4025 : Long(device=cpu) = prim::NumToTensor(%4024), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %4026 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4025, %225), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4027 : int = aten::Int(%4026), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %x2.95 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.47, %235, %4027, %236, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4029 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.95), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4030 : Tensor[] = prim::ListConstruct(%4029, %x1.95), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %4031 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4030, %231), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4032 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4031, %sin.51), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.47 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4018, %4032, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4034 : Tensor[] = prim::ListConstruct(%85, %key_states.47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %hidden_states.699 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4034, %224), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %4036 : Tensor[] = prim::ListConstruct(%86, %value_states.47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %hidden_states.703 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4036, %224), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %4038 : int = aten::size(%hidden_states.699, %237), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4039 : int = aten::size(%hidden_states.699, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.93 : Long(device=cpu) = prim::NumToTensor(%4039), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %4041 : int = aten::size(%hidden_states.699, %243), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4042 : int = aten::size(%hidden_states.699, %235), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4043 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.699, %237, %237, %236, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4044 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4043, %242, %237, %236, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4045 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4044, %243), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4046 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4045, %235, %237, %236, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4047 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4046, %234, %237, %236, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4048 : int[] = prim::ListConstruct(%4038, %4039, %234, %4041, %4042), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %hidden_states.701 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4047, %4048, %239), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4050 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.93, %223), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4051 : int = aten::Int(%4050), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %4052 : int[] = prim::ListConstruct(%4038, %4051, %4041, %4042), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %key.47 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.701, %4052), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4054 : int = aten::size(%hidden_states.703, %237), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4055 : int = aten::size(%hidden_states.703, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.95 : Long(device=cpu) = prim::NumToTensor(%4055), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %4057 : int = aten::size(%hidden_states.703, %243), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4058 : int = aten::size(%hidden_states.703, %235), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4059 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.703, %237, %237, %236, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4060 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4059, %242, %237, %236, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4061 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4060, %243), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4062 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4061, %235, %237, %236, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4063 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4062, %234, %237, %236, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4064 : int[] = prim::ListConstruct(%4054, %4055, %234, %4057, %4058), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %hidden_states.705 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4063, %4064, %239), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4066 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.95, %223), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4067 : int = aten::Int(%4066), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %4068 : int[] = prim::ListConstruct(%4054, %4067, %4057, %4058), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %value.47 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.705, %4068), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4070 : int = aten::size(%key.47, %243), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4071 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4072 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4071, %242, %237, %236, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4073 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4072, %243, %237, %236, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.53 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4073, %235, %237, %4070, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.93 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.47, %key.47, %value.47, %attention_mask.53, %222, %239, %221, %239), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %4076 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.93, %242, %243), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.95 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4076, %237), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %4078 : int[] = prim::ListConstruct(%3983, %3984, %231), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %4079 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.95, %4078), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %4080 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%4079, %237), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.425 : Tensor = prim::GetAttr[name="weight"](%o_proj.47)
  %hidden_states.707 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4080, %weight.425), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn/__module.model.layers.23.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4083 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.707, %hidden_states.699, %hidden_states.703)
  %4084 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4085 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4086 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4083)
  %hidden_states.709 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3978, %4084, %242), scope: __module.model/__module.model.layers.23 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.427 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.47)
  %hidden_states.711 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.709, %229, %239, %239, %241), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4090 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.711, %243), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4091 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm
  %variance.95 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4090, %4091, %220, %241), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4093 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.95, %219, %242), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4094 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4093), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.713 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.711, %4094), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.715 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.713, %229, %239, %239, %241), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4097 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.427, %hidden_states.715), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4098 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%4097, %hidden_states.711)
  %4099 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4100 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4098)
  %down_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_300.Linear = prim::GetAttr[name="down_proj"](%mlp.47)
  %up_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_299.Linear = prim::GetAttr[name="up_proj"](%mlp.47)
  %gate_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_298.Linear = prim::GetAttr[name="gate_proj"](%mlp.47)
  %weight.429 : Tensor = prim::GetAttr[name="weight"](%gate_proj.47)
  %input.47 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4099, %weight.429), scope: __module.model/__module.model.layers.23/__module.model.layers.23.mlp/__module.model.layers.23.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4106 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.mlp/__module.model.layers.23.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.431 : Tensor = prim::GetAttr[name="weight"](%up_proj.47)
  %4108 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4099, %weight.431), scope: __module.model/__module.model.layers.23/__module.model.layers.23.mlp/__module.model.layers.23.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4109 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%4106, %4108), scope: __module.model/__module.model.layers.23/__module.model.layers.23.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.433 : Tensor = prim::GetAttr[name="weight"](%down_proj.47)
  %hidden_states.717 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4109, %weight.433), scope: __module.model/__module.model.layers.23/__module.model.layers.23.mlp/__module.model.layers.23.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.719 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4100, %hidden_states.717, %242), scope: __module.model/__module.model.layers.23 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %4113 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.719, %4085, %4086)
  %4114 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4115 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4116 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4113)
  %mlp.49 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_315.MistralMLP = prim::GetAttr[name="mlp"](%_24)
  %post_attention_layernorm.49 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_317.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_24)
  %self_attn.49 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_310.MistralAttention = prim::GetAttr[name="self_attn"](%_24)
  %input_layernorm.49 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_316.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_24)
  %weight.435 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.49)
  %hidden_states.721 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%4114, %229, %239, %239, %241), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4123 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.721, %243), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4124 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm
  %variance.97 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4123, %4124, %220, %241), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4126 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.97, %219, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4127 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4126), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.723 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.721, %4127), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.725 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.723, %229, %239, %239, %241), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.727 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.435, %hidden_states.725), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4131 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.727, %hidden_states.721)
  %4132 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4133 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4131)
  %o_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_309.Linear = prim::GetAttr[name="o_proj"](%self_attn.49)
  %v_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_308.Linear = prim::GetAttr[name="v_proj"](%self_attn.49)
  %k_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_307.Linear = prim::GetAttr[name="k_proj"](%self_attn.49)
  %q_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_306.Linear = prim::GetAttr[name="q_proj"](%self_attn.49)
  %4138 : int = aten::size(%4132, %237), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %4139 : int = aten::size(%4132, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.437 : Tensor = prim::GetAttr[name="weight"](%q_proj.49)
  %4141 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4132, %weight.437), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn/__module.model.layers.24.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4142 : int[] = prim::ListConstruct(%4138, %4139, %231, %226), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %4143 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%4141, %4142), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.49 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4143, %242, %243), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.439 : Tensor = prim::GetAttr[name="weight"](%k_proj.49)
  %4146 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4132, %weight.439), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn/__module.model.layers.24.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4147 : int[] = prim::ListConstruct(%4138, %4139, %231, %226), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %4148 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4146, %4147), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.49 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4148, %242, %243), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.441 : Tensor = prim::GetAttr[name="weight"](%v_proj.49)
  %4151 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4132, %weight.441), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn/__module.model.layers.24.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4152 : int[] = prim::ListConstruct(%4138, %4139, %231, %226), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %4153 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4151, %4152), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.49 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4153, %242, %243), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.53 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.53 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %4157 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.49, %cos.53), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4158 : int = aten::size(%q.49, %235), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4159 : Long(device=cpu) = prim::NumToTensor(%4158), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %4160 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4159, %225), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4161 : int = aten::Int(%4160), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %x1.97 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.49, %235, %237, %4161, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4163 : int = aten::size(%q.49, %235), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4164 : Long(device=cpu) = prim::NumToTensor(%4163), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %4165 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4164, %225), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4166 : int = aten::Int(%4165), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %x2.97 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.49, %235, %4166, %236, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4168 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.97), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4169 : Tensor[] = prim::ListConstruct(%4168, %x1.97), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %4170 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4169, %231), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4171 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4170, %sin.53), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.49 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4157, %4171, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4173 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.49, %cos.53), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4174 : int = aten::size(%k.49, %235), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4175 : Long(device=cpu) = prim::NumToTensor(%4174), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %4176 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4175, %225), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4177 : int = aten::Int(%4176), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %x1.99 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.49, %235, %237, %4177, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4179 : int = aten::size(%k.49, %235), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4180 : Long(device=cpu) = prim::NumToTensor(%4179), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %4181 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4180, %225), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4182 : int = aten::Int(%4181), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %x2.99 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.49, %235, %4182, %236, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4184 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.99), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4185 : Tensor[] = prim::ListConstruct(%4184, %x1.99), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %4186 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4185, %231), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4187 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4186, %sin.53), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.49 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4173, %4187, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4189 : Tensor[] = prim::ListConstruct(%87, %key_states.49), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %hidden_states.729 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4189, %224), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %4191 : Tensor[] = prim::ListConstruct(%88, %value_states.49), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %hidden_states.733 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4191, %224), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %4193 : int = aten::size(%hidden_states.729, %237), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4194 : int = aten::size(%hidden_states.729, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.97 : Long(device=cpu) = prim::NumToTensor(%4194), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %4196 : int = aten::size(%hidden_states.729, %243), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4197 : int = aten::size(%hidden_states.729, %235), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4198 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.729, %237, %237, %236, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4199 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4198, %242, %237, %236, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4200 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4199, %243), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4201 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4200, %235, %237, %236, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4202 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4201, %234, %237, %236, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4203 : int[] = prim::ListConstruct(%4193, %4194, %234, %4196, %4197), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %hidden_states.731 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4202, %4203, %239), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4205 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.97, %223), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4206 : int = aten::Int(%4205), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %4207 : int[] = prim::ListConstruct(%4193, %4206, %4196, %4197), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %key.49 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.731, %4207), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4209 : int = aten::size(%hidden_states.733, %237), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4210 : int = aten::size(%hidden_states.733, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.99 : Long(device=cpu) = prim::NumToTensor(%4210), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %4212 : int = aten::size(%hidden_states.733, %243), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4213 : int = aten::size(%hidden_states.733, %235), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4214 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.733, %237, %237, %236, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4215 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4214, %242, %237, %236, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4216 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4215, %243), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4217 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4216, %235, %237, %236, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4218 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4217, %234, %237, %236, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4219 : int[] = prim::ListConstruct(%4209, %4210, %234, %4212, %4213), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %hidden_states.735 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4218, %4219, %239), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4221 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.99, %223), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4222 : int = aten::Int(%4221), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %4223 : int[] = prim::ListConstruct(%4209, %4222, %4212, %4213), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %value.49 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.735, %4223), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4225 : int = aten::size(%key.49, %243), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4226 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4227 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4226, %242, %237, %236, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4228 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4227, %243, %237, %236, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.55 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4228, %235, %237, %4225, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.97 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.49, %key.49, %value.49, %attention_mask.55, %222, %239, %221, %239), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %4231 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.97, %242, %243), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.99 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4231, %237), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %4233 : int[] = prim::ListConstruct(%4138, %4139, %231), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %4234 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.99, %4233), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %4235 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%4234, %237), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.443 : Tensor = prim::GetAttr[name="weight"](%o_proj.49)
  %hidden_states.737 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4235, %weight.443), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn/__module.model.layers.24.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4238 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.737, %hidden_states.729, %hidden_states.733)
  %4239 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4240 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4241 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4238)
  %hidden_states.739 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4133, %4239, %242), scope: __module.model/__module.model.layers.24 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.445 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.49)
  %hidden_states.741 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.739, %229, %239, %239, %241), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4245 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.741, %243), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4246 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm
  %variance.99 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4245, %4246, %220, %241), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4248 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.99, %219, %242), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4249 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4248), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.743 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.741, %4249), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.745 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.743, %229, %239, %239, %241), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4252 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.445, %hidden_states.745), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4253 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%4252, %hidden_states.741)
  %4254 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4255 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4253)
  %down_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_313.Linear = prim::GetAttr[name="down_proj"](%mlp.49)
  %up_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_312.Linear = prim::GetAttr[name="up_proj"](%mlp.49)
  %gate_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_311.Linear = prim::GetAttr[name="gate_proj"](%mlp.49)
  %weight.447 : Tensor = prim::GetAttr[name="weight"](%gate_proj.49)
  %input.49 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4254, %weight.447), scope: __module.model/__module.model.layers.24/__module.model.layers.24.mlp/__module.model.layers.24.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4261 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.49), scope: __module.model/__module.model.layers.24/__module.model.layers.24.mlp/__module.model.layers.24.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.449 : Tensor = prim::GetAttr[name="weight"](%up_proj.49)
  %4263 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4254, %weight.449), scope: __module.model/__module.model.layers.24/__module.model.layers.24.mlp/__module.model.layers.24.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4264 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%4261, %4263), scope: __module.model/__module.model.layers.24/__module.model.layers.24.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.451 : Tensor = prim::GetAttr[name="weight"](%down_proj.49)
  %hidden_states.747 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4264, %weight.451), scope: __module.model/__module.model.layers.24/__module.model.layers.24.mlp/__module.model.layers.24.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.749 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4255, %hidden_states.747, %242), scope: __module.model/__module.model.layers.24 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %4268 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.749, %4240, %4241)
  %4269 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4270 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4271 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4268)
  %mlp.51 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_328.MistralMLP = prim::GetAttr[name="mlp"](%_25)
  %post_attention_layernorm.51 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_330.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_25)
  %self_attn.51 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_323.MistralAttention = prim::GetAttr[name="self_attn"](%_25)
  %input_layernorm.51 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_329.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_25)
  %weight.453 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.51)
  %hidden_states.751 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%4269, %229, %239, %239, %241), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4278 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.751, %243), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4279 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm
  %variance.101 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4278, %4279, %220, %241), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4281 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.101, %219, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4282 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4281), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.753 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.751, %4282), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.755 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.753, %229, %239, %239, %241), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.757 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.453, %hidden_states.755), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4286 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.757, %hidden_states.751)
  %4287 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4288 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4286)
  %o_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_322.Linear = prim::GetAttr[name="o_proj"](%self_attn.51)
  %v_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_321.Linear = prim::GetAttr[name="v_proj"](%self_attn.51)
  %k_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_320.Linear = prim::GetAttr[name="k_proj"](%self_attn.51)
  %q_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_319.Linear = prim::GetAttr[name="q_proj"](%self_attn.51)
  %4293 : int = aten::size(%4287, %237), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %4294 : int = aten::size(%4287, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.455 : Tensor = prim::GetAttr[name="weight"](%q_proj.51)
  %4296 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4287, %weight.455), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn/__module.model.layers.25.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4297 : int[] = prim::ListConstruct(%4293, %4294, %231, %226), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %4298 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%4296, %4297), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.51 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4298, %242, %243), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.457 : Tensor = prim::GetAttr[name="weight"](%k_proj.51)
  %4301 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4287, %weight.457), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn/__module.model.layers.25.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4302 : int[] = prim::ListConstruct(%4293, %4294, %231, %226), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %4303 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4301, %4302), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.51 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4303, %242, %243), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.459 : Tensor = prim::GetAttr[name="weight"](%v_proj.51)
  %4306 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4287, %weight.459), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn/__module.model.layers.25.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4307 : int[] = prim::ListConstruct(%4293, %4294, %231, %226), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %4308 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4306, %4307), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.51 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4308, %242, %243), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.55 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.55 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %4312 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.51, %cos.55), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4313 : int = aten::size(%q.51, %235), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4314 : Long(device=cpu) = prim::NumToTensor(%4313), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %4315 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4314, %225), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4316 : int = aten::Int(%4315), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %x1.101 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.51, %235, %237, %4316, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4318 : int = aten::size(%q.51, %235), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4319 : Long(device=cpu) = prim::NumToTensor(%4318), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %4320 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4319, %225), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4321 : int = aten::Int(%4320), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %x2.101 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.51, %235, %4321, %236, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4323 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.101), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4324 : Tensor[] = prim::ListConstruct(%4323, %x1.101), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %4325 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4324, %231), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4326 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4325, %sin.55), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.51 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4312, %4326, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4328 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.51, %cos.55), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4329 : int = aten::size(%k.51, %235), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4330 : Long(device=cpu) = prim::NumToTensor(%4329), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %4331 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4330, %225), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4332 : int = aten::Int(%4331), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %x1.103 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.51, %235, %237, %4332, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4334 : int = aten::size(%k.51, %235), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4335 : Long(device=cpu) = prim::NumToTensor(%4334), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %4336 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4335, %225), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4337 : int = aten::Int(%4336), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %x2.103 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.51, %235, %4337, %236, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4339 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.103), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4340 : Tensor[] = prim::ListConstruct(%4339, %x1.103), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %4341 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4340, %231), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4342 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4341, %sin.55), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.51 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4328, %4342, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4344 : Tensor[] = prim::ListConstruct(%89, %key_states.51), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %hidden_states.759 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4344, %224), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %4346 : Tensor[] = prim::ListConstruct(%90, %value_states.51), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %hidden_states.763 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4346, %224), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %4348 : int = aten::size(%hidden_states.759, %237), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4349 : int = aten::size(%hidden_states.759, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.101 : Long(device=cpu) = prim::NumToTensor(%4349), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %4351 : int = aten::size(%hidden_states.759, %243), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4352 : int = aten::size(%hidden_states.759, %235), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4353 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.759, %237, %237, %236, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4354 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4353, %242, %237, %236, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4355 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4354, %243), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4356 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4355, %235, %237, %236, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4357 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4356, %234, %237, %236, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4358 : int[] = prim::ListConstruct(%4348, %4349, %234, %4351, %4352), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %hidden_states.761 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4357, %4358, %239), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4360 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.101, %223), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4361 : int = aten::Int(%4360), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %4362 : int[] = prim::ListConstruct(%4348, %4361, %4351, %4352), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %key.51 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.761, %4362), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4364 : int = aten::size(%hidden_states.763, %237), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4365 : int = aten::size(%hidden_states.763, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.103 : Long(device=cpu) = prim::NumToTensor(%4365), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %4367 : int = aten::size(%hidden_states.763, %243), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4368 : int = aten::size(%hidden_states.763, %235), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4369 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.763, %237, %237, %236, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4370 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4369, %242, %237, %236, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4371 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4370, %243), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4372 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4371, %235, %237, %236, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4373 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4372, %234, %237, %236, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4374 : int[] = prim::ListConstruct(%4364, %4365, %234, %4367, %4368), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %hidden_states.765 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4373, %4374, %239), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4376 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.103, %223), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4377 : int = aten::Int(%4376), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %4378 : int[] = prim::ListConstruct(%4364, %4377, %4367, %4368), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %value.51 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.765, %4378), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4380 : int = aten::size(%key.51, %243), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4381 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4382 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4381, %242, %237, %236, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4383 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4382, %243, %237, %236, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.57 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4383, %235, %237, %4380, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.101 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.51, %key.51, %value.51, %attention_mask.57, %222, %239, %221, %239), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %4386 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.101, %242, %243), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.103 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4386, %237), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %4388 : int[] = prim::ListConstruct(%4293, %4294, %231), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %4389 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.103, %4388), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %4390 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%4389, %237), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.461 : Tensor = prim::GetAttr[name="weight"](%o_proj.51)
  %hidden_states.767 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4390, %weight.461), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn/__module.model.layers.25.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4393 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.767, %hidden_states.759, %hidden_states.763)
  %4394 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4395 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4396 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4393)
  %hidden_states.769 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4288, %4394, %242), scope: __module.model/__module.model.layers.25 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.463 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.51)
  %hidden_states.771 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.769, %229, %239, %239, %241), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4400 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.771, %243), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4401 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm
  %variance.103 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4400, %4401, %220, %241), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4403 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.103, %219, %242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4404 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4403), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.773 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.771, %4404), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.775 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.773, %229, %239, %239, %241), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4407 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.463, %hidden_states.775), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4408 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%4407, %hidden_states.771)
  %4409 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4410 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4408)
  %down_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_326.Linear = prim::GetAttr[name="down_proj"](%mlp.51)
  %up_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_325.Linear = prim::GetAttr[name="up_proj"](%mlp.51)
  %gate_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_324.Linear = prim::GetAttr[name="gate_proj"](%mlp.51)
  %weight.465 : Tensor = prim::GetAttr[name="weight"](%gate_proj.51)
  %input.51 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4409, %weight.465), scope: __module.model/__module.model.layers.25/__module.model.layers.25.mlp/__module.model.layers.25.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4416 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.51), scope: __module.model/__module.model.layers.25/__module.model.layers.25.mlp/__module.model.layers.25.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.467 : Tensor = prim::GetAttr[name="weight"](%up_proj.51)
  %4418 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4409, %weight.467), scope: __module.model/__module.model.layers.25/__module.model.layers.25.mlp/__module.model.layers.25.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4419 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%4416, %4418), scope: __module.model/__module.model.layers.25/__module.model.layers.25.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.469 : Tensor = prim::GetAttr[name="weight"](%down_proj.51)
  %hidden_states.777 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4419, %weight.469), scope: __module.model/__module.model.layers.25/__module.model.layers.25.mlp/__module.model.layers.25.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.779 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4410, %hidden_states.777, %242), scope: __module.model/__module.model.layers.25 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %4423 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.779, %4395, %4396)
  %4424 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4425 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4426 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4423)
  %mlp.53 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_341.MistralMLP = prim::GetAttr[name="mlp"](%_26)
  %post_attention_layernorm.53 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_343.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_26)
  %self_attn.53 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_336.MistralAttention = prim::GetAttr[name="self_attn"](%_26)
  %input_layernorm.53 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_342.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_26)
  %weight.471 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.53)
  %hidden_states.781 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%4424, %229, %239, %239, %241), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4433 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.781, %243), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4434 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm
  %variance.105 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4433, %4434, %220, %241), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4436 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.105, %219, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4437 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4436), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.783 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.781, %4437), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.785 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.783, %229, %239, %239, %241), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.787 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.471, %hidden_states.785), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4441 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.787, %hidden_states.781)
  %4442 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4443 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4441)
  %o_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_335.Linear = prim::GetAttr[name="o_proj"](%self_attn.53)
  %v_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_334.Linear = prim::GetAttr[name="v_proj"](%self_attn.53)
  %k_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_333.Linear = prim::GetAttr[name="k_proj"](%self_attn.53)
  %q_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_332.Linear = prim::GetAttr[name="q_proj"](%self_attn.53)
  %4448 : int = aten::size(%4442, %237), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %4449 : int = aten::size(%4442, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.473 : Tensor = prim::GetAttr[name="weight"](%q_proj.53)
  %4451 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4442, %weight.473), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn/__module.model.layers.26.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4452 : int[] = prim::ListConstruct(%4448, %4449, %231, %226), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4453 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%4451, %4452), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.53 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4453, %242, %243), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.475 : Tensor = prim::GetAttr[name="weight"](%k_proj.53)
  %4456 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4442, %weight.475), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn/__module.model.layers.26.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4457 : int[] = prim::ListConstruct(%4448, %4449, %231, %226), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4458 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4456, %4457), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.53 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4458, %242, %243), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.477 : Tensor = prim::GetAttr[name="weight"](%v_proj.53)
  %4461 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4442, %weight.477), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn/__module.model.layers.26.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4462 : int[] = prim::ListConstruct(%4448, %4449, %231, %226), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4463 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4461, %4462), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.53 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4463, %242, %243), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.57 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.57 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %4467 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.53, %cos.57), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4468 : int = aten::size(%q.53, %235), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4469 : Long(device=cpu) = prim::NumToTensor(%4468), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4470 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4469, %225), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4471 : int = aten::Int(%4470), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %x1.105 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.53, %235, %237, %4471, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4473 : int = aten::size(%q.53, %235), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4474 : Long(device=cpu) = prim::NumToTensor(%4473), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4475 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4474, %225), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4476 : int = aten::Int(%4475), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %x2.105 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.53, %235, %4476, %236, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4478 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.105), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4479 : Tensor[] = prim::ListConstruct(%4478, %x1.105), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4480 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4479, %231), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4481 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4480, %sin.57), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.53 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4467, %4481, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4483 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.53, %cos.57), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4484 : int = aten::size(%k.53, %235), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4485 : Long(device=cpu) = prim::NumToTensor(%4484), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4486 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4485, %225), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4487 : int = aten::Int(%4486), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %x1.107 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.53, %235, %237, %4487, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4489 : int = aten::size(%k.53, %235), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4490 : Long(device=cpu) = prim::NumToTensor(%4489), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4491 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4490, %225), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4492 : int = aten::Int(%4491), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %x2.107 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.53, %235, %4492, %236, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4494 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.107), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4495 : Tensor[] = prim::ListConstruct(%4494, %x1.107), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4496 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4495, %231), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4497 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4496, %sin.57), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.53 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4483, %4497, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4499 : Tensor[] = prim::ListConstruct(%91, %key_states.53), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %hidden_states.789 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4499, %224), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %4501 : Tensor[] = prim::ListConstruct(%92, %value_states.53), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %hidden_states.793 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4501, %224), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %4503 : int = aten::size(%hidden_states.789, %237), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4504 : int = aten::size(%hidden_states.789, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.105 : Long(device=cpu) = prim::NumToTensor(%4504), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4506 : int = aten::size(%hidden_states.789, %243), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4507 : int = aten::size(%hidden_states.789, %235), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4508 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.789, %237, %237, %236, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4509 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4508, %242, %237, %236, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4510 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4509, %243), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4511 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4510, %235, %237, %236, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4512 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4511, %234, %237, %236, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4513 : int[] = prim::ListConstruct(%4503, %4504, %234, %4506, %4507), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %hidden_states.791 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4512, %4513, %239), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4515 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.105, %223), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4516 : int = aten::Int(%4515), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4517 : int[] = prim::ListConstruct(%4503, %4516, %4506, %4507), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %key.53 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.791, %4517), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4519 : int = aten::size(%hidden_states.793, %237), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4520 : int = aten::size(%hidden_states.793, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.107 : Long(device=cpu) = prim::NumToTensor(%4520), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4522 : int = aten::size(%hidden_states.793, %243), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4523 : int = aten::size(%hidden_states.793, %235), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4524 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.793, %237, %237, %236, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4525 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4524, %242, %237, %236, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4526 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4525, %243), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4527 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4526, %235, %237, %236, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4528 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4527, %234, %237, %236, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4529 : int[] = prim::ListConstruct(%4519, %4520, %234, %4522, %4523), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %hidden_states.795 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4528, %4529, %239), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4531 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.107, %223), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4532 : int = aten::Int(%4531), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4533 : int[] = prim::ListConstruct(%4519, %4532, %4522, %4523), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %value.53 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.795, %4533), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4535 : int = aten::size(%key.53, %243), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4536 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4537 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4536, %242, %237, %236, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4538 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4537, %243, %237, %236, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.59 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4538, %235, %237, %4535, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.105 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.53, %key.53, %value.53, %attention_mask.59, %222, %239, %221, %239), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %4541 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.105, %242, %243), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.107 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4541, %237), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %4543 : int[] = prim::ListConstruct(%4448, %4449, %231), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4544 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.107, %4543), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %4545 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%4544, %237), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.479 : Tensor = prim::GetAttr[name="weight"](%o_proj.53)
  %hidden_states.797 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4545, %weight.479), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn/__module.model.layers.26.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4548 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.797, %hidden_states.789, %hidden_states.793)
  %4549 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4550 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4551 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4548)
  %hidden_states.799 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4443, %4549, %242), scope: __module.model/__module.model.layers.26 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.481 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.53)
  %hidden_states.801 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.799, %229, %239, %239, %241), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4555 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.801, %243), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4556 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm
  %variance.107 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4555, %4556, %220, %241), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4558 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.107, %219, %242), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4559 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4558), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.803 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.801, %4559), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.805 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.803, %229, %239, %239, %241), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4562 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.481, %hidden_states.805), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4563 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%4562, %hidden_states.801)
  %4564 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4565 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4563)
  %down_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_339.Linear = prim::GetAttr[name="down_proj"](%mlp.53)
  %up_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_338.Linear = prim::GetAttr[name="up_proj"](%mlp.53)
  %gate_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_337.Linear = prim::GetAttr[name="gate_proj"](%mlp.53)
  %weight.483 : Tensor = prim::GetAttr[name="weight"](%gate_proj.53)
  %input.53 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4564, %weight.483), scope: __module.model/__module.model.layers.26/__module.model.layers.26.mlp/__module.model.layers.26.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4571 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.53), scope: __module.model/__module.model.layers.26/__module.model.layers.26.mlp/__module.model.layers.26.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.485 : Tensor = prim::GetAttr[name="weight"](%up_proj.53)
  %4573 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4564, %weight.485), scope: __module.model/__module.model.layers.26/__module.model.layers.26.mlp/__module.model.layers.26.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4574 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%4571, %4573), scope: __module.model/__module.model.layers.26/__module.model.layers.26.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.487 : Tensor = prim::GetAttr[name="weight"](%down_proj.53)
  %hidden_states.807 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4574, %weight.487), scope: __module.model/__module.model.layers.26/__module.model.layers.26.mlp/__module.model.layers.26.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.809 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4565, %hidden_states.807, %242), scope: __module.model/__module.model.layers.26 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %4578 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.809, %4550, %4551)
  %4579 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4580 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4581 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4578)
  %mlp.55 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_354.MistralMLP = prim::GetAttr[name="mlp"](%_27)
  %post_attention_layernorm.55 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_356.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_27)
  %self_attn.55 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_349.MistralAttention = prim::GetAttr[name="self_attn"](%_27)
  %input_layernorm.55 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_355.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_27)
  %weight.489 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.55)
  %hidden_states.811 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%4579, %229, %239, %239, %241), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4588 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.811, %243), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4589 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm
  %variance.109 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4588, %4589, %220, %241), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4591 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.109, %219, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4592 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4591), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.813 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.811, %4592), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.815 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.813, %229, %239, %239, %241), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.817 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.489, %hidden_states.815), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4596 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.817, %hidden_states.811)
  %4597 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4598 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4596)
  %o_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_348.Linear = prim::GetAttr[name="o_proj"](%self_attn.55)
  %v_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_347.Linear = prim::GetAttr[name="v_proj"](%self_attn.55)
  %k_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_346.Linear = prim::GetAttr[name="k_proj"](%self_attn.55)
  %q_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_345.Linear = prim::GetAttr[name="q_proj"](%self_attn.55)
  %4603 : int = aten::size(%4597, %237), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %4604 : int = aten::size(%4597, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.491 : Tensor = prim::GetAttr[name="weight"](%q_proj.55)
  %4606 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4597, %weight.491), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn/__module.model.layers.27.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4607 : int[] = prim::ListConstruct(%4603, %4604, %231, %226), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4608 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%4606, %4607), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.55 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4608, %242, %243), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.493 : Tensor = prim::GetAttr[name="weight"](%k_proj.55)
  %4611 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4597, %weight.493), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn/__module.model.layers.27.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4612 : int[] = prim::ListConstruct(%4603, %4604, %231, %226), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4613 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4611, %4612), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.55 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4613, %242, %243), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.495 : Tensor = prim::GetAttr[name="weight"](%v_proj.55)
  %4616 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4597, %weight.495), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn/__module.model.layers.27.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4617 : int[] = prim::ListConstruct(%4603, %4604, %231, %226), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4618 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4616, %4617), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.55 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4618, %242, %243), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.59 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.59 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %4622 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.55, %cos.59), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4623 : int = aten::size(%q.55, %235), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4624 : Long(device=cpu) = prim::NumToTensor(%4623), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4625 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4624, %225), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4626 : int = aten::Int(%4625), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %x1.109 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.55, %235, %237, %4626, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4628 : int = aten::size(%q.55, %235), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4629 : Long(device=cpu) = prim::NumToTensor(%4628), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4630 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4629, %225), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4631 : int = aten::Int(%4630), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %x2.109 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.55, %235, %4631, %236, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4633 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.109), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4634 : Tensor[] = prim::ListConstruct(%4633, %x1.109), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4635 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4634, %231), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4636 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4635, %sin.59), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.55 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4622, %4636, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4638 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.55, %cos.59), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4639 : int = aten::size(%k.55, %235), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4640 : Long(device=cpu) = prim::NumToTensor(%4639), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4641 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4640, %225), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4642 : int = aten::Int(%4641), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %x1.111 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.55, %235, %237, %4642, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4644 : int = aten::size(%k.55, %235), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4645 : Long(device=cpu) = prim::NumToTensor(%4644), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4646 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4645, %225), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4647 : int = aten::Int(%4646), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %x2.111 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.55, %235, %4647, %236, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4649 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.111), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4650 : Tensor[] = prim::ListConstruct(%4649, %x1.111), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4651 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4650, %231), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4652 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4651, %sin.59), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.55 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4638, %4652, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4654 : Tensor[] = prim::ListConstruct(%93, %key_states.55), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %hidden_states.819 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4654, %224), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %4656 : Tensor[] = prim::ListConstruct(%94, %value_states.55), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %hidden_states.823 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4656, %224), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %4658 : int = aten::size(%hidden_states.819, %237), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4659 : int = aten::size(%hidden_states.819, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.109 : Long(device=cpu) = prim::NumToTensor(%4659), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4661 : int = aten::size(%hidden_states.819, %243), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4662 : int = aten::size(%hidden_states.819, %235), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4663 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.819, %237, %237, %236, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4664 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4663, %242, %237, %236, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4665 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4664, %243), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4666 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4665, %235, %237, %236, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4667 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4666, %234, %237, %236, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4668 : int[] = prim::ListConstruct(%4658, %4659, %234, %4661, %4662), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %hidden_states.821 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4667, %4668, %239), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4670 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.109, %223), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4671 : int = aten::Int(%4670), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4672 : int[] = prim::ListConstruct(%4658, %4671, %4661, %4662), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %key.55 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.821, %4672), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4674 : int = aten::size(%hidden_states.823, %237), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4675 : int = aten::size(%hidden_states.823, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.111 : Long(device=cpu) = prim::NumToTensor(%4675), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4677 : int = aten::size(%hidden_states.823, %243), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4678 : int = aten::size(%hidden_states.823, %235), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4679 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.823, %237, %237, %236, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4680 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4679, %242, %237, %236, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4681 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4680, %243), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4682 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4681, %235, %237, %236, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4683 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4682, %234, %237, %236, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4684 : int[] = prim::ListConstruct(%4674, %4675, %234, %4677, %4678), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %hidden_states.825 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4683, %4684, %239), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4686 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.111, %223), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4687 : int = aten::Int(%4686), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4688 : int[] = prim::ListConstruct(%4674, %4687, %4677, %4678), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %value.55 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.825, %4688), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4690 : int = aten::size(%key.55, %243), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4691 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4692 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4691, %242, %237, %236, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4693 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4692, %243, %237, %236, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.61 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4693, %235, %237, %4690, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.109 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.55, %key.55, %value.55, %attention_mask.61, %222, %239, %221, %239), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %4696 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.109, %242, %243), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.111 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4696, %237), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %4698 : int[] = prim::ListConstruct(%4603, %4604, %231), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4699 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.111, %4698), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %4700 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%4699, %237), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.497 : Tensor = prim::GetAttr[name="weight"](%o_proj.55)
  %hidden_states.827 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4700, %weight.497), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn/__module.model.layers.27.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4703 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.827, %hidden_states.819, %hidden_states.823)
  %4704 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4705 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4706 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4703)
  %hidden_states.829 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4598, %4704, %242), scope: __module.model/__module.model.layers.27 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.499 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.55)
  %hidden_states.831 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.829, %229, %239, %239, %241), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4710 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.831, %243), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4711 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm
  %variance.111 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4710, %4711, %220, %241), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4713 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.111, %219, %242), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4714 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4713), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.833 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.831, %4714), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.835 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.833, %229, %239, %239, %241), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4717 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.499, %hidden_states.835), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4718 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%4717, %hidden_states.831)
  %4719 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4720 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4718)
  %down_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_352.Linear = prim::GetAttr[name="down_proj"](%mlp.55)
  %up_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_351.Linear = prim::GetAttr[name="up_proj"](%mlp.55)
  %gate_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_350.Linear = prim::GetAttr[name="gate_proj"](%mlp.55)
  %weight.501 : Tensor = prim::GetAttr[name="weight"](%gate_proj.55)
  %input.55 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4719, %weight.501), scope: __module.model/__module.model.layers.27/__module.model.layers.27.mlp/__module.model.layers.27.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4726 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.55), scope: __module.model/__module.model.layers.27/__module.model.layers.27.mlp/__module.model.layers.27.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.503 : Tensor = prim::GetAttr[name="weight"](%up_proj.55)
  %4728 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4719, %weight.503), scope: __module.model/__module.model.layers.27/__module.model.layers.27.mlp/__module.model.layers.27.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4729 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%4726, %4728), scope: __module.model/__module.model.layers.27/__module.model.layers.27.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.505 : Tensor = prim::GetAttr[name="weight"](%down_proj.55)
  %hidden_states.837 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4729, %weight.505), scope: __module.model/__module.model.layers.27/__module.model.layers.27.mlp/__module.model.layers.27.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.839 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4720, %hidden_states.837, %242), scope: __module.model/__module.model.layers.27 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %4733 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.839, %4705, %4706)
  %4734 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4735 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4736 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4733)
  %mlp.57 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_367.MistralMLP = prim::GetAttr[name="mlp"](%_28)
  %post_attention_layernorm.57 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_369.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_28)
  %self_attn.57 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_362.MistralAttention = prim::GetAttr[name="self_attn"](%_28)
  %input_layernorm.57 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_368.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_28)
  %weight.507 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.57)
  %hidden_states.841 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%4734, %229, %239, %239, %241), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4743 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.841, %243), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4744 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm
  %variance.113 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4743, %4744, %220, %241), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4746 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.113, %219, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4747 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4746), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.843 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.841, %4747), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.845 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.843, %229, %239, %239, %241), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.847 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.507, %hidden_states.845), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4751 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.847, %hidden_states.841)
  %4752 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4753 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4751)
  %o_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_361.Linear = prim::GetAttr[name="o_proj"](%self_attn.57)
  %v_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_360.Linear = prim::GetAttr[name="v_proj"](%self_attn.57)
  %k_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_359.Linear = prim::GetAttr[name="k_proj"](%self_attn.57)
  %q_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_358.Linear = prim::GetAttr[name="q_proj"](%self_attn.57)
  %4758 : int = aten::size(%4752, %237), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %4759 : int = aten::size(%4752, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.509 : Tensor = prim::GetAttr[name="weight"](%q_proj.57)
  %4761 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4752, %weight.509), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn/__module.model.layers.28.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4762 : int[] = prim::ListConstruct(%4758, %4759, %231, %226), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4763 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%4761, %4762), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.57 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4763, %242, %243), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.511 : Tensor = prim::GetAttr[name="weight"](%k_proj.57)
  %4766 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4752, %weight.511), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn/__module.model.layers.28.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4767 : int[] = prim::ListConstruct(%4758, %4759, %231, %226), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4768 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4766, %4767), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.57 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4768, %242, %243), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.513 : Tensor = prim::GetAttr[name="weight"](%v_proj.57)
  %4771 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4752, %weight.513), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn/__module.model.layers.28.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4772 : int[] = prim::ListConstruct(%4758, %4759, %231, %226), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4773 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4771, %4772), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.57 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4773, %242, %243), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.61 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.61 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %4777 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.57, %cos.61), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4778 : int = aten::size(%q.57, %235), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4779 : Long(device=cpu) = prim::NumToTensor(%4778), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4780 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4779, %225), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4781 : int = aten::Int(%4780), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %x1.113 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.57, %235, %237, %4781, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4783 : int = aten::size(%q.57, %235), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4784 : Long(device=cpu) = prim::NumToTensor(%4783), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4785 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4784, %225), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4786 : int = aten::Int(%4785), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %x2.113 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.57, %235, %4786, %236, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4788 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.113), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4789 : Tensor[] = prim::ListConstruct(%4788, %x1.113), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4790 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4789, %231), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4791 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4790, %sin.61), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.57 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4777, %4791, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4793 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.57, %cos.61), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4794 : int = aten::size(%k.57, %235), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4795 : Long(device=cpu) = prim::NumToTensor(%4794), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4796 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4795, %225), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4797 : int = aten::Int(%4796), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %x1.115 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.57, %235, %237, %4797, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4799 : int = aten::size(%k.57, %235), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4800 : Long(device=cpu) = prim::NumToTensor(%4799), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4801 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4800, %225), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4802 : int = aten::Int(%4801), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %x2.115 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.57, %235, %4802, %236, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4804 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.115), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4805 : Tensor[] = prim::ListConstruct(%4804, %x1.115), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4806 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4805, %231), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4807 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4806, %sin.61), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.57 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4793, %4807, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4809 : Tensor[] = prim::ListConstruct(%95, %key_states.57), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %hidden_states.849 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4809, %224), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %4811 : Tensor[] = prim::ListConstruct(%96, %value_states.57), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %hidden_states.853 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4811, %224), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %4813 : int = aten::size(%hidden_states.849, %237), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4814 : int = aten::size(%hidden_states.849, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.113 : Long(device=cpu) = prim::NumToTensor(%4814), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4816 : int = aten::size(%hidden_states.849, %243), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4817 : int = aten::size(%hidden_states.849, %235), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4818 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.849, %237, %237, %236, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4819 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4818, %242, %237, %236, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4820 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4819, %243), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4821 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4820, %235, %237, %236, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4822 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4821, %234, %237, %236, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4823 : int[] = prim::ListConstruct(%4813, %4814, %234, %4816, %4817), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %hidden_states.851 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4822, %4823, %239), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4825 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.113, %223), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4826 : int = aten::Int(%4825), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4827 : int[] = prim::ListConstruct(%4813, %4826, %4816, %4817), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %key.57 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.851, %4827), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4829 : int = aten::size(%hidden_states.853, %237), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4830 : int = aten::size(%hidden_states.853, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.115 : Long(device=cpu) = prim::NumToTensor(%4830), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4832 : int = aten::size(%hidden_states.853, %243), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4833 : int = aten::size(%hidden_states.853, %235), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4834 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.853, %237, %237, %236, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4835 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4834, %242, %237, %236, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4836 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4835, %243), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4837 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4836, %235, %237, %236, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4838 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4837, %234, %237, %236, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4839 : int[] = prim::ListConstruct(%4829, %4830, %234, %4832, %4833), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %hidden_states.855 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4838, %4839, %239), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4841 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.115, %223), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4842 : int = aten::Int(%4841), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4843 : int[] = prim::ListConstruct(%4829, %4842, %4832, %4833), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %value.57 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.855, %4843), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4845 : int = aten::size(%key.57, %243), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4846 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4847 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4846, %242, %237, %236, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4848 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4847, %243, %237, %236, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.63 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4848, %235, %237, %4845, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.113 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.57, %key.57, %value.57, %attention_mask.63, %222, %239, %221, %239), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %4851 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.113, %242, %243), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.115 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4851, %237), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %4853 : int[] = prim::ListConstruct(%4758, %4759, %231), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4854 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.115, %4853), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %4855 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%4854, %237), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.515 : Tensor = prim::GetAttr[name="weight"](%o_proj.57)
  %hidden_states.857 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4855, %weight.515), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn/__module.model.layers.28.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4858 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.857, %hidden_states.849, %hidden_states.853)
  %4859 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4860 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4861 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4858)
  %hidden_states.859 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4753, %4859, %242), scope: __module.model/__module.model.layers.28 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.517 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.57)
  %hidden_states.861 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.859, %229, %239, %239, %241), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4865 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.861, %243), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4866 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm
  %variance.115 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4865, %4866, %220, %241), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4868 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.115, %219, %242), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4869 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4868), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.863 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.861, %4869), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.865 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.863, %229, %239, %239, %241), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4872 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.517, %hidden_states.865), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4873 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%4872, %hidden_states.861)
  %4874 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4875 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4873)
  %down_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_365.Linear = prim::GetAttr[name="down_proj"](%mlp.57)
  %up_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_364.Linear = prim::GetAttr[name="up_proj"](%mlp.57)
  %gate_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_363.Linear = prim::GetAttr[name="gate_proj"](%mlp.57)
  %weight.519 : Tensor = prim::GetAttr[name="weight"](%gate_proj.57)
  %input.57 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4874, %weight.519), scope: __module.model/__module.model.layers.28/__module.model.layers.28.mlp/__module.model.layers.28.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4881 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.57), scope: __module.model/__module.model.layers.28/__module.model.layers.28.mlp/__module.model.layers.28.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.521 : Tensor = prim::GetAttr[name="weight"](%up_proj.57)
  %4883 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4874, %weight.521), scope: __module.model/__module.model.layers.28/__module.model.layers.28.mlp/__module.model.layers.28.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4884 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%4881, %4883), scope: __module.model/__module.model.layers.28/__module.model.layers.28.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.523 : Tensor = prim::GetAttr[name="weight"](%down_proj.57)
  %hidden_states.867 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4884, %weight.523), scope: __module.model/__module.model.layers.28/__module.model.layers.28.mlp/__module.model.layers.28.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.869 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4875, %hidden_states.867, %242), scope: __module.model/__module.model.layers.28 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %4888 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.869, %4860, %4861)
  %4889 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4890 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4891 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4888)
  %mlp.59 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_380.MistralMLP = prim::GetAttr[name="mlp"](%_29)
  %post_attention_layernorm.59 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_382.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_29)
  %self_attn.59 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_375.MistralAttention = prim::GetAttr[name="self_attn"](%_29)
  %input_layernorm.59 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_381.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_29)
  %weight.525 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.59)
  %hidden_states.871 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%4889, %229, %239, %239, %241), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4898 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.871, %243), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4899 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm
  %variance.117 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4898, %4899, %220, %241), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4901 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.117, %219, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4902 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4901), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.873 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.871, %4902), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.875 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.873, %229, %239, %239, %241), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.877 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.525, %hidden_states.875), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4906 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.877, %hidden_states.871)
  %4907 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4908 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4906)
  %o_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_374.Linear = prim::GetAttr[name="o_proj"](%self_attn.59)
  %v_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_373.Linear = prim::GetAttr[name="v_proj"](%self_attn.59)
  %k_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_372.Linear = prim::GetAttr[name="k_proj"](%self_attn.59)
  %q_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_371.Linear = prim::GetAttr[name="q_proj"](%self_attn.59)
  %4913 : int = aten::size(%4907, %237), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %4914 : int = aten::size(%4907, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.527 : Tensor = prim::GetAttr[name="weight"](%q_proj.59)
  %4916 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4907, %weight.527), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn/__module.model.layers.29.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4917 : int[] = prim::ListConstruct(%4913, %4914, %231, %226), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4918 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%4916, %4917), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.59 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4918, %242, %243), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.529 : Tensor = prim::GetAttr[name="weight"](%k_proj.59)
  %4921 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4907, %weight.529), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn/__module.model.layers.29.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4922 : int[] = prim::ListConstruct(%4913, %4914, %231, %226), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4923 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4921, %4922), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.59 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4923, %242, %243), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.531 : Tensor = prim::GetAttr[name="weight"](%v_proj.59)
  %4926 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4907, %weight.531), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn/__module.model.layers.29.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4927 : int[] = prim::ListConstruct(%4913, %4914, %231, %226), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4928 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4926, %4927), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.59 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4928, %242, %243), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.63 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.63 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %4932 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.59, %cos.63), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4933 : int = aten::size(%q.59, %235), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4934 : Long(device=cpu) = prim::NumToTensor(%4933), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4935 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4934, %225), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4936 : int = aten::Int(%4935), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %x1.117 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.59, %235, %237, %4936, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4938 : int = aten::size(%q.59, %235), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4939 : Long(device=cpu) = prim::NumToTensor(%4938), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4940 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4939, %225), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4941 : int = aten::Int(%4940), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %x2.117 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.59, %235, %4941, %236, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4943 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.117), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4944 : Tensor[] = prim::ListConstruct(%4943, %x1.117), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4945 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4944, %231), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4946 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4945, %sin.63), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.59 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4932, %4946, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4948 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.59, %cos.63), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4949 : int = aten::size(%k.59, %235), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4950 : Long(device=cpu) = prim::NumToTensor(%4949), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4951 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4950, %225), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4952 : int = aten::Int(%4951), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %x1.119 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.59, %235, %237, %4952, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4954 : int = aten::size(%k.59, %235), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4955 : Long(device=cpu) = prim::NumToTensor(%4954), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4956 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4955, %225), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4957 : int = aten::Int(%4956), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %x2.119 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.59, %235, %4957, %236, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4959 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.119), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4960 : Tensor[] = prim::ListConstruct(%4959, %x1.119), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4961 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4960, %231), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4962 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4961, %sin.63), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.59 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4948, %4962, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4964 : Tensor[] = prim::ListConstruct(%97, %key_states.59), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %hidden_states.879 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4964, %224), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %4966 : Tensor[] = prim::ListConstruct(%98, %value_states.59), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %hidden_states.883 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4966, %224), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %4968 : int = aten::size(%hidden_states.879, %237), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4969 : int = aten::size(%hidden_states.879, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.117 : Long(device=cpu) = prim::NumToTensor(%4969), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4971 : int = aten::size(%hidden_states.879, %243), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4972 : int = aten::size(%hidden_states.879, %235), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4973 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.879, %237, %237, %236, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4974 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4973, %242, %237, %236, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4975 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4974, %243), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4976 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4975, %235, %237, %236, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4977 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4976, %234, %237, %236, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4978 : int[] = prim::ListConstruct(%4968, %4969, %234, %4971, %4972), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %hidden_states.881 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4977, %4978, %239), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4980 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.117, %223), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4981 : int = aten::Int(%4980), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4982 : int[] = prim::ListConstruct(%4968, %4981, %4971, %4972), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %key.59 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.881, %4982), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4984 : int = aten::size(%hidden_states.883, %237), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4985 : int = aten::size(%hidden_states.883, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.119 : Long(device=cpu) = prim::NumToTensor(%4985), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4987 : int = aten::size(%hidden_states.883, %243), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4988 : int = aten::size(%hidden_states.883, %235), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4989 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.883, %237, %237, %236, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4990 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4989, %242, %237, %236, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4991 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4990, %243), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4992 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4991, %235, %237, %236, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4993 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4992, %234, %237, %236, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4994 : int[] = prim::ListConstruct(%4984, %4985, %234, %4987, %4988), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %hidden_states.885 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4993, %4994, %239), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4996 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.119, %223), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4997 : int = aten::Int(%4996), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4998 : int[] = prim::ListConstruct(%4984, %4997, %4987, %4988), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %value.59 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.885, %4998), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %5000 : int = aten::size(%key.59, %243), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %5001 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %5002 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%5001, %242, %237, %236, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %5003 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%5002, %243, %237, %236, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.65 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%5003, %235, %237, %5000, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.117 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.59, %key.59, %value.59, %attention_mask.65, %222, %239, %221, %239), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %5006 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.117, %242, %243), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.119 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%5006, %237), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %5008 : int[] = prim::ListConstruct(%4913, %4914, %231), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %5009 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.119, %5008), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %5010 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%5009, %237), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.533 : Tensor = prim::GetAttr[name="weight"](%o_proj.59)
  %hidden_states.887 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%5010, %weight.533), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn/__module.model.layers.29.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %5013 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.887, %hidden_states.879, %hidden_states.883)
  %5014 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5015 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %5016 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5013)
  %hidden_states.889 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4908, %5014, %242), scope: __module.model/__module.model.layers.29 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.535 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.59)
  %hidden_states.891 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.889, %229, %239, %239, %241), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %5020 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.891, %243), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %5021 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm
  %variance.119 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%5020, %5021, %220, %241), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %5023 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.119, %219, %242), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %5024 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%5023), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.893 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.891, %5024), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.895 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.893, %229, %239, %239, %241), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %5027 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.535, %hidden_states.895), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %5028 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%5027, %hidden_states.891)
  %5029 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5030 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5028)
  %down_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_378.Linear = prim::GetAttr[name="down_proj"](%mlp.59)
  %up_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_377.Linear = prim::GetAttr[name="up_proj"](%mlp.59)
  %gate_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_376.Linear = prim::GetAttr[name="gate_proj"](%mlp.59)
  %weight.537 : Tensor = prim::GetAttr[name="weight"](%gate_proj.59)
  %input.59 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%5029, %weight.537), scope: __module.model/__module.model.layers.29/__module.model.layers.29.mlp/__module.model.layers.29.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %5036 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.59), scope: __module.model/__module.model.layers.29/__module.model.layers.29.mlp/__module.model.layers.29.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.539 : Tensor = prim::GetAttr[name="weight"](%up_proj.59)
  %5038 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%5029, %weight.539), scope: __module.model/__module.model.layers.29/__module.model.layers.29.mlp/__module.model.layers.29.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %5039 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%5036, %5038), scope: __module.model/__module.model.layers.29/__module.model.layers.29.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.541 : Tensor = prim::GetAttr[name="weight"](%down_proj.59)
  %hidden_states.897 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%5039, %weight.541), scope: __module.model/__module.model.layers.29/__module.model.layers.29.mlp/__module.model.layers.29.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.899 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5030, %hidden_states.897, %242), scope: __module.model/__module.model.layers.29 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %5043 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.899, %5015, %5016)
  %5044 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5045 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %5046 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5043)
  %mlp.61 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_393.MistralMLP = prim::GetAttr[name="mlp"](%_30)
  %post_attention_layernorm.61 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_395.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_30)
  %self_attn.61 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_388.MistralAttention = prim::GetAttr[name="self_attn"](%_30)
  %input_layernorm.61 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_394.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_30)
  %weight.543 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.61)
  %hidden_states.901 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%5044, %229, %239, %239, %241), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %5053 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.901, %243), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %5054 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm
  %variance.121 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%5053, %5054, %220, %241), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %5056 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.121, %219, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %5057 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%5056), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.903 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.901, %5057), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.905 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.903, %229, %239, %239, %241), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.907 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.543, %hidden_states.905), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %5061 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.907, %hidden_states.901)
  %5062 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5063 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5061)
  %o_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_387.Linear = prim::GetAttr[name="o_proj"](%self_attn.61)
  %v_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_386.Linear = prim::GetAttr[name="v_proj"](%self_attn.61)
  %k_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_385.Linear = prim::GetAttr[name="k_proj"](%self_attn.61)
  %q_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_384.Linear = prim::GetAttr[name="q_proj"](%self_attn.61)
  %5068 : int = aten::size(%5062, %237), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %5069 : int = aten::size(%5062, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.545 : Tensor = prim::GetAttr[name="weight"](%q_proj.61)
  %5071 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%5062, %weight.545), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn/__module.model.layers.30.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %5072 : int[] = prim::ListConstruct(%5068, %5069, %231, %226), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %5073 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%5071, %5072), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.61 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%5073, %242, %243), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.547 : Tensor = prim::GetAttr[name="weight"](%k_proj.61)
  %5076 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%5062, %weight.547), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn/__module.model.layers.30.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %5077 : int[] = prim::ListConstruct(%5068, %5069, %231, %226), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %5078 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%5076, %5077), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.61 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%5078, %242, %243), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.549 : Tensor = prim::GetAttr[name="weight"](%v_proj.61)
  %5081 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%5062, %weight.549), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn/__module.model.layers.30.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %5082 : int[] = prim::ListConstruct(%5068, %5069, %231, %226), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %5083 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%5081, %5082), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states.61 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%5083, %242, %243), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.65 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.65 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %5087 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.61, %cos.65), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %5088 : int = aten::size(%q.61, %235), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %5089 : Long(device=cpu) = prim::NumToTensor(%5088), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %5090 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5089, %225), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %5091 : int = aten::Int(%5090), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %x1.121 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.61, %235, %237, %5091, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %5093 : int = aten::size(%q.61, %235), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %5094 : Long(device=cpu) = prim::NumToTensor(%5093), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %5095 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5094, %225), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %5096 : int = aten::Int(%5095), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %x2.121 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.61, %235, %5096, %236, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %5098 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.121), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %5099 : Tensor[] = prim::ListConstruct(%5098, %x1.121), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %5100 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5099, %231), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %5101 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%5100, %sin.65), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.61 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5087, %5101, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %5103 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.61, %cos.65), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %5104 : int = aten::size(%k.61, %235), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %5105 : Long(device=cpu) = prim::NumToTensor(%5104), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %5106 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5105, %225), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %5107 : int = aten::Int(%5106), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %x1.123 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.61, %235, %237, %5107, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %5109 : int = aten::size(%k.61, %235), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %5110 : Long(device=cpu) = prim::NumToTensor(%5109), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %5111 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5110, %225), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %5112 : int = aten::Int(%5111), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %x2.123 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.61, %235, %5112, %236, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %5114 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.123), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %5115 : Tensor[] = prim::ListConstruct(%5114, %x1.123), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %5116 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5115, %231), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %5117 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%5116, %sin.65), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states.61 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%5103, %5117, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %5119 : Tensor[] = prim::ListConstruct(%99, %key_states.61), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %hidden_states.909 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5119, %224), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %5121 : Tensor[] = prim::ListConstruct(%100, %value_states.61), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %hidden_states.913 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5121, %224), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %5123 : int = aten::size(%hidden_states.909, %237), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5124 : int = aten::size(%hidden_states.909, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.121 : Long(device=cpu) = prim::NumToTensor(%5124), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %5126 : int = aten::size(%hidden_states.909, %243), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5127 : int = aten::size(%hidden_states.909, %235), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5128 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.909, %237, %237, %236, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5129 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5128, %242, %237, %236, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5130 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%5129, %243), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5131 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5130, %235, %237, %236, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5132 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5131, %234, %237, %236, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5133 : int[] = prim::ListConstruct(%5123, %5124, %234, %5126, %5127), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %hidden_states.911 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%5132, %5133, %239), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5135 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.121, %223), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %5136 : int = aten::Int(%5135), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %5137 : int[] = prim::ListConstruct(%5123, %5136, %5126, %5127), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %key.61 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.911, %5137), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %5139 : int = aten::size(%hidden_states.913, %237), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5140 : int = aten::size(%hidden_states.913, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.123 : Long(device=cpu) = prim::NumToTensor(%5140), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %5142 : int = aten::size(%hidden_states.913, %243), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5143 : int = aten::size(%hidden_states.913, %235), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5144 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.913, %237, %237, %236, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5145 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5144, %242, %237, %236, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5146 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%5145, %243), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5147 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5146, %235, %237, %236, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5148 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5147, %234, %237, %236, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5149 : int[] = prim::ListConstruct(%5139, %5140, %234, %5142, %5143), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %hidden_states.915 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%5148, %5149, %239), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5151 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.123, %223), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %5152 : int = aten::Int(%5151), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %5153 : int[] = prim::ListConstruct(%5139, %5152, %5142, %5143), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %value.61 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.915, %5153), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %5155 : int = aten::size(%key.61, %243), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %5156 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %5157 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%5156, %242, %237, %236, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %5158 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%5157, %243, %237, %236, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.67 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%5158, %235, %237, %5155, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.121 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.61, %key.61, %value.61, %attention_mask.67, %222, %239, %221, %239), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %5161 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.121, %242, %243), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.123 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%5161, %237), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %5163 : int[] = prim::ListConstruct(%5068, %5069, %231), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %5164 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.123, %5163), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %5165 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%5164, %237), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.551 : Tensor = prim::GetAttr[name="weight"](%o_proj.61)
  %hidden_states.917 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%5165, %weight.551), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn/__module.model.layers.30.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %5168 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.917, %hidden_states.909, %hidden_states.913)
  %5169 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5170 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %5171 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5168)
  %hidden_states.919 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5063, %5169, %242), scope: __module.model/__module.model.layers.30 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.553 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.61)
  %hidden_states.921 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.919, %229, %239, %239, %241), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %5175 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.921, %243), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %5176 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm
  %variance.123 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%5175, %5176, %220, %241), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %5178 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.123, %219, %242), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %5179 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%5178), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.923 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.921, %5179), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.925 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.923, %229, %239, %239, %241), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %5182 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.553, %hidden_states.925), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %5183 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%5182, %hidden_states.921)
  %5184 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5185 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5183)
  %down_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_391.Linear = prim::GetAttr[name="down_proj"](%mlp.61)
  %up_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_390.Linear = prim::GetAttr[name="up_proj"](%mlp.61)
  %gate_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_389.Linear = prim::GetAttr[name="gate_proj"](%mlp.61)
  %weight.555 : Tensor = prim::GetAttr[name="weight"](%gate_proj.61)
  %input.61 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%5184, %weight.555), scope: __module.model/__module.model.layers.30/__module.model.layers.30.mlp/__module.model.layers.30.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %5191 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.61), scope: __module.model/__module.model.layers.30/__module.model.layers.30.mlp/__module.model.layers.30.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.557 : Tensor = prim::GetAttr[name="weight"](%up_proj.61)
  %5193 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%5184, %weight.557), scope: __module.model/__module.model.layers.30/__module.model.layers.30.mlp/__module.model.layers.30.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %5194 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%5191, %5193), scope: __module.model/__module.model.layers.30/__module.model.layers.30.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.559 : Tensor = prim::GetAttr[name="weight"](%down_proj.61)
  %hidden_states.927 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%5194, %weight.559), scope: __module.model/__module.model.layers.30/__module.model.layers.30.mlp/__module.model.layers.30.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.929 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5185, %hidden_states.927, %242), scope: __module.model/__module.model.layers.30 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %5198 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.929, %5170, %5171)
  %5199 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5200 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %5201 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5198)
  %mlp : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_406.MistralMLP = prim::GetAttr[name="mlp"](%_31)
  %post_attention_layernorm : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_408.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_31)
  %self_attn : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_401.MistralAttention = prim::GetAttr[name="self_attn"](%_31)
  %input_layernorm : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_407.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_31)
  %weight.561 : Tensor = prim::GetAttr[name="weight"](%input_layernorm)
  %hidden_states.931 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%5199, %229, %239, %239, %241), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %5208 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.931, %243), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %5209 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm
  %variance.125 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%5208, %5209, %220, %241), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %5211 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.125, %219, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %5212 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%5211), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.933 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.931, %5212), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.935 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.933, %229, %239, %239, %241), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.937 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.561, %hidden_states.935), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %5216 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.937, %hidden_states.931)
  %5217 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5218 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5216)
  %o_proj : __torch__.torch.nn.modules.linear.___torch_mangle_400.Linear = prim::GetAttr[name="o_proj"](%self_attn)
  %v_proj : __torch__.torch.nn.modules.linear.___torch_mangle_399.Linear = prim::GetAttr[name="v_proj"](%self_attn)
  %k_proj : __torch__.torch.nn.modules.linear.___torch_mangle_398.Linear = prim::GetAttr[name="k_proj"](%self_attn)
  %q_proj : __torch__.torch.nn.modules.linear.___torch_mangle_397.Linear = prim::GetAttr[name="q_proj"](%self_attn)
  %5223 : int = aten::size(%5217, %237), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %5224 : int = aten::size(%5217, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.563 : Tensor = prim::GetAttr[name="weight"](%q_proj)
  %5226 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%5217, %weight.563), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn/__module.model.layers.31.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %5227 : int[] = prim::ListConstruct(%5223, %5224, %231, %226), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %5228 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%5226, %5227), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%5228, %242, %243), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.565 : Tensor = prim::GetAttr[name="weight"](%k_proj)
  %5231 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%5217, %weight.565), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn/__module.model.layers.31.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %5232 : int[] = prim::ListConstruct(%5223, %5224, %231, %226), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %5233 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%5231, %5232), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%5233, %242, %243), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.567 : Tensor = prim::GetAttr[name="weight"](%v_proj)
  %5236 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%5217, %weight.567), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn/__module.model.layers.31.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %5237 : int[] = prim::ListConstruct(%5223, %5224, %231, %226), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %5238 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%5236, %5237), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %value_states : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%5238, %242, %243), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%395, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%396, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %5242 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q, %cos), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %5243 : int = aten::size(%q, %235), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %5244 : Long(device=cpu) = prim::NumToTensor(%5243), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %5245 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5244, %225), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %5246 : int = aten::Int(%5245), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %x1.125 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q, %235, %237, %5246, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %5248 : int = aten::size(%q, %235), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %5249 : Long(device=cpu) = prim::NumToTensor(%5248), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %5250 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5249, %225), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %5251 : int = aten::Int(%5250), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %x2.125 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q, %235, %5251, %236, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %5253 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.125), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %5254 : Tensor[] = prim::ListConstruct(%5253, %x1.125), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %5255 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5254, %231), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %5256 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%5255, %sin), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5242, %5256, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %5258 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k, %cos), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %5259 : int = aten::size(%k, %235), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %5260 : Long(device=cpu) = prim::NumToTensor(%5259), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %5261 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5260, %225), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %5262 : int = aten::Int(%5261), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %x1 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k, %235, %237, %5262, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %5264 : int = aten::size(%k, %235), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %5265 : Long(device=cpu) = prim::NumToTensor(%5264), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %5266 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5265, %225), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %5267 : int = aten::Int(%5266), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %x2 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k, %235, %5267, %236, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %5269 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %5270 : Tensor[] = prim::ListConstruct(%5269, %x1), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %5271 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5270, %231), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %5272 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%5271, %sin), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %key_states : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%5258, %5272, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %5274 : Tensor[] = prim::ListConstruct(%101, %key_states), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %hidden_states.939 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5274, %224), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:500:0
  %5276 : Tensor[] = prim::ListConstruct(%102, %value_states), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %hidden_states.943 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5276, %224), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:501:0
  %5278 : int = aten::size(%hidden_states.939, %237), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5279 : int = aten::size(%hidden_states.939, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.125 : Long(device=cpu) = prim::NumToTensor(%5279), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %5281 : int = aten::size(%hidden_states.939, %243), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5282 : int = aten::size(%hidden_states.939, %235), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5283 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.939, %237, %237, %236, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5284 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5283, %242, %237, %236, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5285 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%5284, %243), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5286 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5285, %235, %237, %236, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5287 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5286, %234, %237, %236, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5288 : int[] = prim::ListConstruct(%5278, %5279, %234, %5281, %5282), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %hidden_states.941 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%5287, %5288, %239), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5290 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.125, %223), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %5291 : int = aten::Int(%5290), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %5292 : int[] = prim::ListConstruct(%5278, %5291, %5281, %5282), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %key : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.941, %5292), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %5294 : int = aten::size(%hidden_states.943, %237), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5295 : int = aten::size(%hidden_states.943, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads : Long(device=cpu) = prim::NumToTensor(%5295), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %5297 : int = aten::size(%hidden_states.943, %243), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5298 : int = aten::size(%hidden_states.943, %235), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %5299 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.943, %237, %237, %236, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5300 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5299, %242, %237, %236, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5301 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%5300, %243), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5302 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5301, %235, %237, %236, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5303 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5302, %234, %237, %236, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5304 : int[] = prim::ListConstruct(%5294, %5295, %234, %5297, %5298), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %hidden_states.945 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%5303, %5304, %239), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %5306 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads, %223), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %5307 : int = aten::Int(%5306), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %5308 : int[] = prim::ListConstruct(%5294, %5307, %5297, %5298), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %value : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.945, %5308), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %5310 : int = aten::size(%key, %243), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %5311 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %237, %237, %236, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %5312 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%5311, %242, %237, %236, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %5313 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%5312, %243, %237, %236, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%5313, %235, %237, %5310, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.125 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query, %key, %value, %attention_mask, %222, %239, %221, %239), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %5316 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.125, %242, %243), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%5316, %237), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %5318 : int[] = prim::ListConstruct(%5223, %5224, %231), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %5319 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output, %5318), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %5320 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%5319, %237), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.569 : Tensor = prim::GetAttr[name="weight"](%o_proj)
  %hidden_states.947 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%5320, %weight.569), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn/__module.model.layers.31.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %5323 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.947, %hidden_states.939, %hidden_states.943)
  %5324 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5325 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %5326 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5323)
  %hidden_states.949 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5218, %5324, %242), scope: __module.model/__module.model.layers.31 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.571 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm)
  %hidden_states.951 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.949, %229, %239, %239, %241), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %5330 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.951, %243), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %5331 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm
  %variance.127 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%5330, %5331, %220, %241), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %5333 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.127, %219, %242), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %5334 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%5333), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.953 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.951, %5334), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.955 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.953, %229, %239, %239, %241), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %5337 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.571, %hidden_states.955), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %5338 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%5337, %hidden_states.951)
  %5339 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5340 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5338)
  %down_proj : __torch__.torch.nn.modules.linear.___torch_mangle_404.Linear = prim::GetAttr[name="down_proj"](%mlp)
  %up_proj : __torch__.torch.nn.modules.linear.___torch_mangle_403.Linear = prim::GetAttr[name="up_proj"](%mlp)
  %gate_proj : __torch__.torch.nn.modules.linear.___torch_mangle_402.Linear = prim::GetAttr[name="gate_proj"](%mlp)
  %weight.573 : Tensor = prim::GetAttr[name="weight"](%gate_proj)
  %input : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%5339, %weight.573), scope: __module.model/__module.model.layers.31/__module.model.layers.31.mlp/__module.model.layers.31.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %5346 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input), scope: __module.model/__module.model.layers.31/__module.model.layers.31.mlp/__module.model.layers.31.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.575 : Tensor = prim::GetAttr[name="weight"](%up_proj)
  %5348 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%5339, %weight.575), scope: __module.model/__module.model.layers.31/__module.model.layers.31.mlp/__module.model.layers.31.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %5349 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%5346, %5348), scope: __module.model/__module.model.layers.31/__module.model.layers.31.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.577 : Tensor = prim::GetAttr[name="weight"](%down_proj)
  %hidden_states.957 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%5349, %weight.577), scope: __module.model/__module.model.layers.31/__module.model.layers.31.mlp/__module.model.layers.31.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.959 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5340, %hidden_states.957, %242), scope: __module.model/__module.model.layers.31 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %5353 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.959, %5325, %5326)
  %5354 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5355 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %5356 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5353)
  %weight.579 : Tensor = prim::GetAttr[name="weight"](%norm)
  %hidden_states.961 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%5354, %229, %239, %239, %241), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %5359 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.961, %243), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %5360 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.norm
  %variance : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%5359, %5360, %220, %241), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %5362 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance, %219, %242), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %5363 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%5362), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.963 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.961, %5363), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.965 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.963, %229, %239, %239, %241), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.579, %hidden_states.965), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %5367 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states, %550, %551, %705, %706, %860, %861, %1015, %1016, %1170, %1171, %1325, %1326, %1480, %1481, %1635, %1636, %1790, %1791, %1945, %1946, %2100, %2101, %2255, %2256, %2410, %2411, %2565, %2566, %2720, %2721, %2875, %2876, %3030, %3031, %3185, %3186, %3340, %3341, %3495, %3496, %3650, %3651, %3805, %3806, %3960, %3961, %4115, %4116, %4270, %4271, %4425, %4426, %4580, %4581, %4735, %4736, %4890, %4891, %5045, %5046, %5200, %5201, %5355, %5356)
  %104 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %105 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %106 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %107 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %108 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %109 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %110 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %111 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %112 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %113 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %114 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %115 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %116 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %117 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %118 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %119 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %120 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %121 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %122 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %123 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %124 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %125 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %126 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %127 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %128 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %129 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %130 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %131 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %132 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %133 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %134 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %135 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %136 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %137 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %138 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %139 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %140 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %141 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %142 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %143 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %144 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %145 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %146 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %147 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %148 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %149 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %150 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %151 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %152 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %153 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %154 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %155 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %156 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %157 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %158 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %159 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %160 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %161 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %162 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %163 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %164 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %165 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %166 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %167 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %168 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5367)
  %169 : int = prim::Constant[value=0]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %170 : int = prim::Constant[value=0]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %171 : int = prim::Constant[value=9223372036854775807]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %172 : int = prim::Constant[value=1]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %173 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%104, %169, %170, %171, %172) # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %174 : int = prim::Constant[value=1]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %175 : int = prim::Constant[value=0]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %176 : int = prim::Constant[value=9223372036854775807]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %177 : int = prim::Constant[value=1]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %178 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%173, %174, %175, %176, %177) # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %179 : int = prim::Constant[value=2]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %180 : int = prim::Constant[value=0]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %181 : int = prim::Constant[value=9223372036854775807]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %182 : int = prim::Constant[value=1]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %183 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%178, %179, %180, %181, %182) # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %weight : Tensor = prim::GetAttr[name="weight"](%lm_head)
  %5369 : Float(2, 16, 32000, strides=[512000, 32000, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%183, %weight), scope: __module.lm_head # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %185 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%105, %106)
  %186 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%107, %108)
  %187 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%109, %110)
  %188 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%111, %112)
  %189 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%113, %114)
  %190 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%115, %116)
  %191 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%117, %118)
  %192 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%119, %120)
  %193 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%121, %122)
  %194 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%123, %124)
  %195 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%125, %126)
  %196 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%127, %128)
  %197 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%129, %130)
  %198 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%131, %132)
  %199 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%133, %134)
  %200 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%135, %136)
  %201 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%137, %138)
  %202 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%139, %140)
  %203 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%141, %142)
  %204 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%143, %144)
  %205 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%145, %146)
  %206 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%147, %148)
  %207 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%149, %150)
  %208 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%151, %152)
  %209 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%153, %154)
  %210 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%155, %156)
  %211 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%157, %158)
  %212 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%159, %160)
  %213 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%161, %162)
  %214 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%163, %164)
  %215 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%165, %166)
  %216 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%167, %168)
  %217 : ((Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu))) = prim::TupleConstruct(%185, %186, %187, %188, %189, %190, %191, %192, %193, %194, %195, %196, %197, %198, %199, %200, %201, %202, %203, %204, %205, %206, %207, %208, %209, %210, %211, %212, %213, %214, %215, %216)
  %218 : (Float(2, 16, 32000, strides=[512000, 32000, 1], requires_grad=0, device=cpu), ((Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)))) = prim::TupleConstruct(%5369, %217)
  return (%218)

2026-01-14 09:30:18,865 - DEBUG - matplotlib data path: /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/matplotlib/mpl-data
2026-01-14 09:30:18,878 - DEBUG - CONFIGDIR=/home/phil2/.config/matplotlib
2026-01-14 09:30:18,880 - DEBUG - interactive is False
2026-01-14 09:30:18,880 - DEBUG - platform is linux
2026-01-14 09:30:18,987 - DEBUG - CACHEDIR=/home/phil2/.cache/matplotlib
2026-01-14 09:30:18,995 - DEBUG - Using fontManager instance from /home/phil2/.cache/matplotlib/fontlist-v390.json
2026-01-14 09:30:19,521 - DEBUG - {'conversion_parameters': {'framework': 'pytorch', 'is_python_object': True}}
2026-01-14 09:30:19,522 - DEBUG - Model Conversion API started
2026-01-14 09:30:19,525 - DEBUG - Placeholder shapes : [<PartialShape: [?,?]>, <PartialShape: [?,?]>, <PartialShape: [?,?]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>, <PartialShape: [?,8,?,128]>]
2026-01-14 09:30:46,769 - INFO - >>> [Bake] Remediation: Enforcing STRICT STATIC shapes [1, 128] in-memory...
2026-01-14 09:30:46,813 - INFO -     > Locking input_ids to [1, 128]
2026-01-14 09:30:46,814 - INFO -     > Locking attention_mask to [1, 128]
2026-01-14 09:30:46,814 - INFO -     > Locking position_ids to [1, 128]
2026-01-14 09:30:46,814 - INFO -     > Locking beam_idx to [1]
2026-01-14 09:30:46,814 - INFO - Applying reshape directly to in-memory graph...
2026-01-14 09:30:46,893 - INFO - >>> [Bake] Propagating shapes to internal States/Variables...
2026-01-14 09:30:46,898 - INFO - >>> [Bake] Saving optimized binaries to ./models/neuralchat_int4...
2026-01-14 09:30:48,113 - INFO - >>> [Bake] Saving Tokenizer...
2026-01-14 09:30:48,173 - INFO - >>> [Bake] Cleaning up memory...
2026-01-14 09:30:48,525 - INFO - >>> [Bake] Verifying static shapes...
2026-01-14 09:30:48,596 - INFO -     > [Verify Input] input_ids: [1,128]
2026-01-14 09:30:48,597 - INFO -     > [Verify Input] attention_mask: [1,128]
2026-01-14 09:30:48,597 - INFO -     > [Verify Input] position_ids: [1,128]
2026-01-14 09:30:48,597 - INFO -     > [Verify Input] beam_idx: [1]
2026-01-14 09:30:48,597 - INFO -     > [Verify Output] logits: [1,128,32000]
2026-01-14 09:30:48,597 - INFO - >>> [Bake] Verification Passed: Model is fully static.
2026-01-14 09:30:48,597 - INFO - >>> [Bake] Success! Optimized NPU-ready model is at ./models/neuralchat_int4
