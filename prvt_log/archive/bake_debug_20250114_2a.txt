2026-01-14 19:28:21,538 - INFO - >>> [Bake] Starting process for model: Intel/neural-chat-7b-v3-1
2026-01-14 19:28:21,538 - INFO - >>> [Bake] Downloading to ./model_staging...
2026-01-14 19:28:21,539 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2026-01-14 19:28:21,767 - DEBUG - https://huggingface.co:443 "GET /api/models/Intel/neural-chat-7b-v3-1/revision/main HTTP/1.1" 200 7138
2026-01-14 19:28:21,772 - DEBUG - Attempting to acquire lock 137406012304096 on model_staging/.cache/huggingface/download/.gitattributes.lock
2026-01-14 19:28:21,773 - DEBUG - Attempting to acquire lock 137406012305008 on model_staging/.cache/huggingface/download/README.md.lock
2026-01-14 19:28:21,774 - DEBUG - Attempting to acquire lock 137406012306160 on model_staging/.cache/huggingface/download/LICENSE.lock
2026-01-14 19:28:21,774 - DEBUG - Lock 137406012304096 acquired on model_staging/.cache/huggingface/download/.gitattributes.lock
2026-01-14 19:28:21,775 - DEBUG - Attempting to acquire lock 137406012307120 on model_staging/.cache/huggingface/download/config.json.lock
2026-01-14 19:28:21,775 - DEBUG - Attempting to acquire lock 137406012307648 on model_staging/.cache/huggingface/download/generation_config.json.lock
2026-01-14 19:28:21,776 - DEBUG - Lock 137406012306160 acquired on model_staging/.cache/huggingface/download/LICENSE.lock
2026-01-14 19:28:21,776 - DEBUG - Attempting to acquire lock 137406012306448 on model_staging/.cache/huggingface/download/model-00001-of-00002.safetensors.lock
2026-01-14 19:28:21,777 - DEBUG - Attempting to acquire lock 137406012507536 on model_staging/.cache/huggingface/download/model.safetensors.index.json.lock
2026-01-14 19:28:21,777 - DEBUG - Lock 137406012305008 acquired on model_staging/.cache/huggingface/download/README.md.lock
2026-01-14 19:28:21,777 - DEBUG - Attempting to release lock 137406012304096 on model_staging/.cache/huggingface/download/.gitattributes.lock
2026-01-14 19:28:21,777 - DEBUG - Attempting to acquire lock 137414729215792 on model_staging/.cache/huggingface/download/model-00002-of-00002.safetensors.lock
2026-01-14 19:28:21,777 - DEBUG - Lock 137406012307120 acquired on model_staging/.cache/huggingface/download/config.json.lock
2026-01-14 19:28:21,777 - DEBUG - Lock 137406012307648 acquired on model_staging/.cache/huggingface/download/generation_config.json.lock
2026-01-14 19:28:21,777 - DEBUG - Attempting to release lock 137406012306160 on model_staging/.cache/huggingface/download/LICENSE.lock
2026-01-14 19:28:21,778 - DEBUG - Lock 137406012306448 acquired on model_staging/.cache/huggingface/download/model-00001-of-00002.safetensors.lock
2026-01-14 19:28:21,778 - DEBUG - Lock 137406012507536 acquired on model_staging/.cache/huggingface/download/model.safetensors.index.json.lock
2026-01-14 19:28:21,778 - DEBUG - Attempting to release lock 137406012305008 on model_staging/.cache/huggingface/download/README.md.lock
2026-01-14 19:28:21,778 - DEBUG - Lock 137406012304096 released on model_staging/.cache/huggingface/download/.gitattributes.lock
2026-01-14 19:28:21,778 - DEBUG - Attempting to release lock 137406012307120 on model_staging/.cache/huggingface/download/config.json.lock
2026-01-14 19:28:21,778 - DEBUG - Lock 137414729215792 acquired on model_staging/.cache/huggingface/download/model-00002-of-00002.safetensors.lock
2026-01-14 19:28:21,778 - DEBUG - Lock 137406012306160 released on model_staging/.cache/huggingface/download/LICENSE.lock
2026-01-14 19:28:21,778 - DEBUG - Attempting to release lock 137406012307648 on model_staging/.cache/huggingface/download/generation_config.json.lock
2026-01-14 19:28:21,778 - DEBUG - Lock 137406012305008 released on model_staging/.cache/huggingface/download/README.md.lock
2026-01-14 19:28:21,778 - DEBUG - Attempting to release lock 137406012507536 on model_staging/.cache/huggingface/download/model.safetensors.index.json.lock
2026-01-14 19:28:21,778 - DEBUG - Attempting to release lock 137406012306448 on model_staging/.cache/huggingface/download/model-00001-of-00002.safetensors.lock
2026-01-14 19:28:21,779 - DEBUG - Lock 137406012307120 released on model_staging/.cache/huggingface/download/config.json.lock
2026-01-14 19:28:21,779 - DEBUG - Attempting to acquire lock 137406012508064 on model_staging/.cache/huggingface/download/pytorch_model-00001-of-00002.bin.lock
2026-01-14 19:28:21,779 - DEBUG - Attempting to acquire lock 137406012306112 on model_staging/.cache/huggingface/download/pytorch_model-00002-of-00002.bin.lock
2026-01-14 19:28:21,779 - DEBUG - Lock 137406012307648 released on model_staging/.cache/huggingface/download/generation_config.json.lock
2026-01-14 19:28:21,779 - DEBUG - Attempting to release lock 137414729215792 on model_staging/.cache/huggingface/download/model-00002-of-00002.safetensors.lock
2026-01-14 19:28:21,779 - DEBUG - Attempting to acquire lock 137406012305008 on model_staging/.cache/huggingface/download/pytorch_model.bin.index.json.lock
2026-01-14 19:28:21,779 - DEBUG - Lock 137406012507536 released on model_staging/.cache/huggingface/download/model.safetensors.index.json.lock
2026-01-14 19:28:21,779 - DEBUG - Lock 137406012306448 released on model_staging/.cache/huggingface/download/model-00001-of-00002.safetensors.lock
2026-01-14 19:28:21,780 - DEBUG - Attempting to acquire lock 137406012306064 on model_staging/.cache/huggingface/download/special_tokens_map.json.lock
2026-01-14 19:28:21,780 - DEBUG - Lock 137406012306112 acquired on model_staging/.cache/huggingface/download/pytorch_model-00002-of-00002.bin.lock
2026-01-14 19:28:21,780 - DEBUG - Lock 137406012508064 acquired on model_staging/.cache/huggingface/download/pytorch_model-00001-of-00002.bin.lock
2026-01-14 19:28:21,780 - DEBUG - Lock 137414729215792 released on model_staging/.cache/huggingface/download/model-00002-of-00002.safetensors.lock
2026-01-14 19:28:21,780 - DEBUG - Lock 137406012305008 acquired on model_staging/.cache/huggingface/download/pytorch_model.bin.index.json.lock
2026-01-14 19:28:21,781 - DEBUG - Attempting to acquire lock 137406012305248 on model_staging/.cache/huggingface/download/tokenizer.json.lock
2026-01-14 19:28:21,781 - DEBUG - Lock 137406012306064 acquired on model_staging/.cache/huggingface/download/special_tokens_map.json.lock
2026-01-14 19:28:21,781 - DEBUG - Attempting to acquire lock 137406012507248 on model_staging/.cache/huggingface/download/tokenizer_config.json.lock
2026-01-14 19:28:21,781 - DEBUG - Attempting to release lock 137406012508064 on model_staging/.cache/huggingface/download/pytorch_model-00001-of-00002.bin.lock
2026-01-14 19:28:21,781 - DEBUG - Attempting to acquire lock 137406012509504 on model_staging/.cache/huggingface/download/tokenizer.model.lock
2026-01-14 19:28:21,781 - DEBUG - Attempting to release lock 137406012306112 on model_staging/.cache/huggingface/download/pytorch_model-00002-of-00002.bin.lock
2026-01-14 19:28:21,781 - DEBUG - Attempting to release lock 137406012305008 on model_staging/.cache/huggingface/download/pytorch_model.bin.index.json.lock
2026-01-14 19:28:21,781 - DEBUG - Attempting to release lock 137406012306064 on model_staging/.cache/huggingface/download/special_tokens_map.json.lock
2026-01-14 19:28:21,782 - DEBUG - Lock 137406012508064 released on model_staging/.cache/huggingface/download/pytorch_model-00001-of-00002.bin.lock
2026-01-14 19:28:21,782 - DEBUG - Lock 137406012305248 acquired on model_staging/.cache/huggingface/download/tokenizer.json.lock
2026-01-14 19:28:21,782 - DEBUG - Lock 137406012509504 acquired on model_staging/.cache/huggingface/download/tokenizer.model.lock
2026-01-14 19:28:21,782 - DEBUG - Lock 137406012507248 acquired on model_staging/.cache/huggingface/download/tokenizer_config.json.lock
2026-01-14 19:28:21,782 - DEBUG - Lock 137406012306112 released on model_staging/.cache/huggingface/download/pytorch_model-00002-of-00002.bin.lock
2026-01-14 19:28:21,782 - DEBUG - Lock 137406012305008 released on model_staging/.cache/huggingface/download/pytorch_model.bin.index.json.lock
2026-01-14 19:28:21,782 - DEBUG - Lock 137406012306064 released on model_staging/.cache/huggingface/download/special_tokens_map.json.lock
2026-01-14 19:28:21,783 - DEBUG - Attempting to release lock 137406012509504 on model_staging/.cache/huggingface/download/tokenizer.model.lock
2026-01-14 19:28:21,783 - DEBUG - Attempting to release lock 137406012305248 on model_staging/.cache/huggingface/download/tokenizer.json.lock
2026-01-14 19:28:21,783 - DEBUG - Attempting to release lock 137406012507248 on model_staging/.cache/huggingface/download/tokenizer_config.json.lock
2026-01-14 19:28:21,783 - DEBUG - Lock 137406012509504 released on model_staging/.cache/huggingface/download/tokenizer.model.lock
2026-01-14 19:28:21,783 - DEBUG - Lock 137406012305248 released on model_staging/.cache/huggingface/download/tokenizer.json.lock
2026-01-14 19:28:21,783 - DEBUG - Lock 137406012507248 released on model_staging/.cache/huggingface/download/tokenizer_config.json.lock
2026-01-14 19:28:21,784 - INFO - >>> [Bake] Loading NNCF config from src/python/nncf_config.json...
2026-01-14 19:28:21,784 - INFO - >>> [Bake] Stage 1: Exporting + Compressing (Dynamic)...
2026-01-14 19:28:21,932 - DEBUG - Patching module Embedding(32000, 4096)
2026-01-14 19:28:21,932 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,932 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,933 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,934 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,935 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,936 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,937 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,938 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,938 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,938 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,938 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,938 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,938 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,938 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,938 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,938 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,938 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,938 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,938 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,938 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,938 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,938 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,938 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,938 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,938 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,938 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,938 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,939 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,940 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,941 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,941 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,941 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,941 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,941 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,941 - DEBUG - Patching module Linear(in_features=4096, out_features=1024, bias=False)
2026-01-14 19:28:21,941 - DEBUG - Patching module Linear(in_features=4096, out_features=4096, bias=False)
2026-01-14 19:28:21,941 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,941 - DEBUG - Patching module Linear(in_features=4096, out_features=14336, bias=False)
2026-01-14 19:28:21,941 - DEBUG - Patching module Linear(in_features=14336, out_features=4096, bias=False)
2026-01-14 19:28:21,941 - DEBUG - Patching module Linear(in_features=4096, out_features=32000, bias=False)
2026-01-14 19:28:21,941 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,941 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,941 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,941 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,941 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,941 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,941 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,941 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,941 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,942 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,943 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:21,944 - DEBUG - Casting module MistralRMSNorm((4096,), eps=1e-05) to float32
2026-01-14 19:28:24,105 - DEBUG - Inlined graph:
graph(%self.1 : __torch__.transformers.models.mistral.modeling_mistral.MistralForCausalLM,
      %input_ids : Long(2, 16, strides=[16, 1], requires_grad=0, device=cpu),
      %attention_mask.1 : Long(2, 16, strides=[16, 1], requires_grad=0, device=cpu),
      %position_ids : Long(2, 16, strides=[16, 1], requires_grad=0, device=cpu)):
  %lm_head : __torch__.torch.nn.modules.linear.___torch_mangle_411.Linear = prim::GetAttr[name="lm_head"](%self.1)
  %model : __torch__.transformers.models.mistral.modeling_mistral.MistralModel = prim::GetAttr[name="model"](%self.1)
  %24 : Double(requires_grad=0, device=cpu) = prim::Constant[value={1e-05}](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %25 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %26 : float = prim::Constant[value=0.088388347648318447](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %27 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %28 : Long(requires_grad=0, device=cpu) = prim::Constant[value={4}](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %29 : Long(requires_grad=0, device=cpu) = prim::Constant[value={2}](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %30 : int = prim::Constant[value=128](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %31 : Double(requires_grad=0, device=cpu) = prim::Constant[value={1}](), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:300:0
  %32 : Float(requires_grad=0, device=cpu) = prim::Constant[value={-65504}](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:208:0
  %33 : int = prim::Constant[value=6](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:207:0
  %34 : Float(requires_grad=0, device=cpu) = prim::Constant[value={0}](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:207:0
  %35 : int = prim::Constant[value=-1](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:412:0
  %36 : Long(requires_grad=0, device=cpu) = prim::Constant[value={4096}](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:88:0
  %37 : int = prim::Constant[value=4](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:407:0
  %38 : int = prim::Constant[value=3](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:405:0
  %39 : int = prim::Constant[value=9223372036854775807](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:405:0
  %40 : int = prim::Constant[value=2](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:405:0
  %41 : int = prim::Constant[value=11](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:723:0
  %42 : bool = prim::Constant[value=0](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:348:0
  %43 : Device = prim::Constant[value="cpu"](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:348:0
  %44 : NoneType = prim::Constant(), scope: __module.model
  %45 : int = prim::Constant[value=0](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:348:0
  %46 : Long(requires_grad=0, device=cpu) = prim::Constant[value={0}](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:349:0
  %47 : int = prim::Constant[value=1](), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:349:0
  %norm : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_410.MistralRMSNorm = prim::GetAttr[name="norm"](%model)
  %layers : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_31 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_409.MistralDecoderLayer = prim::GetAttr[name="31"](%layers)
  %layers.61 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_30 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_396.MistralDecoderLayer = prim::GetAttr[name="30"](%layers.61)
  %layers.59 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_29 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_383.MistralDecoderLayer = prim::GetAttr[name="29"](%layers.59)
  %layers.57 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_28 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_370.MistralDecoderLayer = prim::GetAttr[name="28"](%layers.57)
  %layers.55 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_27 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_357.MistralDecoderLayer = prim::GetAttr[name="27"](%layers.55)
  %layers.53 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_26 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_344.MistralDecoderLayer = prim::GetAttr[name="26"](%layers.53)
  %layers.51 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_25 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_331.MistralDecoderLayer = prim::GetAttr[name="25"](%layers.51)
  %layers.49 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_24 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_318.MistralDecoderLayer = prim::GetAttr[name="24"](%layers.49)
  %layers.47 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_23 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_305.MistralDecoderLayer = prim::GetAttr[name="23"](%layers.47)
  %layers.45 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_22 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_292.MistralDecoderLayer = prim::GetAttr[name="22"](%layers.45)
  %layers.43 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_21 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_279.MistralDecoderLayer = prim::GetAttr[name="21"](%layers.43)
  %layers.41 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_20 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_266.MistralDecoderLayer = prim::GetAttr[name="20"](%layers.41)
  %layers.39 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_19 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_253.MistralDecoderLayer = prim::GetAttr[name="19"](%layers.39)
  %layers.37 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_18 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_240.MistralDecoderLayer = prim::GetAttr[name="18"](%layers.37)
  %layers.35 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_17 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_227.MistralDecoderLayer = prim::GetAttr[name="17"](%layers.35)
  %layers.33 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_16 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_214.MistralDecoderLayer = prim::GetAttr[name="16"](%layers.33)
  %layers.31 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_15 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_201.MistralDecoderLayer = prim::GetAttr[name="15"](%layers.31)
  %layers.29 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_14 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_188.MistralDecoderLayer = prim::GetAttr[name="14"](%layers.29)
  %layers.27 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_13 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_175.MistralDecoderLayer = prim::GetAttr[name="13"](%layers.27)
  %layers.25 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_12 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_162.MistralDecoderLayer = prim::GetAttr[name="12"](%layers.25)
  %layers.23 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_11 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_149.MistralDecoderLayer = prim::GetAttr[name="11"](%layers.23)
  %layers.21 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_10 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_136.MistralDecoderLayer = prim::GetAttr[name="10"](%layers.21)
  %layers.19 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_9 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_123.MistralDecoderLayer = prim::GetAttr[name="9"](%layers.19)
  %layers.17 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_8 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_110.MistralDecoderLayer = prim::GetAttr[name="8"](%layers.17)
  %layers.15 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_7 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_97.MistralDecoderLayer = prim::GetAttr[name="7"](%layers.15)
  %layers.13 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_6 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_84.MistralDecoderLayer = prim::GetAttr[name="6"](%layers.13)
  %layers.11 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_5 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_71.MistralDecoderLayer = prim::GetAttr[name="5"](%layers.11)
  %layers.9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_4 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_58.MistralDecoderLayer = prim::GetAttr[name="4"](%layers.9)
  %layers.7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_3 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_45.MistralDecoderLayer = prim::GetAttr[name="3"](%layers.7)
  %layers.5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_2 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_32.MistralDecoderLayer = prim::GetAttr[name="2"](%layers.5)
  %layers.3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_1 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_19.MistralDecoderLayer = prim::GetAttr[name="1"](%layers.3)
  %layers.1 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%model)
  %_0 : __torch__.transformers.models.mistral.modeling_mistral.MistralDecoderLayer = prim::GetAttr[name="0"](%layers.1)
  %rotary_emb : __torch__.transformers.models.mistral.modeling_mistral.MistralRotaryEmbedding = prim::GetAttr[name="rotary_emb"](%model)
  %embed_tokens : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="embed_tokens"](%model)
  %weight.1 : Tensor = prim::GetAttr[name="weight"](%embed_tokens)
  %inputs_embeds : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None, False, False)(%weight.1, %input_ids), scope: __module.model/__module.model.embed_tokens # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %117 : int = aten::size(%inputs_embeds, %47), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:349:0
  %118 : Long(device=cpu) = prim::NumToTensor(%117), scope: __module.model
  %119 : Long(requires_grad=0, device=cpu) = aten::add(%118, %46, %47), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:349:0
  %120 : Scalar = aten::ScalarImplicit(%119), scope: __module.model
  %cache_position : Long(16, strides=[1], requires_grad=0, device=cpu) = aten::arange(%45, %120, %44, %44, %43, %42), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:348:0
  %attention_mask.3 : Bool(2, 16, strides=[16, 1], requires_grad=0, device=cpu) = aten::to(%attention_mask.1, %43, %41, %42, %42, %44), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:723:0
  %123 : int = aten::size(%inputs_embeds, %47), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:730:0
  %124 : int = aten::size(%inputs_embeds, %45), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:893:0
  %125 : int = aten::size(%cache_position, %45), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:391:0
  %126 : Long(1, 16, strides=[16, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%cache_position, %45), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:405:0
  %127 : Long(1, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%126, %47), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:405:0
  %128 : Long(1, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::slice(%127, %40, %45, %39, %47), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:405:0
  %q_idx : Long(1, 1, 16, 1, strides=[16, 16, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%128, %38), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:405:0
  %130 : Long(2, strides=[1], requires_grad=0, device=cpu) = aten::arange(%124, %37, %44, %43, %42), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:407:0
  %131 : Long(2, strides=[1], requires_grad=0, device=cpu) = aten::slice(%130, %45, %45, %39, %47), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:407:0
  %132 : Long(2, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%131, %47), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:407:0
  %133 : Long(2, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%132, %40), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:407:0
  %batch_idx : Long(2, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%133, %38), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:407:0
  %135 : Long(16, strides=[1], requires_grad=0, device=cpu) = aten::arange(%123, %37, %44, %43, %42), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:408:0
  %136 : Long(1, 16, strides=[16, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%135, %45), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:408:0
  %137 : Long(1, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%136, %47), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:408:0
  %138 : Long(1, 1, 1, 16, strides=[16, 16, 16, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%137, %40), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:408:0
  %139 : Long(1, 1, 1, 16, strides=[16, 16, 16, 1], requires_grad=0, device=cpu) = aten::slice(%138, %38, %45, %39, %47), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:408:0
  %kv_idx : Long(1, 1, 1, 16, strides=[16, 16, 16, 1], requires_grad=0, device=cpu) = aten::add(%139, %46, %47), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:408:0
  %141 : int[] = prim::ListConstruct(), scope: __module.model
  %result.5 : Bool(requires_grad=0, device=cpu) = aten::new_ones(%q_idx, %141, %41, %44, %44, %42), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:52:0
  %143 : int[] = prim::ListConstruct(), scope: __module.model
  %result.1 : Bool(requires_grad=0, device=cpu) = aten::new_ones(%q_idx, %143, %41, %44, %44, %42), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:52:0
  %145 : Long(1, 1, 16, 1, strides=[16, 16, 1, 1], requires_grad=0, device=cpu) = aten::sub(%q_idx, %36, %47), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:88:0
  %146 : Bool(1, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::gt(%kv_idx, %145), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:88:0
  %147 : Bool(1, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::to(%146, %41, %45, %43, %44, %42, %42, %44), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:54:0
  %result.3 : Bool(1, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::__and__(%result.1, %147), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:54:0
  %149 : Bool(1, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::le(%kv_idx, %q_idx), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:78:0
  %150 : Bool(1, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::to(%149, %41, %45, %43, %44, %42, %42, %44), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:54:0
  %151 : Bool(1, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::__and__(%result.3, %150), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:54:0
  %152 : Bool(1, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::to(%151, %41, %45, %43, %44, %42, %42, %44), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:54:0
  %result : Bool(1, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::__and__(%result.5, %152), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:54:0
  %154 : Tensor?[] = prim::ListConstruct(%batch_idx, %kv_idx), scope: __module.model
  %155 : Bool(2, 1, 1, 16, strides=[16, 16, 16, 1], requires_grad=0, device=cpu) = aten::index(%attention_mask.3, %154), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:142:0
  %156 : Bool(2, 1, 1, 16, strides=[16, 16, 16, 1], requires_grad=0, device=cpu) = aten::to(%155, %41, %45, %43, %44, %42, %42, %44), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:54:0
  %causal_mask : Bool(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::__and__(%result, %156), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/masking_utils.py:54:0
  %158 : int[] = prim::ListConstruct(%124, %35, %125, %123), scope: __module.model
  %mask : Bool(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::expand(%causal_mask, %158, %42), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/onnx/model_patcher.py:412:0
  %160 : Float(requires_grad=0, device=cpu) = aten::to(%34, %43, %33, %42, %42, %44), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:207:0
  %161 : Float(requires_grad=0, device=cpu) = aten::detach(%160), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:207:0
  %162 : Float(requires_grad=0, device=cpu) = aten::to(%32, %43, %33, %42, %42, %44), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:208:0
  %163 : Float(requires_grad=0, device=cpu) = aten::detach(%162), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:208:0
  %attention_mask.5 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::where(%mask, %161, %163), scope: __module.model # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:205:0
  %inv_freq : Tensor = prim::GetAttr[name="inv_freq"](%rotary_emb)
  %166 : Float(1, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%inv_freq, %45), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:293:0
  %167 : Float(1, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%166, %47, %45, %39, %47), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:293:0
  %168 : Float(1, 64, 1, strides=[64, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%167, %40), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:293:0
  %169 : Float(1, 64, 1, strides=[64, 1, 1], requires_grad=0, device=cpu) = aten::to(%168, %33, %42, %42, %44), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:293:0
  %170 : int = aten::size(%position_ids, %45), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:293:0
  %171 : int[] = prim::ListConstruct(%170, %35, %47), scope: __module.model/__module.model.rotary_emb
  %172 : Float(2, 64, 1, strides=[0, 1, 1], requires_grad=0, device=cpu) = aten::expand(%169, %171, %42), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:293:0
  %inv_freq_expanded.1 : Float(2, 64, 1, strides=[0, 1, 1], requires_grad=0, device=cpu) = aten::to(%172, %33, %45, %43, %44, %42, %42, %44), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:293:0
  %174 : Long(2, 16, strides=[16, 1], requires_grad=0, device=cpu) = aten::slice(%position_ids, %45, %45, %39, %47), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:294:0
  %175 : Long(2, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%174, %47), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:294:0
  %176 : Long(2, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::slice(%175, %40, %45, %39, %47), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:294:0
  %position_ids_expanded.1 : Float(2, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::to(%176, %33, %42, %42, %44), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:294:0
  %inv_freq_expanded : Float(2, 64, 1, strides=[0, 1, 1], requires_grad=0, device=cpu) = aten::to(%inv_freq_expanded.1, %33, %42, %42, %44), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:298:0
  %position_ids_expanded : Float(2, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::to(%position_ids_expanded.1, %33, %42, %42, %44), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:298:0
  %180 : Float(2, 64, 16, strides=[1024, 16, 1], requires_grad=0, device=cpu) = aten::matmul(%inv_freq_expanded, %position_ids_expanded), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:298:0
  %freqs : Float(2, 16, 64, strides=[1024, 1, 16], requires_grad=0, device=cpu) = aten::transpose(%180, %47, %40), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:298:0
  %182 : Tensor[] = prim::ListConstruct(%freqs, %freqs), scope: __module.model/__module.model.rotary_emb
  %emb : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%182, %35), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:299:0
  %184 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::cos(%emb), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:300:0
  %cos.1 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%184, %31), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:300:0
  %186 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::sin(%emb), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:301:0
  %sin.1 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%186, %31), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:301:0
  %cos.3 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::to(%cos.1, %33, %42, %42, %44), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:303:0
  %sin.3 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::to(%sin.1, %33, %42, %42, %44), scope: __module.model/__module.model.rotary_emb # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:303:0
  %190 : (Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu), Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%cos.3, %sin.3)
  %191 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu), %192 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%190)
  %mlp.1 : __torch__.transformers.models.mistral.modeling_mistral.MistralMLP = prim::GetAttr[name="mlp"](%_0)
  %post_attention_layernorm.1 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_6.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_0)
  %self_attn.1 : __torch__.transformers.models.mistral.modeling_mistral.MistralAttention = prim::GetAttr[name="self_attn"](%_0)
  %input_layernorm.1 : __torch__.transformers.models.mistral.modeling_mistral.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_0)
  %weight.3 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.1)
  %hidden_states.1 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%inputs_embeds, %33, %42, %42, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %199 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.1, %40), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %200 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm
  %variance.1 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%199, %200, %25, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %202 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.1, %24, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %203 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%202), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.3 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.1, %203), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.5 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.3, %33, %42, %42, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.7 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.3, %hidden_states.5), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %207 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.7, %hidden_states.1)
  %208 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %209 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%207)
  %o_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_2.Linear = prim::GetAttr[name="o_proj"](%self_attn.1)
  %v_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_1.Linear = prim::GetAttr[name="v_proj"](%self_attn.1)
  %k_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_0.Linear = prim::GetAttr[name="k_proj"](%self_attn.1)
  %q_proj.1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_proj"](%self_attn.1)
  %214 : int = aten::size(%208, %45), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %215 : int = aten::size(%208, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.5 : Tensor = prim::GetAttr[name="weight"](%q_proj.1)
  %217 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%208, %weight.5), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn/__module.model.layers.0.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %218 : int[] = prim::ListConstruct(%214, %215, %35, %30), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %219 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%217, %218), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.1 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%219, %47, %40), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.7 : Tensor = prim::GetAttr[name="weight"](%k_proj.1)
  %222 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%208, %weight.7), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn/__module.model.layers.0.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %223 : int[] = prim::ListConstruct(%214, %215, %35, %30), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %224 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%222, %223), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.1 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%224, %47, %40), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.9 : Tensor = prim::GetAttr[name="weight"](%v_proj.1)
  %227 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%208, %weight.9), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn/__module.model.layers.0.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %228 : int[] = prim::ListConstruct(%214, %215, %35, %30), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %229 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%227, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.13 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%229, %47, %40), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.5 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.5 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %233 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.1, %cos.5), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %234 : int = aten::size(%q.1, %38), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %235 : Long(device=cpu) = prim::NumToTensor(%234), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %236 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%235, %29), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %237 : int = aten::Int(%236), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %x1.1 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.1, %38, %45, %237, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %239 : int = aten::size(%q.1, %38), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %240 : Long(device=cpu) = prim::NumToTensor(%239), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %241 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%240, %29), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %242 : int = aten::Int(%241), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %x2.1 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.1, %38, %242, %39, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %244 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.1), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %245 : Tensor[] = prim::ListConstruct(%244, %x1.1), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %246 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%245, %35), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %247 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%246, %sin.5), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.1 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%233, %247, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %249 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.1, %cos.5), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %250 : int = aten::size(%k.1, %38), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %251 : Long(device=cpu) = prim::NumToTensor(%250), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %252 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%251, %29), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %253 : int = aten::Int(%252), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %x1.3 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.1, %38, %45, %253, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %255 : int = aten::size(%k.1, %38), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %256 : Long(device=cpu) = prim::NumToTensor(%255), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %257 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%256, %29), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %258 : int = aten::Int(%257), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %x2.3 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.1, %38, %258, %39, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %260 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.3), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %261 : Tensor[] = prim::ListConstruct(%260, %x1.3), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %262 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%261, %35), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %263 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%262, %sin.5), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.9 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%249, %263, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %265 : int = aten::size(%hidden_states.9, %45), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %266 : int = aten::size(%hidden_states.9, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.1 : Long(device=cpu) = prim::NumToTensor(%266), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %268 : int = aten::size(%hidden_states.9, %40), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %269 : int = aten::size(%hidden_states.9, %38), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %270 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.9, %45, %45, %39, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %271 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%270, %47, %45, %39, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %272 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%271, %40), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %273 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%272, %38, %45, %39, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %274 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%273, %37, %45, %39, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %275 : int[] = prim::ListConstruct(%265, %266, %37, %268, %269), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %hidden_states.11 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%274, %275, %42), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %277 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.1, %28), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %278 : int = aten::Int(%277), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %279 : int[] = prim::ListConstruct(%265, %278, %268, %269), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %key.1 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.11, %279), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %281 : int = aten::size(%hidden_states.13, %45), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %282 : int = aten::size(%hidden_states.13, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.3 : Long(device=cpu) = prim::NumToTensor(%282), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %284 : int = aten::size(%hidden_states.13, %40), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %285 : int = aten::size(%hidden_states.13, %38), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %286 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.13, %45, %45, %39, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %287 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%286, %47, %45, %39, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %288 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%287, %40), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %289 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%288, %38, %45, %39, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %290 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%289, %37, %45, %39, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %291 : int[] = prim::ListConstruct(%281, %282, %37, %284, %285), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %hidden_states.15 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%290, %291, %42), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %293 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.3, %28), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %294 : int = aten::Int(%293), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %295 : int[] = prim::ListConstruct(%281, %294, %284, %285), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %value.1 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.15, %295), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %297 : int = aten::size(%key.1, %40), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %298 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %299 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%298, %47, %45, %39, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %300 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%299, %40, %45, %39, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.7 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%300, %38, %45, %297, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.1 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.1, %key.1, %value.1, %attention_mask.7, %27, %42, %26, %42), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %303 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.1, %47, %40), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.3 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%303, %45), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %305 : int[] = prim::ListConstruct(%214, %215, %35), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn
  %306 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.3, %305), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %307 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%306, %45), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.11 : Tensor = prim::GetAttr[name="weight"](%o_proj.1)
  %hidden_states.17 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%307, %weight.11), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn/__module.model.layers.0.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.19 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%209, %hidden_states.17, %47), scope: __module.model/__module.model.layers.0 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.13 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.1)
  %hidden_states.21 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.19, %33, %42, %42, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %313 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.21, %40), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %314 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm
  %variance.3 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%313, %314, %25, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %316 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.3, %24, %47), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %317 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%316), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.23 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.21, %317), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.25 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.23, %33, %42, %42, %44), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %320 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.13, %hidden_states.25), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %321 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%320, %hidden_states.21)
  %322 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %323 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%321)
  %down_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name="down_proj"](%mlp.1)
  %up_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name="up_proj"](%mlp.1)
  %gate_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name="gate_proj"](%mlp.1)
  %weight.15 : Tensor = prim::GetAttr[name="weight"](%gate_proj.1)
  %input.1 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%322, %weight.15), scope: __module.model/__module.model.layers.0/__module.model.layers.0.mlp/__module.model.layers.0.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %329 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.1), scope: __module.model/__module.model.layers.0/__module.model.layers.0.mlp/__module.model.layers.0.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.17 : Tensor = prim::GetAttr[name="weight"](%up_proj.1)
  %331 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%322, %weight.17), scope: __module.model/__module.model.layers.0/__module.model.layers.0.mlp/__module.model.layers.0.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %332 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%329, %331), scope: __module.model/__module.model.layers.0/__module.model.layers.0.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.19 : Tensor = prim::GetAttr[name="weight"](%down_proj.1)
  %hidden_states.27 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%332, %weight.19), scope: __module.model/__module.model.layers.0/__module.model.layers.0.mlp/__module.model.layers.0.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.29 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%323, %hidden_states.27, %47), scope: __module.model/__module.model.layers.0 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.3 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_16.MistralMLP = prim::GetAttr[name="mlp"](%_1)
  %post_attention_layernorm.3 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_18.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_1)
  %self_attn.3 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_11.MistralAttention = prim::GetAttr[name="self_attn"](%_1)
  %input_layernorm.3 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_17.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_1)
  %weight.21 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.3)
  %hidden_states.31 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.29, %33, %42, %42, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %342 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.31, %40), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %343 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm
  %variance.5 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%342, %343, %25, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %345 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.5, %24, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %346 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%345), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.33 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.31, %346), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.35 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.33, %33, %42, %42, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.37 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.21, %hidden_states.35), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %350 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.37, %hidden_states.31)
  %351 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %352 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%350)
  %o_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name="o_proj"](%self_attn.3)
  %v_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_9.Linear = prim::GetAttr[name="v_proj"](%self_attn.3)
  %k_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name="k_proj"](%self_attn.3)
  %q_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_7.Linear = prim::GetAttr[name="q_proj"](%self_attn.3)
  %357 : int = aten::size(%351, %45), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %358 : int = aten::size(%351, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.23 : Tensor = prim::GetAttr[name="weight"](%q_proj.3)
  %360 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%351, %weight.23), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn/__module.model.layers.1.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %361 : int[] = prim::ListConstruct(%357, %358, %35, %30), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %362 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%360, %361), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.3 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%362, %47, %40), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.25 : Tensor = prim::GetAttr[name="weight"](%k_proj.3)
  %365 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%351, %weight.25), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn/__module.model.layers.1.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %366 : int[] = prim::ListConstruct(%357, %358, %35, %30), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %367 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%365, %366), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.3 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%367, %47, %40), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.27 : Tensor = prim::GetAttr[name="weight"](%v_proj.3)
  %370 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%351, %weight.27), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn/__module.model.layers.1.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %371 : int[] = prim::ListConstruct(%357, %358, %35, %30), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %372 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%370, %371), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.43 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%372, %47, %40), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.7 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.7 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %376 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.3, %cos.7), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %377 : int = aten::size(%q.3, %38), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %378 : Long(device=cpu) = prim::NumToTensor(%377), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %379 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%378, %29), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %380 : int = aten::Int(%379), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %x1.5 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.3, %38, %45, %380, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %382 : int = aten::size(%q.3, %38), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %383 : Long(device=cpu) = prim::NumToTensor(%382), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %384 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%383, %29), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %385 : int = aten::Int(%384), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %x2.5 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.3, %38, %385, %39, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %387 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.5), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %388 : Tensor[] = prim::ListConstruct(%387, %x1.5), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %389 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%388, %35), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %390 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%389, %sin.7), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.3 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%376, %390, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %392 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.3, %cos.7), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %393 : int = aten::size(%k.3, %38), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %394 : Long(device=cpu) = prim::NumToTensor(%393), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %395 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%394, %29), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %396 : int = aten::Int(%395), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %x1.7 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.3, %38, %45, %396, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %398 : int = aten::size(%k.3, %38), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %399 : Long(device=cpu) = prim::NumToTensor(%398), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %400 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%399, %29), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %401 : int = aten::Int(%400), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %x2.7 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.3, %38, %401, %39, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %403 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.7), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %404 : Tensor[] = prim::ListConstruct(%403, %x1.7), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %405 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%404, %35), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %406 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%405, %sin.7), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.39 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%392, %406, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %408 : int = aten::size(%hidden_states.39, %45), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %409 : int = aten::size(%hidden_states.39, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.5 : Long(device=cpu) = prim::NumToTensor(%409), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %411 : int = aten::size(%hidden_states.39, %40), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %412 : int = aten::size(%hidden_states.39, %38), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %413 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.39, %45, %45, %39, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %414 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%413, %47, %45, %39, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %415 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%414, %40), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %416 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%415, %38, %45, %39, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %417 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%416, %37, %45, %39, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %418 : int[] = prim::ListConstruct(%408, %409, %37, %411, %412), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %hidden_states.41 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%417, %418, %42), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %420 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.5, %28), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %421 : int = aten::Int(%420), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %422 : int[] = prim::ListConstruct(%408, %421, %411, %412), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %key.3 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.41, %422), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %424 : int = aten::size(%hidden_states.43, %45), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %425 : int = aten::size(%hidden_states.43, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.7 : Long(device=cpu) = prim::NumToTensor(%425), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %427 : int = aten::size(%hidden_states.43, %40), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %428 : int = aten::size(%hidden_states.43, %38), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %429 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.43, %45, %45, %39, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %430 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%429, %47, %45, %39, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %431 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%430, %40), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %432 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%431, %38, %45, %39, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %433 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%432, %37, %45, %39, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %434 : int[] = prim::ListConstruct(%424, %425, %37, %427, %428), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %hidden_states.45 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%433, %434, %42), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %436 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.7, %28), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %437 : int = aten::Int(%436), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %438 : int[] = prim::ListConstruct(%424, %437, %427, %428), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %value.3 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.45, %438), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %440 : int = aten::size(%key.3, %40), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %441 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %442 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%441, %47, %45, %39, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %443 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%442, %40, %45, %39, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.9 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%443, %38, %45, %440, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.5 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.3, %key.3, %value.3, %attention_mask.9, %27, %42, %26, %42), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %446 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.5, %47, %40), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.7 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%446, %45), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %448 : int[] = prim::ListConstruct(%357, %358, %35), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn
  %449 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.7, %448), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %450 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%449, %45), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.29 : Tensor = prim::GetAttr[name="weight"](%o_proj.3)
  %hidden_states.47 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%450, %weight.29), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn/__module.model.layers.1.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.49 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%352, %hidden_states.47, %47), scope: __module.model/__module.model.layers.1 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.31 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.3)
  %hidden_states.51 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.49, %33, %42, %42, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %456 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.51, %40), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %457 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm
  %variance.7 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%456, %457, %25, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %459 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.7, %24, %47), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %460 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%459), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.53 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.51, %460), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.55 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.53, %33, %42, %42, %44), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %463 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.31, %hidden_states.55), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %464 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%463, %hidden_states.51)
  %465 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %466 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%464)
  %down_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_14.Linear = prim::GetAttr[name="down_proj"](%mlp.3)
  %up_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_13.Linear = prim::GetAttr[name="up_proj"](%mlp.3)
  %gate_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_12.Linear = prim::GetAttr[name="gate_proj"](%mlp.3)
  %weight.33 : Tensor = prim::GetAttr[name="weight"](%gate_proj.3)
  %input.3 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%465, %weight.33), scope: __module.model/__module.model.layers.1/__module.model.layers.1.mlp/__module.model.layers.1.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %472 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.3), scope: __module.model/__module.model.layers.1/__module.model.layers.1.mlp/__module.model.layers.1.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.35 : Tensor = prim::GetAttr[name="weight"](%up_proj.3)
  %474 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%465, %weight.35), scope: __module.model/__module.model.layers.1/__module.model.layers.1.mlp/__module.model.layers.1.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %475 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%472, %474), scope: __module.model/__module.model.layers.1/__module.model.layers.1.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.37 : Tensor = prim::GetAttr[name="weight"](%down_proj.3)
  %hidden_states.57 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%475, %weight.37), scope: __module.model/__module.model.layers.1/__module.model.layers.1.mlp/__module.model.layers.1.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.59 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%466, %hidden_states.57, %47), scope: __module.model/__module.model.layers.1 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.5 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_29.MistralMLP = prim::GetAttr[name="mlp"](%_2)
  %post_attention_layernorm.5 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_31.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_2)
  %self_attn.5 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_24.MistralAttention = prim::GetAttr[name="self_attn"](%_2)
  %input_layernorm.5 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_30.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_2)
  %weight.39 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.5)
  %hidden_states.61 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.59, %33, %42, %42, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %485 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.61, %40), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %486 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm
  %variance.9 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%485, %486, %25, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %488 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.9, %24, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %489 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%488), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.63 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.61, %489), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.65 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.63, %33, %42, %42, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.67 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.39, %hidden_states.65), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %493 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.67, %hidden_states.61)
  %494 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %495 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%493)
  %o_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_23.Linear = prim::GetAttr[name="o_proj"](%self_attn.5)
  %v_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_22.Linear = prim::GetAttr[name="v_proj"](%self_attn.5)
  %k_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_21.Linear = prim::GetAttr[name="k_proj"](%self_attn.5)
  %q_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_20.Linear = prim::GetAttr[name="q_proj"](%self_attn.5)
  %500 : int = aten::size(%494, %45), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %501 : int = aten::size(%494, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.41 : Tensor = prim::GetAttr[name="weight"](%q_proj.5)
  %503 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%494, %weight.41), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn/__module.model.layers.2.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %504 : int[] = prim::ListConstruct(%500, %501, %35, %30), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %505 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%503, %504), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.5 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%505, %47, %40), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.43 : Tensor = prim::GetAttr[name="weight"](%k_proj.5)
  %508 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%494, %weight.43), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn/__module.model.layers.2.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %509 : int[] = prim::ListConstruct(%500, %501, %35, %30), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %510 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%508, %509), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.5 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%510, %47, %40), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.45 : Tensor = prim::GetAttr[name="weight"](%v_proj.5)
  %513 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%494, %weight.45), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn/__module.model.layers.2.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %514 : int[] = prim::ListConstruct(%500, %501, %35, %30), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %515 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%513, %514), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.73 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%515, %47, %40), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.9 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.9 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %519 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.5, %cos.9), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %520 : int = aten::size(%q.5, %38), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %521 : Long(device=cpu) = prim::NumToTensor(%520), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %522 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%521, %29), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %523 : int = aten::Int(%522), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %x1.9 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.5, %38, %45, %523, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %525 : int = aten::size(%q.5, %38), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %526 : Long(device=cpu) = prim::NumToTensor(%525), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %527 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%526, %29), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %528 : int = aten::Int(%527), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %x2.9 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.5, %38, %528, %39, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %530 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.9), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %531 : Tensor[] = prim::ListConstruct(%530, %x1.9), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %532 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%531, %35), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %533 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%532, %sin.9), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.5 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%519, %533, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %535 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.5, %cos.9), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %536 : int = aten::size(%k.5, %38), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %537 : Long(device=cpu) = prim::NumToTensor(%536), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %538 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%537, %29), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %539 : int = aten::Int(%538), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %x1.11 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.5, %38, %45, %539, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %541 : int = aten::size(%k.5, %38), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %542 : Long(device=cpu) = prim::NumToTensor(%541), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %543 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%542, %29), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %544 : int = aten::Int(%543), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %x2.11 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.5, %38, %544, %39, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %546 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.11), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %547 : Tensor[] = prim::ListConstruct(%546, %x1.11), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %548 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%547, %35), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %549 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%548, %sin.9), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.69 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%535, %549, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %551 : int = aten::size(%hidden_states.69, %45), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %552 : int = aten::size(%hidden_states.69, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.9 : Long(device=cpu) = prim::NumToTensor(%552), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %554 : int = aten::size(%hidden_states.69, %40), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %555 : int = aten::size(%hidden_states.69, %38), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %556 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.69, %45, %45, %39, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %557 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%556, %47, %45, %39, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %558 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%557, %40), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %559 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%558, %38, %45, %39, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %560 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%559, %37, %45, %39, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %561 : int[] = prim::ListConstruct(%551, %552, %37, %554, %555), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %hidden_states.71 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%560, %561, %42), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %563 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.9, %28), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %564 : int = aten::Int(%563), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %565 : int[] = prim::ListConstruct(%551, %564, %554, %555), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %key.5 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.71, %565), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %567 : int = aten::size(%hidden_states.73, %45), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %568 : int = aten::size(%hidden_states.73, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.11 : Long(device=cpu) = prim::NumToTensor(%568), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %570 : int = aten::size(%hidden_states.73, %40), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %571 : int = aten::size(%hidden_states.73, %38), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %572 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.73, %45, %45, %39, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %573 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%572, %47, %45, %39, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %574 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%573, %40), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %575 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%574, %38, %45, %39, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %576 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%575, %37, %45, %39, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %577 : int[] = prim::ListConstruct(%567, %568, %37, %570, %571), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %hidden_states.75 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%576, %577, %42), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %579 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.11, %28), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %580 : int = aten::Int(%579), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %581 : int[] = prim::ListConstruct(%567, %580, %570, %571), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %value.5 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.75, %581), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %583 : int = aten::size(%key.5, %40), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %584 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %585 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%584, %47, %45, %39, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %586 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%585, %40, %45, %39, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.11 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%586, %38, %45, %583, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.9 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.5, %key.5, %value.5, %attention_mask.11, %27, %42, %26, %42), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %589 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.9, %47, %40), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.11 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%589, %45), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %591 : int[] = prim::ListConstruct(%500, %501, %35), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn
  %592 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.11, %591), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %593 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%592, %45), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.47 : Tensor = prim::GetAttr[name="weight"](%o_proj.5)
  %hidden_states.77 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%593, %weight.47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn/__module.model.layers.2.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.79 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%495, %hidden_states.77, %47), scope: __module.model/__module.model.layers.2 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.49 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.5)
  %hidden_states.81 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.79, %33, %42, %42, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %599 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.81, %40), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %600 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm
  %variance.11 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%599, %600, %25, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %602 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.11, %24, %47), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %603 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%602), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.83 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.81, %603), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.85 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.83, %33, %42, %42, %44), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %606 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.49, %hidden_states.85), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %607 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%606, %hidden_states.81)
  %608 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %609 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%607)
  %down_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_27.Linear = prim::GetAttr[name="down_proj"](%mlp.5)
  %up_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_26.Linear = prim::GetAttr[name="up_proj"](%mlp.5)
  %gate_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_25.Linear = prim::GetAttr[name="gate_proj"](%mlp.5)
  %weight.51 : Tensor = prim::GetAttr[name="weight"](%gate_proj.5)
  %input.5 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%608, %weight.51), scope: __module.model/__module.model.layers.2/__module.model.layers.2.mlp/__module.model.layers.2.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %615 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.5), scope: __module.model/__module.model.layers.2/__module.model.layers.2.mlp/__module.model.layers.2.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.53 : Tensor = prim::GetAttr[name="weight"](%up_proj.5)
  %617 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%608, %weight.53), scope: __module.model/__module.model.layers.2/__module.model.layers.2.mlp/__module.model.layers.2.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %618 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%615, %617), scope: __module.model/__module.model.layers.2/__module.model.layers.2.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.55 : Tensor = prim::GetAttr[name="weight"](%down_proj.5)
  %hidden_states.87 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%618, %weight.55), scope: __module.model/__module.model.layers.2/__module.model.layers.2.mlp/__module.model.layers.2.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.89 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%609, %hidden_states.87, %47), scope: __module.model/__module.model.layers.2 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.7 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_42.MistralMLP = prim::GetAttr[name="mlp"](%_3)
  %post_attention_layernorm.7 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_44.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_3)
  %self_attn.7 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_37.MistralAttention = prim::GetAttr[name="self_attn"](%_3)
  %input_layernorm.7 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_43.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_3)
  %weight.57 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.7)
  %hidden_states.91 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.89, %33, %42, %42, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %628 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.91, %40), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %629 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm
  %variance.13 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%628, %629, %25, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %631 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.13, %24, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %632 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%631), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.93 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.91, %632), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.95 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.93, %33, %42, %42, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.97 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.57, %hidden_states.95), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %636 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.97, %hidden_states.91)
  %637 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %638 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%636)
  %o_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_36.Linear = prim::GetAttr[name="o_proj"](%self_attn.7)
  %v_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_35.Linear = prim::GetAttr[name="v_proj"](%self_attn.7)
  %k_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_34.Linear = prim::GetAttr[name="k_proj"](%self_attn.7)
  %q_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_33.Linear = prim::GetAttr[name="q_proj"](%self_attn.7)
  %643 : int = aten::size(%637, %45), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %644 : int = aten::size(%637, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.59 : Tensor = prim::GetAttr[name="weight"](%q_proj.7)
  %646 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%637, %weight.59), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn/__module.model.layers.3.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %647 : int[] = prim::ListConstruct(%643, %644, %35, %30), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %648 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%646, %647), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.7 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%648, %47, %40), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.61 : Tensor = prim::GetAttr[name="weight"](%k_proj.7)
  %651 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%637, %weight.61), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn/__module.model.layers.3.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %652 : int[] = prim::ListConstruct(%643, %644, %35, %30), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %653 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%651, %652), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.7 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%653, %47, %40), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.63 : Tensor = prim::GetAttr[name="weight"](%v_proj.7)
  %656 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%637, %weight.63), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn/__module.model.layers.3.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %657 : int[] = prim::ListConstruct(%643, %644, %35, %30), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %658 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%656, %657), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.103 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%658, %47, %40), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.11 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.11 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %662 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.7, %cos.11), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %663 : int = aten::size(%q.7, %38), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %664 : Long(device=cpu) = prim::NumToTensor(%663), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %665 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%664, %29), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %666 : int = aten::Int(%665), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %x1.13 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.7, %38, %45, %666, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %668 : int = aten::size(%q.7, %38), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %669 : Long(device=cpu) = prim::NumToTensor(%668), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %670 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%669, %29), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %671 : int = aten::Int(%670), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %x2.13 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.7, %38, %671, %39, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %673 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.13), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %674 : Tensor[] = prim::ListConstruct(%673, %x1.13), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %675 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%674, %35), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %676 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%675, %sin.11), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.7 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%662, %676, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %678 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.7, %cos.11), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %679 : int = aten::size(%k.7, %38), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %680 : Long(device=cpu) = prim::NumToTensor(%679), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %681 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%680, %29), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %682 : int = aten::Int(%681), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %x1.15 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.7, %38, %45, %682, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %684 : int = aten::size(%k.7, %38), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %685 : Long(device=cpu) = prim::NumToTensor(%684), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %686 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%685, %29), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %687 : int = aten::Int(%686), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %x2.15 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.7, %38, %687, %39, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %689 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.15), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %690 : Tensor[] = prim::ListConstruct(%689, %x1.15), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %691 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%690, %35), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %692 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%691, %sin.11), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.99 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%678, %692, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %694 : int = aten::size(%hidden_states.99, %45), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %695 : int = aten::size(%hidden_states.99, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.13 : Long(device=cpu) = prim::NumToTensor(%695), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %697 : int = aten::size(%hidden_states.99, %40), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %698 : int = aten::size(%hidden_states.99, %38), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %699 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.99, %45, %45, %39, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %700 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%699, %47, %45, %39, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %701 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%700, %40), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %702 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%701, %38, %45, %39, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %703 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%702, %37, %45, %39, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %704 : int[] = prim::ListConstruct(%694, %695, %37, %697, %698), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %hidden_states.101 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%703, %704, %42), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %706 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.13, %28), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %707 : int = aten::Int(%706), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %708 : int[] = prim::ListConstruct(%694, %707, %697, %698), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %key.7 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.101, %708), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %710 : int = aten::size(%hidden_states.103, %45), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %711 : int = aten::size(%hidden_states.103, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.15 : Long(device=cpu) = prim::NumToTensor(%711), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %713 : int = aten::size(%hidden_states.103, %40), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %714 : int = aten::size(%hidden_states.103, %38), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %715 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.103, %45, %45, %39, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %716 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%715, %47, %45, %39, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %717 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%716, %40), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %718 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%717, %38, %45, %39, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %719 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%718, %37, %45, %39, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %720 : int[] = prim::ListConstruct(%710, %711, %37, %713, %714), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %hidden_states.105 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%719, %720, %42), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %722 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.15, %28), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %723 : int = aten::Int(%722), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %724 : int[] = prim::ListConstruct(%710, %723, %713, %714), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %value.7 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.105, %724), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %726 : int = aten::size(%key.7, %40), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %727 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %728 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%727, %47, %45, %39, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %729 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%728, %40, %45, %39, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.13 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%729, %38, %45, %726, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.13 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.7, %key.7, %value.7, %attention_mask.13, %27, %42, %26, %42), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %732 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.13, %47, %40), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.15 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%732, %45), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %734 : int[] = prim::ListConstruct(%643, %644, %35), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn
  %735 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.15, %734), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %736 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%735, %45), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.65 : Tensor = prim::GetAttr[name="weight"](%o_proj.7)
  %hidden_states.107 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%736, %weight.65), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn/__module.model.layers.3.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.109 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%638, %hidden_states.107, %47), scope: __module.model/__module.model.layers.3 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.67 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.7)
  %hidden_states.111 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.109, %33, %42, %42, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %742 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.111, %40), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %743 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm
  %variance.15 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%742, %743, %25, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %745 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.15, %24, %47), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %746 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%745), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.113 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.111, %746), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.115 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.113, %33, %42, %42, %44), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %749 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.67, %hidden_states.115), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %750 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%749, %hidden_states.111)
  %751 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %752 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%750)
  %down_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_40.Linear = prim::GetAttr[name="down_proj"](%mlp.7)
  %up_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_39.Linear = prim::GetAttr[name="up_proj"](%mlp.7)
  %gate_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_38.Linear = prim::GetAttr[name="gate_proj"](%mlp.7)
  %weight.69 : Tensor = prim::GetAttr[name="weight"](%gate_proj.7)
  %input.7 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%751, %weight.69), scope: __module.model/__module.model.layers.3/__module.model.layers.3.mlp/__module.model.layers.3.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %758 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.7), scope: __module.model/__module.model.layers.3/__module.model.layers.3.mlp/__module.model.layers.3.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.71 : Tensor = prim::GetAttr[name="weight"](%up_proj.7)
  %760 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%751, %weight.71), scope: __module.model/__module.model.layers.3/__module.model.layers.3.mlp/__module.model.layers.3.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %761 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%758, %760), scope: __module.model/__module.model.layers.3/__module.model.layers.3.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.73 : Tensor = prim::GetAttr[name="weight"](%down_proj.7)
  %hidden_states.117 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%761, %weight.73), scope: __module.model/__module.model.layers.3/__module.model.layers.3.mlp/__module.model.layers.3.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.119 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%752, %hidden_states.117, %47), scope: __module.model/__module.model.layers.3 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.9 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_55.MistralMLP = prim::GetAttr[name="mlp"](%_4)
  %post_attention_layernorm.9 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_57.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_4)
  %self_attn.9 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_50.MistralAttention = prim::GetAttr[name="self_attn"](%_4)
  %input_layernorm.9 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_56.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_4)
  %weight.75 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.9)
  %hidden_states.121 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.119, %33, %42, %42, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %771 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.121, %40), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %772 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm
  %variance.17 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%771, %772, %25, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %774 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.17, %24, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %775 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%774), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.123 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.121, %775), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.125 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.123, %33, %42, %42, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.127 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.75, %hidden_states.125), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %779 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.127, %hidden_states.121)
  %780 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %781 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%779)
  %o_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_49.Linear = prim::GetAttr[name="o_proj"](%self_attn.9)
  %v_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_48.Linear = prim::GetAttr[name="v_proj"](%self_attn.9)
  %k_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_47.Linear = prim::GetAttr[name="k_proj"](%self_attn.9)
  %q_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_46.Linear = prim::GetAttr[name="q_proj"](%self_attn.9)
  %786 : int = aten::size(%780, %45), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %787 : int = aten::size(%780, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.77 : Tensor = prim::GetAttr[name="weight"](%q_proj.9)
  %789 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%780, %weight.77), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn/__module.model.layers.4.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %790 : int[] = prim::ListConstruct(%786, %787, %35, %30), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %791 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%789, %790), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.9 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%791, %47, %40), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.79 : Tensor = prim::GetAttr[name="weight"](%k_proj.9)
  %794 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%780, %weight.79), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn/__module.model.layers.4.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %795 : int[] = prim::ListConstruct(%786, %787, %35, %30), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %796 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%794, %795), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.9 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%796, %47, %40), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.81 : Tensor = prim::GetAttr[name="weight"](%v_proj.9)
  %799 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%780, %weight.81), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn/__module.model.layers.4.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %800 : int[] = prim::ListConstruct(%786, %787, %35, %30), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %801 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%799, %800), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.133 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%801, %47, %40), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.13 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.13 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %805 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.9, %cos.13), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %806 : int = aten::size(%q.9, %38), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %807 : Long(device=cpu) = prim::NumToTensor(%806), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %808 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%807, %29), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %809 : int = aten::Int(%808), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %x1.17 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.9, %38, %45, %809, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %811 : int = aten::size(%q.9, %38), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %812 : Long(device=cpu) = prim::NumToTensor(%811), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %813 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%812, %29), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %814 : int = aten::Int(%813), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %x2.17 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.9, %38, %814, %39, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %816 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.17), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %817 : Tensor[] = prim::ListConstruct(%816, %x1.17), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %818 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%817, %35), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %819 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%818, %sin.13), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.9 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%805, %819, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %821 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.9, %cos.13), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %822 : int = aten::size(%k.9, %38), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %823 : Long(device=cpu) = prim::NumToTensor(%822), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %824 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%823, %29), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %825 : int = aten::Int(%824), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %x1.19 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.9, %38, %45, %825, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %827 : int = aten::size(%k.9, %38), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %828 : Long(device=cpu) = prim::NumToTensor(%827), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %829 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%828, %29), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %830 : int = aten::Int(%829), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %x2.19 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.9, %38, %830, %39, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %832 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.19), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %833 : Tensor[] = prim::ListConstruct(%832, %x1.19), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %834 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%833, %35), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %835 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%834, %sin.13), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.129 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%821, %835, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %837 : int = aten::size(%hidden_states.129, %45), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %838 : int = aten::size(%hidden_states.129, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.17 : Long(device=cpu) = prim::NumToTensor(%838), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %840 : int = aten::size(%hidden_states.129, %40), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %841 : int = aten::size(%hidden_states.129, %38), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %842 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.129, %45, %45, %39, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %843 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%842, %47, %45, %39, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %844 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%843, %40), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %845 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%844, %38, %45, %39, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %846 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%845, %37, %45, %39, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %847 : int[] = prim::ListConstruct(%837, %838, %37, %840, %841), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %hidden_states.131 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%846, %847, %42), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %849 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.17, %28), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %850 : int = aten::Int(%849), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %851 : int[] = prim::ListConstruct(%837, %850, %840, %841), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %key.9 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.131, %851), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %853 : int = aten::size(%hidden_states.133, %45), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %854 : int = aten::size(%hidden_states.133, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.19 : Long(device=cpu) = prim::NumToTensor(%854), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %856 : int = aten::size(%hidden_states.133, %40), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %857 : int = aten::size(%hidden_states.133, %38), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %858 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.133, %45, %45, %39, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %859 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%858, %47, %45, %39, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %860 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%859, %40), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %861 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%860, %38, %45, %39, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %862 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%861, %37, %45, %39, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %863 : int[] = prim::ListConstruct(%853, %854, %37, %856, %857), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %hidden_states.135 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%862, %863, %42), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %865 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.19, %28), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %866 : int = aten::Int(%865), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %867 : int[] = prim::ListConstruct(%853, %866, %856, %857), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %value.9 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.135, %867), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %869 : int = aten::size(%key.9, %40), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %870 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %871 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%870, %47, %45, %39, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %872 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%871, %40, %45, %39, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.15 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%872, %38, %45, %869, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.17 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.9, %key.9, %value.9, %attention_mask.15, %27, %42, %26, %42), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %875 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.17, %47, %40), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.19 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%875, %45), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %877 : int[] = prim::ListConstruct(%786, %787, %35), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn
  %878 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.19, %877), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %879 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%878, %45), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.83 : Tensor = prim::GetAttr[name="weight"](%o_proj.9)
  %hidden_states.137 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%879, %weight.83), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn/__module.model.layers.4.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.139 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%781, %hidden_states.137, %47), scope: __module.model/__module.model.layers.4 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.85 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.9)
  %hidden_states.141 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.139, %33, %42, %42, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %885 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.141, %40), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %886 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm
  %variance.19 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%885, %886, %25, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %888 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.19, %24, %47), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %889 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%888), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.143 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.141, %889), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.145 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.143, %33, %42, %42, %44), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %892 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.85, %hidden_states.145), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %893 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%892, %hidden_states.141)
  %894 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %895 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%893)
  %down_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_53.Linear = prim::GetAttr[name="down_proj"](%mlp.9)
  %up_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_52.Linear = prim::GetAttr[name="up_proj"](%mlp.9)
  %gate_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_51.Linear = prim::GetAttr[name="gate_proj"](%mlp.9)
  %weight.87 : Tensor = prim::GetAttr[name="weight"](%gate_proj.9)
  %input.9 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%894, %weight.87), scope: __module.model/__module.model.layers.4/__module.model.layers.4.mlp/__module.model.layers.4.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %901 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.9), scope: __module.model/__module.model.layers.4/__module.model.layers.4.mlp/__module.model.layers.4.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.89 : Tensor = prim::GetAttr[name="weight"](%up_proj.9)
  %903 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%894, %weight.89), scope: __module.model/__module.model.layers.4/__module.model.layers.4.mlp/__module.model.layers.4.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %904 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%901, %903), scope: __module.model/__module.model.layers.4/__module.model.layers.4.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.91 : Tensor = prim::GetAttr[name="weight"](%down_proj.9)
  %hidden_states.147 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%904, %weight.91), scope: __module.model/__module.model.layers.4/__module.model.layers.4.mlp/__module.model.layers.4.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.149 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%895, %hidden_states.147, %47), scope: __module.model/__module.model.layers.4 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.11 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_68.MistralMLP = prim::GetAttr[name="mlp"](%_5)
  %post_attention_layernorm.11 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_70.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_5)
  %self_attn.11 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_63.MistralAttention = prim::GetAttr[name="self_attn"](%_5)
  %input_layernorm.11 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_69.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_5)
  %weight.93 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.11)
  %hidden_states.151 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.149, %33, %42, %42, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %914 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.151, %40), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %915 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm
  %variance.21 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%914, %915, %25, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %917 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.21, %24, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %918 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%917), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.153 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.151, %918), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.155 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.153, %33, %42, %42, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.157 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.93, %hidden_states.155), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %922 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.157, %hidden_states.151)
  %923 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %924 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%922)
  %o_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_62.Linear = prim::GetAttr[name="o_proj"](%self_attn.11)
  %v_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_61.Linear = prim::GetAttr[name="v_proj"](%self_attn.11)
  %k_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_60.Linear = prim::GetAttr[name="k_proj"](%self_attn.11)
  %q_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_59.Linear = prim::GetAttr[name="q_proj"](%self_attn.11)
  %929 : int = aten::size(%923, %45), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %930 : int = aten::size(%923, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.95 : Tensor = prim::GetAttr[name="weight"](%q_proj.11)
  %932 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%923, %weight.95), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn/__module.model.layers.5.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %933 : int[] = prim::ListConstruct(%929, %930, %35, %30), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %934 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%932, %933), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.11 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%934, %47, %40), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.97 : Tensor = prim::GetAttr[name="weight"](%k_proj.11)
  %937 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%923, %weight.97), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn/__module.model.layers.5.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %938 : int[] = prim::ListConstruct(%929, %930, %35, %30), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %939 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%937, %938), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.11 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%939, %47, %40), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.99 : Tensor = prim::GetAttr[name="weight"](%v_proj.11)
  %942 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%923, %weight.99), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn/__module.model.layers.5.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %943 : int[] = prim::ListConstruct(%929, %930, %35, %30), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %944 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%942, %943), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.163 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%944, %47, %40), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.15 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.15 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %948 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.11, %cos.15), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %949 : int = aten::size(%q.11, %38), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %950 : Long(device=cpu) = prim::NumToTensor(%949), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %951 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%950, %29), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %952 : int = aten::Int(%951), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %x1.21 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.11, %38, %45, %952, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %954 : int = aten::size(%q.11, %38), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %955 : Long(device=cpu) = prim::NumToTensor(%954), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %956 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%955, %29), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %957 : int = aten::Int(%956), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %x2.21 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.11, %38, %957, %39, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %959 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.21), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %960 : Tensor[] = prim::ListConstruct(%959, %x1.21), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %961 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%960, %35), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %962 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%961, %sin.15), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.11 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%948, %962, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %964 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.11, %cos.15), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %965 : int = aten::size(%k.11, %38), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %966 : Long(device=cpu) = prim::NumToTensor(%965), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %967 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%966, %29), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %968 : int = aten::Int(%967), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %x1.23 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.11, %38, %45, %968, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %970 : int = aten::size(%k.11, %38), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %971 : Long(device=cpu) = prim::NumToTensor(%970), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %972 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%971, %29), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %973 : int = aten::Int(%972), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %x2.23 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.11, %38, %973, %39, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %975 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.23), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %976 : Tensor[] = prim::ListConstruct(%975, %x1.23), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %977 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%976, %35), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %978 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%977, %sin.15), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.159 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%964, %978, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %980 : int = aten::size(%hidden_states.159, %45), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %981 : int = aten::size(%hidden_states.159, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.21 : Long(device=cpu) = prim::NumToTensor(%981), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %983 : int = aten::size(%hidden_states.159, %40), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %984 : int = aten::size(%hidden_states.159, %38), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %985 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.159, %45, %45, %39, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %986 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%985, %47, %45, %39, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %987 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%986, %40), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %988 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%987, %38, %45, %39, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %989 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%988, %37, %45, %39, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %990 : int[] = prim::ListConstruct(%980, %981, %37, %983, %984), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %hidden_states.161 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%989, %990, %42), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %992 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.21, %28), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %993 : int = aten::Int(%992), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %994 : int[] = prim::ListConstruct(%980, %993, %983, %984), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %key.11 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.161, %994), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %996 : int = aten::size(%hidden_states.163, %45), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %997 : int = aten::size(%hidden_states.163, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.23 : Long(device=cpu) = prim::NumToTensor(%997), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %999 : int = aten::size(%hidden_states.163, %40), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1000 : int = aten::size(%hidden_states.163, %38), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1001 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.163, %45, %45, %39, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1002 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1001, %47, %45, %39, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1003 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1002, %40), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1004 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1003, %38, %45, %39, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1005 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1004, %37, %45, %39, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1006 : int[] = prim::ListConstruct(%996, %997, %37, %999, %1000), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %hidden_states.165 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1005, %1006, %42), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1008 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.23, %28), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1009 : int = aten::Int(%1008), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %1010 : int[] = prim::ListConstruct(%996, %1009, %999, %1000), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %value.11 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.165, %1010), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1012 : int = aten::size(%key.11, %40), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1013 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1014 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1013, %47, %45, %39, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1015 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1014, %40, %45, %39, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.17 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1015, %38, %45, %1012, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.21 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.11, %key.11, %value.11, %attention_mask.17, %27, %42, %26, %42), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %1018 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.21, %47, %40), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.23 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1018, %45), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %1020 : int[] = prim::ListConstruct(%929, %930, %35), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn
  %1021 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.23, %1020), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %1022 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%1021, %45), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.101 : Tensor = prim::GetAttr[name="weight"](%o_proj.11)
  %hidden_states.167 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1022, %weight.101), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn/__module.model.layers.5.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.169 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%924, %hidden_states.167, %47), scope: __module.model/__module.model.layers.5 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.103 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.11)
  %hidden_states.171 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.169, %33, %42, %42, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1028 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.171, %40), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1029 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm
  %variance.23 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1028, %1029, %25, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1031 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.23, %24, %47), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1032 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1031), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.173 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.171, %1032), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.175 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.173, %33, %42, %42, %44), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1035 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.103, %hidden_states.175), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1036 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%1035, %hidden_states.171)
  %1037 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1038 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1036)
  %down_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_66.Linear = prim::GetAttr[name="down_proj"](%mlp.11)
  %up_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_65.Linear = prim::GetAttr[name="up_proj"](%mlp.11)
  %gate_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_64.Linear = prim::GetAttr[name="gate_proj"](%mlp.11)
  %weight.105 : Tensor = prim::GetAttr[name="weight"](%gate_proj.11)
  %input.11 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1037, %weight.105), scope: __module.model/__module.model.layers.5/__module.model.layers.5.mlp/__module.model.layers.5.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1044 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.11), scope: __module.model/__module.model.layers.5/__module.model.layers.5.mlp/__module.model.layers.5.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.107 : Tensor = prim::GetAttr[name="weight"](%up_proj.11)
  %1046 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1037, %weight.107), scope: __module.model/__module.model.layers.5/__module.model.layers.5.mlp/__module.model.layers.5.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1047 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%1044, %1046), scope: __module.model/__module.model.layers.5/__module.model.layers.5.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.109 : Tensor = prim::GetAttr[name="weight"](%down_proj.11)
  %hidden_states.177 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1047, %weight.109), scope: __module.model/__module.model.layers.5/__module.model.layers.5.mlp/__module.model.layers.5.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.179 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1038, %hidden_states.177, %47), scope: __module.model/__module.model.layers.5 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.13 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_81.MistralMLP = prim::GetAttr[name="mlp"](%_6)
  %post_attention_layernorm.13 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_83.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_6)
  %self_attn.13 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_76.MistralAttention = prim::GetAttr[name="self_attn"](%_6)
  %input_layernorm.13 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_82.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_6)
  %weight.111 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.13)
  %hidden_states.181 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.179, %33, %42, %42, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1057 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.181, %40), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1058 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm
  %variance.25 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1057, %1058, %25, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1060 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.25, %24, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1061 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1060), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.183 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.181, %1061), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.185 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.183, %33, %42, %42, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.187 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.111, %hidden_states.185), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1065 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.187, %hidden_states.181)
  %1066 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1067 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1065)
  %o_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_75.Linear = prim::GetAttr[name="o_proj"](%self_attn.13)
  %v_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_74.Linear = prim::GetAttr[name="v_proj"](%self_attn.13)
  %k_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_73.Linear = prim::GetAttr[name="k_proj"](%self_attn.13)
  %q_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_72.Linear = prim::GetAttr[name="q_proj"](%self_attn.13)
  %1072 : int = aten::size(%1066, %45), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %1073 : int = aten::size(%1066, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.113 : Tensor = prim::GetAttr[name="weight"](%q_proj.13)
  %1075 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1066, %weight.113), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn/__module.model.layers.6.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1076 : int[] = prim::ListConstruct(%1072, %1073, %35, %30), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1077 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%1075, %1076), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.13 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1077, %47, %40), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.115 : Tensor = prim::GetAttr[name="weight"](%k_proj.13)
  %1080 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1066, %weight.115), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn/__module.model.layers.6.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1081 : int[] = prim::ListConstruct(%1072, %1073, %35, %30), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1082 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1080, %1081), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.13 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1082, %47, %40), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.117 : Tensor = prim::GetAttr[name="weight"](%v_proj.13)
  %1085 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1066, %weight.117), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn/__module.model.layers.6.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1086 : int[] = prim::ListConstruct(%1072, %1073, %35, %30), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1087 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1085, %1086), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.193 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1087, %47, %40), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.17 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.17 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %1091 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.13, %cos.17), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1092 : int = aten::size(%q.13, %38), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1093 : Long(device=cpu) = prim::NumToTensor(%1092), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1094 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1093, %29), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1095 : int = aten::Int(%1094), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %x1.25 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.13, %38, %45, %1095, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1097 : int = aten::size(%q.13, %38), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1098 : Long(device=cpu) = prim::NumToTensor(%1097), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1099 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1098, %29), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1100 : int = aten::Int(%1099), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %x2.25 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.13, %38, %1100, %39, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1102 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.25), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1103 : Tensor[] = prim::ListConstruct(%1102, %x1.25), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1104 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1103, %35), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1105 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1104, %sin.17), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.13 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1091, %1105, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1107 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.13, %cos.17), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1108 : int = aten::size(%k.13, %38), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1109 : Long(device=cpu) = prim::NumToTensor(%1108), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1110 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1109, %29), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1111 : int = aten::Int(%1110), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %x1.27 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.13, %38, %45, %1111, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1113 : int = aten::size(%k.13, %38), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1114 : Long(device=cpu) = prim::NumToTensor(%1113), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1115 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1114, %29), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1116 : int = aten::Int(%1115), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %x2.27 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.13, %38, %1116, %39, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1118 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.27), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1119 : Tensor[] = prim::ListConstruct(%1118, %x1.27), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1120 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1119, %35), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1121 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1120, %sin.17), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.189 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1107, %1121, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1123 : int = aten::size(%hidden_states.189, %45), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1124 : int = aten::size(%hidden_states.189, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.25 : Long(device=cpu) = prim::NumToTensor(%1124), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1126 : int = aten::size(%hidden_states.189, %40), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1127 : int = aten::size(%hidden_states.189, %38), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1128 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.189, %45, %45, %39, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1129 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1128, %47, %45, %39, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1130 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1129, %40), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1131 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1130, %38, %45, %39, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1132 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1131, %37, %45, %39, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1133 : int[] = prim::ListConstruct(%1123, %1124, %37, %1126, %1127), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %hidden_states.191 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1132, %1133, %42), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1135 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.25, %28), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1136 : int = aten::Int(%1135), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1137 : int[] = prim::ListConstruct(%1123, %1136, %1126, %1127), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %key.13 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.191, %1137), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1139 : int = aten::size(%hidden_states.193, %45), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1140 : int = aten::size(%hidden_states.193, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.27 : Long(device=cpu) = prim::NumToTensor(%1140), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1142 : int = aten::size(%hidden_states.193, %40), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1143 : int = aten::size(%hidden_states.193, %38), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1144 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.193, %45, %45, %39, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1145 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1144, %47, %45, %39, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1146 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1145, %40), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1147 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1146, %38, %45, %39, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1148 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1147, %37, %45, %39, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1149 : int[] = prim::ListConstruct(%1139, %1140, %37, %1142, %1143), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %hidden_states.195 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1148, %1149, %42), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1151 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.27, %28), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1152 : int = aten::Int(%1151), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1153 : int[] = prim::ListConstruct(%1139, %1152, %1142, %1143), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %value.13 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.195, %1153), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1155 : int = aten::size(%key.13, %40), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1156 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1157 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1156, %47, %45, %39, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1158 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1157, %40, %45, %39, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.19 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1158, %38, %45, %1155, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.25 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.13, %key.13, %value.13, %attention_mask.19, %27, %42, %26, %42), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %1161 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.25, %47, %40), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.27 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1161, %45), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %1163 : int[] = prim::ListConstruct(%1072, %1073, %35), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn
  %1164 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.27, %1163), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %1165 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%1164, %45), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.119 : Tensor = prim::GetAttr[name="weight"](%o_proj.13)
  %hidden_states.197 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1165, %weight.119), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn/__module.model.layers.6.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.199 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1067, %hidden_states.197, %47), scope: __module.model/__module.model.layers.6 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.121 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.13)
  %hidden_states.201 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.199, %33, %42, %42, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1171 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.201, %40), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1172 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm
  %variance.27 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1171, %1172, %25, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1174 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.27, %24, %47), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1175 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1174), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.203 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.201, %1175), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.205 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.203, %33, %42, %42, %44), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1178 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.121, %hidden_states.205), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1179 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%1178, %hidden_states.201)
  %1180 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1181 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1179)
  %down_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_79.Linear = prim::GetAttr[name="down_proj"](%mlp.13)
  %up_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_78.Linear = prim::GetAttr[name="up_proj"](%mlp.13)
  %gate_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_77.Linear = prim::GetAttr[name="gate_proj"](%mlp.13)
  %weight.123 : Tensor = prim::GetAttr[name="weight"](%gate_proj.13)
  %input.13 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1180, %weight.123), scope: __module.model/__module.model.layers.6/__module.model.layers.6.mlp/__module.model.layers.6.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1187 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.13), scope: __module.model/__module.model.layers.6/__module.model.layers.6.mlp/__module.model.layers.6.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.125 : Tensor = prim::GetAttr[name="weight"](%up_proj.13)
  %1189 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1180, %weight.125), scope: __module.model/__module.model.layers.6/__module.model.layers.6.mlp/__module.model.layers.6.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1190 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%1187, %1189), scope: __module.model/__module.model.layers.6/__module.model.layers.6.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.127 : Tensor = prim::GetAttr[name="weight"](%down_proj.13)
  %hidden_states.207 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1190, %weight.127), scope: __module.model/__module.model.layers.6/__module.model.layers.6.mlp/__module.model.layers.6.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.209 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1181, %hidden_states.207, %47), scope: __module.model/__module.model.layers.6 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.15 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_94.MistralMLP = prim::GetAttr[name="mlp"](%_7)
  %post_attention_layernorm.15 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_96.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_7)
  %self_attn.15 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_89.MistralAttention = prim::GetAttr[name="self_attn"](%_7)
  %input_layernorm.15 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_95.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_7)
  %weight.129 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.15)
  %hidden_states.211 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.209, %33, %42, %42, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1200 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.211, %40), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1201 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm
  %variance.29 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1200, %1201, %25, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1203 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.29, %24, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1204 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1203), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.213 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.211, %1204), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.215 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.213, %33, %42, %42, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.217 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.129, %hidden_states.215), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1208 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.217, %hidden_states.211)
  %1209 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1210 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1208)
  %o_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_88.Linear = prim::GetAttr[name="o_proj"](%self_attn.15)
  %v_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_87.Linear = prim::GetAttr[name="v_proj"](%self_attn.15)
  %k_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_86.Linear = prim::GetAttr[name="k_proj"](%self_attn.15)
  %q_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_85.Linear = prim::GetAttr[name="q_proj"](%self_attn.15)
  %1215 : int = aten::size(%1209, %45), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %1216 : int = aten::size(%1209, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.131 : Tensor = prim::GetAttr[name="weight"](%q_proj.15)
  %1218 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1209, %weight.131), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn/__module.model.layers.7.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1219 : int[] = prim::ListConstruct(%1215, %1216, %35, %30), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1220 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%1218, %1219), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.15 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1220, %47, %40), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.133 : Tensor = prim::GetAttr[name="weight"](%k_proj.15)
  %1223 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1209, %weight.133), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn/__module.model.layers.7.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1224 : int[] = prim::ListConstruct(%1215, %1216, %35, %30), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1225 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1223, %1224), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.15 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1225, %47, %40), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.135 : Tensor = prim::GetAttr[name="weight"](%v_proj.15)
  %1228 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1209, %weight.135), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn/__module.model.layers.7.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1229 : int[] = prim::ListConstruct(%1215, %1216, %35, %30), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1230 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1228, %1229), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.223 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1230, %47, %40), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.19 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.19 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %1234 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.15, %cos.19), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1235 : int = aten::size(%q.15, %38), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1236 : Long(device=cpu) = prim::NumToTensor(%1235), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1237 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1236, %29), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1238 : int = aten::Int(%1237), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %x1.29 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.15, %38, %45, %1238, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1240 : int = aten::size(%q.15, %38), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1241 : Long(device=cpu) = prim::NumToTensor(%1240), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1242 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1241, %29), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1243 : int = aten::Int(%1242), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %x2.29 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.15, %38, %1243, %39, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1245 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.29), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1246 : Tensor[] = prim::ListConstruct(%1245, %x1.29), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1247 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1246, %35), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1248 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1247, %sin.19), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.15 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1234, %1248, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1250 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.15, %cos.19), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1251 : int = aten::size(%k.15, %38), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1252 : Long(device=cpu) = prim::NumToTensor(%1251), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1253 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1252, %29), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1254 : int = aten::Int(%1253), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %x1.31 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.15, %38, %45, %1254, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1256 : int = aten::size(%k.15, %38), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1257 : Long(device=cpu) = prim::NumToTensor(%1256), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1258 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1257, %29), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1259 : int = aten::Int(%1258), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %x2.31 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.15, %38, %1259, %39, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1261 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.31), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1262 : Tensor[] = prim::ListConstruct(%1261, %x1.31), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1263 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1262, %35), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1264 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1263, %sin.19), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.219 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1250, %1264, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1266 : int = aten::size(%hidden_states.219, %45), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1267 : int = aten::size(%hidden_states.219, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.29 : Long(device=cpu) = prim::NumToTensor(%1267), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1269 : int = aten::size(%hidden_states.219, %40), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1270 : int = aten::size(%hidden_states.219, %38), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1271 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.219, %45, %45, %39, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1272 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1271, %47, %45, %39, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1273 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1272, %40), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1274 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1273, %38, %45, %39, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1275 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1274, %37, %45, %39, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1276 : int[] = prim::ListConstruct(%1266, %1267, %37, %1269, %1270), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %hidden_states.221 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1275, %1276, %42), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1278 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.29, %28), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1279 : int = aten::Int(%1278), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1280 : int[] = prim::ListConstruct(%1266, %1279, %1269, %1270), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %key.15 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.221, %1280), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1282 : int = aten::size(%hidden_states.223, %45), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1283 : int = aten::size(%hidden_states.223, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.31 : Long(device=cpu) = prim::NumToTensor(%1283), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1285 : int = aten::size(%hidden_states.223, %40), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1286 : int = aten::size(%hidden_states.223, %38), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1287 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.223, %45, %45, %39, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1288 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1287, %47, %45, %39, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1289 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1288, %40), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1290 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1289, %38, %45, %39, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1291 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1290, %37, %45, %39, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1292 : int[] = prim::ListConstruct(%1282, %1283, %37, %1285, %1286), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %hidden_states.225 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1291, %1292, %42), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1294 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.31, %28), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1295 : int = aten::Int(%1294), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1296 : int[] = prim::ListConstruct(%1282, %1295, %1285, %1286), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %value.15 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.225, %1296), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1298 : int = aten::size(%key.15, %40), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1299 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1300 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1299, %47, %45, %39, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1301 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1300, %40, %45, %39, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.21 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1301, %38, %45, %1298, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.29 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.15, %key.15, %value.15, %attention_mask.21, %27, %42, %26, %42), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %1304 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.29, %47, %40), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.31 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1304, %45), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %1306 : int[] = prim::ListConstruct(%1215, %1216, %35), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn
  %1307 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.31, %1306), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %1308 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%1307, %45), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.137 : Tensor = prim::GetAttr[name="weight"](%o_proj.15)
  %hidden_states.227 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1308, %weight.137), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn/__module.model.layers.7.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.229 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1210, %hidden_states.227, %47), scope: __module.model/__module.model.layers.7 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.139 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.15)
  %hidden_states.231 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.229, %33, %42, %42, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1314 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.231, %40), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1315 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm
  %variance.31 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1314, %1315, %25, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1317 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.31, %24, %47), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1318 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1317), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.233 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.231, %1318), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.235 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.233, %33, %42, %42, %44), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1321 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.139, %hidden_states.235), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1322 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%1321, %hidden_states.231)
  %1323 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1324 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1322)
  %down_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_92.Linear = prim::GetAttr[name="down_proj"](%mlp.15)
  %up_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_91.Linear = prim::GetAttr[name="up_proj"](%mlp.15)
  %gate_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_90.Linear = prim::GetAttr[name="gate_proj"](%mlp.15)
  %weight.141 : Tensor = prim::GetAttr[name="weight"](%gate_proj.15)
  %input.15 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1323, %weight.141), scope: __module.model/__module.model.layers.7/__module.model.layers.7.mlp/__module.model.layers.7.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1330 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.15), scope: __module.model/__module.model.layers.7/__module.model.layers.7.mlp/__module.model.layers.7.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.143 : Tensor = prim::GetAttr[name="weight"](%up_proj.15)
  %1332 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1323, %weight.143), scope: __module.model/__module.model.layers.7/__module.model.layers.7.mlp/__module.model.layers.7.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1333 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%1330, %1332), scope: __module.model/__module.model.layers.7/__module.model.layers.7.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.145 : Tensor = prim::GetAttr[name="weight"](%down_proj.15)
  %hidden_states.237 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1333, %weight.145), scope: __module.model/__module.model.layers.7/__module.model.layers.7.mlp/__module.model.layers.7.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.239 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1324, %hidden_states.237, %47), scope: __module.model/__module.model.layers.7 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.17 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_107.MistralMLP = prim::GetAttr[name="mlp"](%_8)
  %post_attention_layernorm.17 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_109.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_8)
  %self_attn.17 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_102.MistralAttention = prim::GetAttr[name="self_attn"](%_8)
  %input_layernorm.17 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_108.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_8)
  %weight.147 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.17)
  %hidden_states.241 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.239, %33, %42, %42, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1343 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.241, %40), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1344 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm
  %variance.33 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1343, %1344, %25, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1346 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.33, %24, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1347 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1346), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.243 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.241, %1347), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.245 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.243, %33, %42, %42, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.247 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.147, %hidden_states.245), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1351 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.247, %hidden_states.241)
  %1352 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1353 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1351)
  %o_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_101.Linear = prim::GetAttr[name="o_proj"](%self_attn.17)
  %v_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_100.Linear = prim::GetAttr[name="v_proj"](%self_attn.17)
  %k_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_99.Linear = prim::GetAttr[name="k_proj"](%self_attn.17)
  %q_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_98.Linear = prim::GetAttr[name="q_proj"](%self_attn.17)
  %1358 : int = aten::size(%1352, %45), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %1359 : int = aten::size(%1352, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.149 : Tensor = prim::GetAttr[name="weight"](%q_proj.17)
  %1361 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1352, %weight.149), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn/__module.model.layers.8.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1362 : int[] = prim::ListConstruct(%1358, %1359, %35, %30), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1363 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%1361, %1362), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.17 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1363, %47, %40), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.151 : Tensor = prim::GetAttr[name="weight"](%k_proj.17)
  %1366 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1352, %weight.151), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn/__module.model.layers.8.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1367 : int[] = prim::ListConstruct(%1358, %1359, %35, %30), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1368 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1366, %1367), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.17 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1368, %47, %40), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.153 : Tensor = prim::GetAttr[name="weight"](%v_proj.17)
  %1371 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1352, %weight.153), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn/__module.model.layers.8.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1372 : int[] = prim::ListConstruct(%1358, %1359, %35, %30), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1373 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1371, %1372), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.253 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1373, %47, %40), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.21 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.21 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %1377 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.17, %cos.21), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1378 : int = aten::size(%q.17, %38), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1379 : Long(device=cpu) = prim::NumToTensor(%1378), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1380 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1379, %29), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1381 : int = aten::Int(%1380), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %x1.33 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.17, %38, %45, %1381, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1383 : int = aten::size(%q.17, %38), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1384 : Long(device=cpu) = prim::NumToTensor(%1383), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1385 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1384, %29), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1386 : int = aten::Int(%1385), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %x2.33 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.17, %38, %1386, %39, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1388 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.33), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1389 : Tensor[] = prim::ListConstruct(%1388, %x1.33), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1390 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1389, %35), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1391 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1390, %sin.21), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.17 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1377, %1391, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1393 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.17, %cos.21), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1394 : int = aten::size(%k.17, %38), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1395 : Long(device=cpu) = prim::NumToTensor(%1394), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1396 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1395, %29), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1397 : int = aten::Int(%1396), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %x1.35 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.17, %38, %45, %1397, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1399 : int = aten::size(%k.17, %38), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1400 : Long(device=cpu) = prim::NumToTensor(%1399), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1401 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1400, %29), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1402 : int = aten::Int(%1401), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %x2.35 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.17, %38, %1402, %39, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1404 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.35), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1405 : Tensor[] = prim::ListConstruct(%1404, %x1.35), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1406 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1405, %35), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1407 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1406, %sin.21), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.249 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1393, %1407, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1409 : int = aten::size(%hidden_states.249, %45), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1410 : int = aten::size(%hidden_states.249, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.33 : Long(device=cpu) = prim::NumToTensor(%1410), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1412 : int = aten::size(%hidden_states.249, %40), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1413 : int = aten::size(%hidden_states.249, %38), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1414 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.249, %45, %45, %39, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1415 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1414, %47, %45, %39, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1416 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1415, %40), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1417 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1416, %38, %45, %39, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1418 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1417, %37, %45, %39, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1419 : int[] = prim::ListConstruct(%1409, %1410, %37, %1412, %1413), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %hidden_states.251 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1418, %1419, %42), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1421 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.33, %28), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1422 : int = aten::Int(%1421), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1423 : int[] = prim::ListConstruct(%1409, %1422, %1412, %1413), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %key.17 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.251, %1423), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1425 : int = aten::size(%hidden_states.253, %45), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1426 : int = aten::size(%hidden_states.253, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.35 : Long(device=cpu) = prim::NumToTensor(%1426), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1428 : int = aten::size(%hidden_states.253, %40), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1429 : int = aten::size(%hidden_states.253, %38), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1430 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.253, %45, %45, %39, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1431 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1430, %47, %45, %39, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1432 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1431, %40), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1433 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1432, %38, %45, %39, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1434 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1433, %37, %45, %39, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1435 : int[] = prim::ListConstruct(%1425, %1426, %37, %1428, %1429), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %hidden_states.255 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1434, %1435, %42), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1437 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.35, %28), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1438 : int = aten::Int(%1437), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1439 : int[] = prim::ListConstruct(%1425, %1438, %1428, %1429), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %value.17 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.255, %1439), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1441 : int = aten::size(%key.17, %40), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1442 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1443 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1442, %47, %45, %39, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1444 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1443, %40, %45, %39, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.23 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1444, %38, %45, %1441, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.33 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.17, %key.17, %value.17, %attention_mask.23, %27, %42, %26, %42), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %1447 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.33, %47, %40), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.35 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1447, %45), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %1449 : int[] = prim::ListConstruct(%1358, %1359, %35), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn
  %1450 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.35, %1449), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %1451 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%1450, %45), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.155 : Tensor = prim::GetAttr[name="weight"](%o_proj.17)
  %hidden_states.257 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1451, %weight.155), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn/__module.model.layers.8.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.259 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1353, %hidden_states.257, %47), scope: __module.model/__module.model.layers.8 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.157 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.17)
  %hidden_states.261 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.259, %33, %42, %42, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1457 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.261, %40), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1458 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm
  %variance.35 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1457, %1458, %25, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1460 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.35, %24, %47), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1461 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1460), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.263 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.261, %1461), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.265 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.263, %33, %42, %42, %44), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1464 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.157, %hidden_states.265), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1465 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%1464, %hidden_states.261)
  %1466 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1467 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1465)
  %down_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_105.Linear = prim::GetAttr[name="down_proj"](%mlp.17)
  %up_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_104.Linear = prim::GetAttr[name="up_proj"](%mlp.17)
  %gate_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_103.Linear = prim::GetAttr[name="gate_proj"](%mlp.17)
  %weight.159 : Tensor = prim::GetAttr[name="weight"](%gate_proj.17)
  %input.17 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1466, %weight.159), scope: __module.model/__module.model.layers.8/__module.model.layers.8.mlp/__module.model.layers.8.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1473 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.17), scope: __module.model/__module.model.layers.8/__module.model.layers.8.mlp/__module.model.layers.8.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.161 : Tensor = prim::GetAttr[name="weight"](%up_proj.17)
  %1475 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1466, %weight.161), scope: __module.model/__module.model.layers.8/__module.model.layers.8.mlp/__module.model.layers.8.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1476 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%1473, %1475), scope: __module.model/__module.model.layers.8/__module.model.layers.8.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.163 : Tensor = prim::GetAttr[name="weight"](%down_proj.17)
  %hidden_states.267 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1476, %weight.163), scope: __module.model/__module.model.layers.8/__module.model.layers.8.mlp/__module.model.layers.8.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.269 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1467, %hidden_states.267, %47), scope: __module.model/__module.model.layers.8 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.19 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_120.MistralMLP = prim::GetAttr[name="mlp"](%_9)
  %post_attention_layernorm.19 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_122.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_9)
  %self_attn.19 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_115.MistralAttention = prim::GetAttr[name="self_attn"](%_9)
  %input_layernorm.19 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_121.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_9)
  %weight.165 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.19)
  %hidden_states.271 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.269, %33, %42, %42, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1486 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.271, %40), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1487 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm
  %variance.37 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1486, %1487, %25, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1489 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.37, %24, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1490 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1489), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.273 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.271, %1490), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.275 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.273, %33, %42, %42, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.277 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.165, %hidden_states.275), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1494 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.277, %hidden_states.271)
  %1495 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1496 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1494)
  %o_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_114.Linear = prim::GetAttr[name="o_proj"](%self_attn.19)
  %v_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_113.Linear = prim::GetAttr[name="v_proj"](%self_attn.19)
  %k_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_112.Linear = prim::GetAttr[name="k_proj"](%self_attn.19)
  %q_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_111.Linear = prim::GetAttr[name="q_proj"](%self_attn.19)
  %1501 : int = aten::size(%1495, %45), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %1502 : int = aten::size(%1495, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.167 : Tensor = prim::GetAttr[name="weight"](%q_proj.19)
  %1504 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1495, %weight.167), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn/__module.model.layers.9.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1505 : int[] = prim::ListConstruct(%1501, %1502, %35, %30), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1506 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%1504, %1505), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.19 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1506, %47, %40), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.169 : Tensor = prim::GetAttr[name="weight"](%k_proj.19)
  %1509 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1495, %weight.169), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn/__module.model.layers.9.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1510 : int[] = prim::ListConstruct(%1501, %1502, %35, %30), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1511 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1509, %1510), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.19 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1511, %47, %40), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.171 : Tensor = prim::GetAttr[name="weight"](%v_proj.19)
  %1514 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1495, %weight.171), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn/__module.model.layers.9.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1515 : int[] = prim::ListConstruct(%1501, %1502, %35, %30), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1516 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1514, %1515), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.283 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1516, %47, %40), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.23 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.23 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %1520 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.19, %cos.23), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1521 : int = aten::size(%q.19, %38), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1522 : Long(device=cpu) = prim::NumToTensor(%1521), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1523 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1522, %29), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1524 : int = aten::Int(%1523), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %x1.37 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.19, %38, %45, %1524, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1526 : int = aten::size(%q.19, %38), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1527 : Long(device=cpu) = prim::NumToTensor(%1526), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1528 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1527, %29), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1529 : int = aten::Int(%1528), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %x2.37 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.19, %38, %1529, %39, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1531 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.37), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1532 : Tensor[] = prim::ListConstruct(%1531, %x1.37), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1533 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1532, %35), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1534 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1533, %sin.23), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.19 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1520, %1534, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1536 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.19, %cos.23), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1537 : int = aten::size(%k.19, %38), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1538 : Long(device=cpu) = prim::NumToTensor(%1537), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1539 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1538, %29), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1540 : int = aten::Int(%1539), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %x1.39 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.19, %38, %45, %1540, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1542 : int = aten::size(%k.19, %38), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1543 : Long(device=cpu) = prim::NumToTensor(%1542), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1544 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1543, %29), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1545 : int = aten::Int(%1544), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %x2.39 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.19, %38, %1545, %39, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1547 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.39), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1548 : Tensor[] = prim::ListConstruct(%1547, %x1.39), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1549 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1548, %35), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1550 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1549, %sin.23), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.279 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1536, %1550, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1552 : int = aten::size(%hidden_states.279, %45), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1553 : int = aten::size(%hidden_states.279, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.37 : Long(device=cpu) = prim::NumToTensor(%1553), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1555 : int = aten::size(%hidden_states.279, %40), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1556 : int = aten::size(%hidden_states.279, %38), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1557 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.279, %45, %45, %39, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1558 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1557, %47, %45, %39, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1559 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1558, %40), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1560 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1559, %38, %45, %39, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1561 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1560, %37, %45, %39, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1562 : int[] = prim::ListConstruct(%1552, %1553, %37, %1555, %1556), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %hidden_states.281 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1561, %1562, %42), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1564 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.37, %28), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1565 : int = aten::Int(%1564), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1566 : int[] = prim::ListConstruct(%1552, %1565, %1555, %1556), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %key.19 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.281, %1566), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1568 : int = aten::size(%hidden_states.283, %45), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1569 : int = aten::size(%hidden_states.283, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.39 : Long(device=cpu) = prim::NumToTensor(%1569), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1571 : int = aten::size(%hidden_states.283, %40), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1572 : int = aten::size(%hidden_states.283, %38), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1573 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.283, %45, %45, %39, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1574 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1573, %47, %45, %39, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1575 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1574, %40), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1576 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1575, %38, %45, %39, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1577 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1576, %37, %45, %39, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1578 : int[] = prim::ListConstruct(%1568, %1569, %37, %1571, %1572), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %hidden_states.285 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1577, %1578, %42), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1580 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.39, %28), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1581 : int = aten::Int(%1580), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1582 : int[] = prim::ListConstruct(%1568, %1581, %1571, %1572), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %value.19 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.285, %1582), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1584 : int = aten::size(%key.19, %40), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1585 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1586 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1585, %47, %45, %39, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1587 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1586, %40, %45, %39, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.25 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1587, %38, %45, %1584, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.37 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.19, %key.19, %value.19, %attention_mask.25, %27, %42, %26, %42), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %1590 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.37, %47, %40), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.39 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1590, %45), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %1592 : int[] = prim::ListConstruct(%1501, %1502, %35), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn
  %1593 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.39, %1592), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %1594 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%1593, %45), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.173 : Tensor = prim::GetAttr[name="weight"](%o_proj.19)
  %hidden_states.287 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1594, %weight.173), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn/__module.model.layers.9.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.289 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1496, %hidden_states.287, %47), scope: __module.model/__module.model.layers.9 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.175 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.19)
  %hidden_states.291 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.289, %33, %42, %42, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1600 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.291, %40), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1601 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm
  %variance.39 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1600, %1601, %25, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1603 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.39, %24, %47), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1604 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1603), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.293 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.291, %1604), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.295 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.293, %33, %42, %42, %44), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1607 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.175, %hidden_states.295), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1608 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%1607, %hidden_states.291)
  %1609 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1610 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1608)
  %down_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_118.Linear = prim::GetAttr[name="down_proj"](%mlp.19)
  %up_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_117.Linear = prim::GetAttr[name="up_proj"](%mlp.19)
  %gate_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_116.Linear = prim::GetAttr[name="gate_proj"](%mlp.19)
  %weight.177 : Tensor = prim::GetAttr[name="weight"](%gate_proj.19)
  %input.19 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1609, %weight.177), scope: __module.model/__module.model.layers.9/__module.model.layers.9.mlp/__module.model.layers.9.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1616 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.19), scope: __module.model/__module.model.layers.9/__module.model.layers.9.mlp/__module.model.layers.9.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.179 : Tensor = prim::GetAttr[name="weight"](%up_proj.19)
  %1618 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1609, %weight.179), scope: __module.model/__module.model.layers.9/__module.model.layers.9.mlp/__module.model.layers.9.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1619 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%1616, %1618), scope: __module.model/__module.model.layers.9/__module.model.layers.9.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.181 : Tensor = prim::GetAttr[name="weight"](%down_proj.19)
  %hidden_states.297 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1619, %weight.181), scope: __module.model/__module.model.layers.9/__module.model.layers.9.mlp/__module.model.layers.9.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.299 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1610, %hidden_states.297, %47), scope: __module.model/__module.model.layers.9 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.21 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_133.MistralMLP = prim::GetAttr[name="mlp"](%_10)
  %post_attention_layernorm.21 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_135.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_10)
  %self_attn.21 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_128.MistralAttention = prim::GetAttr[name="self_attn"](%_10)
  %input_layernorm.21 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_134.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_10)
  %weight.183 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.21)
  %hidden_states.301 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.299, %33, %42, %42, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1629 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.301, %40), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1630 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm
  %variance.41 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1629, %1630, %25, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1632 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.41, %24, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1633 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1632), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.303 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.301, %1633), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.305 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.303, %33, %42, %42, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.307 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.183, %hidden_states.305), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1637 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.307, %hidden_states.301)
  %1638 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1639 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1637)
  %o_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_127.Linear = prim::GetAttr[name="o_proj"](%self_attn.21)
  %v_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_126.Linear = prim::GetAttr[name="v_proj"](%self_attn.21)
  %k_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_125.Linear = prim::GetAttr[name="k_proj"](%self_attn.21)
  %q_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_124.Linear = prim::GetAttr[name="q_proj"](%self_attn.21)
  %1644 : int = aten::size(%1638, %45), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %1645 : int = aten::size(%1638, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.185 : Tensor = prim::GetAttr[name="weight"](%q_proj.21)
  %1647 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1638, %weight.185), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn/__module.model.layers.10.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1648 : int[] = prim::ListConstruct(%1644, %1645, %35, %30), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1649 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%1647, %1648), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.21 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1649, %47, %40), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.187 : Tensor = prim::GetAttr[name="weight"](%k_proj.21)
  %1652 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1638, %weight.187), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn/__module.model.layers.10.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1653 : int[] = prim::ListConstruct(%1644, %1645, %35, %30), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1654 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1652, %1653), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.21 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1654, %47, %40), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.189 : Tensor = prim::GetAttr[name="weight"](%v_proj.21)
  %1657 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1638, %weight.189), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn/__module.model.layers.10.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1658 : int[] = prim::ListConstruct(%1644, %1645, %35, %30), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1659 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1657, %1658), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.313 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1659, %47, %40), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.25 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.25 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %1663 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.21, %cos.25), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1664 : int = aten::size(%q.21, %38), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1665 : Long(device=cpu) = prim::NumToTensor(%1664), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1666 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1665, %29), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1667 : int = aten::Int(%1666), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %x1.41 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.21, %38, %45, %1667, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1669 : int = aten::size(%q.21, %38), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1670 : Long(device=cpu) = prim::NumToTensor(%1669), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1671 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1670, %29), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1672 : int = aten::Int(%1671), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %x2.41 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.21, %38, %1672, %39, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1674 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.41), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1675 : Tensor[] = prim::ListConstruct(%1674, %x1.41), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1676 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1675, %35), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1677 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1676, %sin.25), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.21 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1663, %1677, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1679 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.21, %cos.25), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1680 : int = aten::size(%k.21, %38), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1681 : Long(device=cpu) = prim::NumToTensor(%1680), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1682 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1681, %29), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1683 : int = aten::Int(%1682), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %x1.43 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.21, %38, %45, %1683, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1685 : int = aten::size(%k.21, %38), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1686 : Long(device=cpu) = prim::NumToTensor(%1685), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1687 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1686, %29), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1688 : int = aten::Int(%1687), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %x2.43 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.21, %38, %1688, %39, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1690 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.43), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1691 : Tensor[] = prim::ListConstruct(%1690, %x1.43), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1692 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1691, %35), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1693 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1692, %sin.25), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.309 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1679, %1693, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1695 : int = aten::size(%hidden_states.309, %45), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1696 : int = aten::size(%hidden_states.309, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.41 : Long(device=cpu) = prim::NumToTensor(%1696), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1698 : int = aten::size(%hidden_states.309, %40), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1699 : int = aten::size(%hidden_states.309, %38), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1700 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.309, %45, %45, %39, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1701 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1700, %47, %45, %39, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1702 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1701, %40), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1703 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1702, %38, %45, %39, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1704 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1703, %37, %45, %39, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1705 : int[] = prim::ListConstruct(%1695, %1696, %37, %1698, %1699), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %hidden_states.311 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1704, %1705, %42), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1707 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.41, %28), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1708 : int = aten::Int(%1707), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1709 : int[] = prim::ListConstruct(%1695, %1708, %1698, %1699), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %key.21 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.311, %1709), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1711 : int = aten::size(%hidden_states.313, %45), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1712 : int = aten::size(%hidden_states.313, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.43 : Long(device=cpu) = prim::NumToTensor(%1712), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1714 : int = aten::size(%hidden_states.313, %40), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1715 : int = aten::size(%hidden_states.313, %38), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1716 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.313, %45, %45, %39, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1717 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1716, %47, %45, %39, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1718 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1717, %40), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1719 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1718, %38, %45, %39, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1720 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1719, %37, %45, %39, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1721 : int[] = prim::ListConstruct(%1711, %1712, %37, %1714, %1715), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %hidden_states.315 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1720, %1721, %42), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1723 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.43, %28), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1724 : int = aten::Int(%1723), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1725 : int[] = prim::ListConstruct(%1711, %1724, %1714, %1715), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %value.21 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.315, %1725), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1727 : int = aten::size(%key.21, %40), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1728 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1729 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1728, %47, %45, %39, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1730 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1729, %40, %45, %39, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.27 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1730, %38, %45, %1727, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.41 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.21, %key.21, %value.21, %attention_mask.27, %27, %42, %26, %42), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %1733 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.41, %47, %40), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.43 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1733, %45), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %1735 : int[] = prim::ListConstruct(%1644, %1645, %35), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn
  %1736 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.43, %1735), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %1737 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%1736, %45), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.191 : Tensor = prim::GetAttr[name="weight"](%o_proj.21)
  %hidden_states.317 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1737, %weight.191), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn/__module.model.layers.10.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.319 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1639, %hidden_states.317, %47), scope: __module.model/__module.model.layers.10 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.193 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.21)
  %hidden_states.321 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.319, %33, %42, %42, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1743 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.321, %40), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1744 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm
  %variance.43 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1743, %1744, %25, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1746 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.43, %24, %47), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1747 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1746), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.323 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.321, %1747), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.325 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.323, %33, %42, %42, %44), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1750 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.193, %hidden_states.325), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1751 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%1750, %hidden_states.321)
  %1752 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1753 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1751)
  %down_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_131.Linear = prim::GetAttr[name="down_proj"](%mlp.21)
  %up_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_130.Linear = prim::GetAttr[name="up_proj"](%mlp.21)
  %gate_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_129.Linear = prim::GetAttr[name="gate_proj"](%mlp.21)
  %weight.195 : Tensor = prim::GetAttr[name="weight"](%gate_proj.21)
  %input.21 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1752, %weight.195), scope: __module.model/__module.model.layers.10/__module.model.layers.10.mlp/__module.model.layers.10.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1759 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.21), scope: __module.model/__module.model.layers.10/__module.model.layers.10.mlp/__module.model.layers.10.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.197 : Tensor = prim::GetAttr[name="weight"](%up_proj.21)
  %1761 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1752, %weight.197), scope: __module.model/__module.model.layers.10/__module.model.layers.10.mlp/__module.model.layers.10.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1762 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%1759, %1761), scope: __module.model/__module.model.layers.10/__module.model.layers.10.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.199 : Tensor = prim::GetAttr[name="weight"](%down_proj.21)
  %hidden_states.327 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1762, %weight.199), scope: __module.model/__module.model.layers.10/__module.model.layers.10.mlp/__module.model.layers.10.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.329 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1753, %hidden_states.327, %47), scope: __module.model/__module.model.layers.10 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.23 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_146.MistralMLP = prim::GetAttr[name="mlp"](%_11)
  %post_attention_layernorm.23 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_148.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_11)
  %self_attn.23 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_141.MistralAttention = prim::GetAttr[name="self_attn"](%_11)
  %input_layernorm.23 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_147.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_11)
  %weight.201 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.23)
  %hidden_states.331 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.329, %33, %42, %42, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1772 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.331, %40), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1773 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm
  %variance.45 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1772, %1773, %25, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1775 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.45, %24, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1776 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1775), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.333 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.331, %1776), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.335 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.333, %33, %42, %42, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.337 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.201, %hidden_states.335), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1780 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.337, %hidden_states.331)
  %1781 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1782 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1780)
  %o_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_140.Linear = prim::GetAttr[name="o_proj"](%self_attn.23)
  %v_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_139.Linear = prim::GetAttr[name="v_proj"](%self_attn.23)
  %k_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_138.Linear = prim::GetAttr[name="k_proj"](%self_attn.23)
  %q_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_137.Linear = prim::GetAttr[name="q_proj"](%self_attn.23)
  %1787 : int = aten::size(%1781, %45), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %1788 : int = aten::size(%1781, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.203 : Tensor = prim::GetAttr[name="weight"](%q_proj.23)
  %1790 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1781, %weight.203), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn/__module.model.layers.11.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1791 : int[] = prim::ListConstruct(%1787, %1788, %35, %30), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1792 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%1790, %1791), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.23 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1792, %47, %40), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.205 : Tensor = prim::GetAttr[name="weight"](%k_proj.23)
  %1795 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1781, %weight.205), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn/__module.model.layers.11.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1796 : int[] = prim::ListConstruct(%1787, %1788, %35, %30), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1797 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1795, %1796), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.23 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1797, %47, %40), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.207 : Tensor = prim::GetAttr[name="weight"](%v_proj.23)
  %1800 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1781, %weight.207), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn/__module.model.layers.11.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1801 : int[] = prim::ListConstruct(%1787, %1788, %35, %30), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1802 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1800, %1801), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.343 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1802, %47, %40), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.27 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.27 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %1806 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.23, %cos.27), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1807 : int = aten::size(%q.23, %38), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1808 : Long(device=cpu) = prim::NumToTensor(%1807), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1809 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1808, %29), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1810 : int = aten::Int(%1809), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %x1.45 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.23, %38, %45, %1810, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1812 : int = aten::size(%q.23, %38), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1813 : Long(device=cpu) = prim::NumToTensor(%1812), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1814 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1813, %29), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1815 : int = aten::Int(%1814), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %x2.45 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.23, %38, %1815, %39, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1817 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.45), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1818 : Tensor[] = prim::ListConstruct(%1817, %x1.45), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1819 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1818, %35), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1820 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1819, %sin.27), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.23 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1806, %1820, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1822 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.23, %cos.27), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1823 : int = aten::size(%k.23, %38), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1824 : Long(device=cpu) = prim::NumToTensor(%1823), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1825 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1824, %29), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1826 : int = aten::Int(%1825), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %x1.47 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.23, %38, %45, %1826, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1828 : int = aten::size(%k.23, %38), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1829 : Long(device=cpu) = prim::NumToTensor(%1828), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1830 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1829, %29), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1831 : int = aten::Int(%1830), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %x2.47 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.23, %38, %1831, %39, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1833 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1834 : Tensor[] = prim::ListConstruct(%1833, %x1.47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1835 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1834, %35), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1836 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1835, %sin.27), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.339 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1822, %1836, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1838 : int = aten::size(%hidden_states.339, %45), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1839 : int = aten::size(%hidden_states.339, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.45 : Long(device=cpu) = prim::NumToTensor(%1839), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1841 : int = aten::size(%hidden_states.339, %40), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1842 : int = aten::size(%hidden_states.339, %38), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1843 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.339, %45, %45, %39, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1844 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1843, %47, %45, %39, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1845 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1844, %40), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1846 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1845, %38, %45, %39, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1847 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1846, %37, %45, %39, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1848 : int[] = prim::ListConstruct(%1838, %1839, %37, %1841, %1842), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %hidden_states.341 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1847, %1848, %42), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1850 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.45, %28), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1851 : int = aten::Int(%1850), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1852 : int[] = prim::ListConstruct(%1838, %1851, %1841, %1842), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %key.23 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.341, %1852), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1854 : int = aten::size(%hidden_states.343, %45), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1855 : int = aten::size(%hidden_states.343, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.47 : Long(device=cpu) = prim::NumToTensor(%1855), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1857 : int = aten::size(%hidden_states.343, %40), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1858 : int = aten::size(%hidden_states.343, %38), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1859 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.343, %45, %45, %39, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1860 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1859, %47, %45, %39, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1861 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1860, %40), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1862 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1861, %38, %45, %39, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1863 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1862, %37, %45, %39, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1864 : int[] = prim::ListConstruct(%1854, %1855, %37, %1857, %1858), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %hidden_states.345 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1863, %1864, %42), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1866 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.47, %28), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1867 : int = aten::Int(%1866), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1868 : int[] = prim::ListConstruct(%1854, %1867, %1857, %1858), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %value.23 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.345, %1868), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1870 : int = aten::size(%key.23, %40), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1871 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1872 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1871, %47, %45, %39, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %1873 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1872, %40, %45, %39, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.29 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%1873, %38, %45, %1870, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.45 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.23, %key.23, %value.23, %attention_mask.29, %27, %42, %26, %42), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %1876 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.45, %47, %40), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.47 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1876, %45), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %1878 : int[] = prim::ListConstruct(%1787, %1788, %35), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn
  %1879 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.47, %1878), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %1880 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%1879, %45), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.209 : Tensor = prim::GetAttr[name="weight"](%o_proj.23)
  %hidden_states.347 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1880, %weight.209), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn/__module.model.layers.11.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.349 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1782, %hidden_states.347, %47), scope: __module.model/__module.model.layers.11 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.211 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.23)
  %hidden_states.351 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.349, %33, %42, %42, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1886 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.351, %40), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1887 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm
  %variance.47 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1886, %1887, %25, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1889 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.47, %24, %47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1890 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1889), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.353 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.351, %1890), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.355 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.353, %33, %42, %42, %44), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1893 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.211, %hidden_states.355), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1894 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%1893, %hidden_states.351)
  %1895 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1896 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1894)
  %down_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_144.Linear = prim::GetAttr[name="down_proj"](%mlp.23)
  %up_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_143.Linear = prim::GetAttr[name="up_proj"](%mlp.23)
  %gate_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_142.Linear = prim::GetAttr[name="gate_proj"](%mlp.23)
  %weight.213 : Tensor = prim::GetAttr[name="weight"](%gate_proj.23)
  %input.23 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1895, %weight.213), scope: __module.model/__module.model.layers.11/__module.model.layers.11.mlp/__module.model.layers.11.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1902 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.23), scope: __module.model/__module.model.layers.11/__module.model.layers.11.mlp/__module.model.layers.11.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.215 : Tensor = prim::GetAttr[name="weight"](%up_proj.23)
  %1904 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1895, %weight.215), scope: __module.model/__module.model.layers.11/__module.model.layers.11.mlp/__module.model.layers.11.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1905 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%1902, %1904), scope: __module.model/__module.model.layers.11/__module.model.layers.11.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.217 : Tensor = prim::GetAttr[name="weight"](%down_proj.23)
  %hidden_states.357 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1905, %weight.217), scope: __module.model/__module.model.layers.11/__module.model.layers.11.mlp/__module.model.layers.11.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.359 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1896, %hidden_states.357, %47), scope: __module.model/__module.model.layers.11 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.25 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_159.MistralMLP = prim::GetAttr[name="mlp"](%_12)
  %post_attention_layernorm.25 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_161.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_12)
  %self_attn.25 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_154.MistralAttention = prim::GetAttr[name="self_attn"](%_12)
  %input_layernorm.25 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_160.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_12)
  %weight.219 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.25)
  %hidden_states.361 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.359, %33, %42, %42, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %1915 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.361, %40), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1916 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm
  %variance.49 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1915, %1916, %25, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %1918 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.49, %24, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %1919 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1918), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.363 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.361, %1919), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.365 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.363, %33, %42, %42, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.367 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.219, %hidden_states.365), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %1923 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.367, %hidden_states.361)
  %1924 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1925 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1923)
  %o_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_153.Linear = prim::GetAttr[name="o_proj"](%self_attn.25)
  %v_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_152.Linear = prim::GetAttr[name="v_proj"](%self_attn.25)
  %k_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_151.Linear = prim::GetAttr[name="k_proj"](%self_attn.25)
  %q_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_150.Linear = prim::GetAttr[name="q_proj"](%self_attn.25)
  %1930 : int = aten::size(%1924, %45), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %1931 : int = aten::size(%1924, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.221 : Tensor = prim::GetAttr[name="weight"](%q_proj.25)
  %1933 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1924, %weight.221), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn/__module.model.layers.12.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1934 : int[] = prim::ListConstruct(%1930, %1931, %35, %30), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %1935 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%1933, %1934), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.25 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1935, %47, %40), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.223 : Tensor = prim::GetAttr[name="weight"](%k_proj.25)
  %1938 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1924, %weight.223), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn/__module.model.layers.12.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1939 : int[] = prim::ListConstruct(%1930, %1931, %35, %30), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %1940 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1938, %1939), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.25 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1940, %47, %40), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.225 : Tensor = prim::GetAttr[name="weight"](%v_proj.25)
  %1943 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%1924, %weight.225), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn/__module.model.layers.12.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %1944 : int[] = prim::ListConstruct(%1930, %1931, %35, %30), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %1945 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%1943, %1944), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.373 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1945, %47, %40), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.29 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.29 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %1949 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.25, %cos.29), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1950 : int = aten::size(%q.25, %38), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1951 : Long(device=cpu) = prim::NumToTensor(%1950), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %1952 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1951, %29), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1953 : int = aten::Int(%1952), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %x1.49 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.25, %38, %45, %1953, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1955 : int = aten::size(%q.25, %38), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1956 : Long(device=cpu) = prim::NumToTensor(%1955), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %1957 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1956, %29), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1958 : int = aten::Int(%1957), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %x2.49 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.25, %38, %1958, %39, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1960 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.49), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1961 : Tensor[] = prim::ListConstruct(%1960, %x1.49), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %1962 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1961, %35), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1963 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1962, %sin.29), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.25 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1949, %1963, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %1965 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.25, %cos.29), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1966 : int = aten::size(%k.25, %38), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1967 : Long(device=cpu) = prim::NumToTensor(%1966), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %1968 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1967, %29), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1969 : int = aten::Int(%1968), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %x1.51 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.25, %38, %45, %1969, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %1971 : int = aten::size(%k.25, %38), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1972 : Long(device=cpu) = prim::NumToTensor(%1971), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %1973 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1972, %29), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %1974 : int = aten::Int(%1973), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %x2.51 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.25, %38, %1974, %39, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %1976 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.51), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1977 : Tensor[] = prim::ListConstruct(%1976, %x1.51), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %1978 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1977, %35), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %1979 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1978, %sin.29), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.369 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1965, %1979, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %1981 : int = aten::size(%hidden_states.369, %45), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1982 : int = aten::size(%hidden_states.369, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.49 : Long(device=cpu) = prim::NumToTensor(%1982), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %1984 : int = aten::size(%hidden_states.369, %40), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1985 : int = aten::size(%hidden_states.369, %38), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1986 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.369, %45, %45, %39, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1987 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1986, %47, %45, %39, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1988 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1987, %40), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1989 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1988, %38, %45, %39, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1990 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%1989, %37, %45, %39, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1991 : int[] = prim::ListConstruct(%1981, %1982, %37, %1984, %1985), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %hidden_states.371 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%1990, %1991, %42), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %1993 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.49, %28), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1994 : int = aten::Int(%1993), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %1995 : int[] = prim::ListConstruct(%1981, %1994, %1984, %1985), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %key.25 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.371, %1995), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %1997 : int = aten::size(%hidden_states.373, %45), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %1998 : int = aten::size(%hidden_states.373, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.51 : Long(device=cpu) = prim::NumToTensor(%1998), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2000 : int = aten::size(%hidden_states.373, %40), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2001 : int = aten::size(%hidden_states.373, %38), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2002 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.373, %45, %45, %39, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2003 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2002, %47, %45, %39, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2004 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2003, %40), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2005 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2004, %38, %45, %39, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2006 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2005, %37, %45, %39, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2007 : int[] = prim::ListConstruct(%1997, %1998, %37, %2000, %2001), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %hidden_states.375 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2006, %2007, %42), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2009 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.51, %28), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2010 : int = aten::Int(%2009), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2011 : int[] = prim::ListConstruct(%1997, %2010, %2000, %2001), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %value.25 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.375, %2011), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2013 : int = aten::size(%key.25, %40), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2014 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2015 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2014, %47, %45, %39, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2016 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2015, %40, %45, %39, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.31 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2016, %38, %45, %2013, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.49 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.25, %key.25, %value.25, %attention_mask.31, %27, %42, %26, %42), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %2019 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.49, %47, %40), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.51 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2019, %45), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %2021 : int[] = prim::ListConstruct(%1930, %1931, %35), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn
  %2022 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.51, %2021), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %2023 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%2022, %45), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.227 : Tensor = prim::GetAttr[name="weight"](%o_proj.25)
  %hidden_states.377 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2023, %weight.227), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn/__module.model.layers.12.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.379 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1925, %hidden_states.377, %47), scope: __module.model/__module.model.layers.12 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.229 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.25)
  %hidden_states.381 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.379, %33, %42, %42, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2029 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.381, %40), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2030 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm
  %variance.51 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2029, %2030, %25, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2032 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.51, %24, %47), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2033 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2032), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.383 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.381, %2033), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.385 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.383, %33, %42, %42, %44), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2036 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.229, %hidden_states.385), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2037 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%2036, %hidden_states.381)
  %2038 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2039 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2037)
  %down_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_157.Linear = prim::GetAttr[name="down_proj"](%mlp.25)
  %up_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_156.Linear = prim::GetAttr[name="up_proj"](%mlp.25)
  %gate_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_155.Linear = prim::GetAttr[name="gate_proj"](%mlp.25)
  %weight.231 : Tensor = prim::GetAttr[name="weight"](%gate_proj.25)
  %input.25 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2038, %weight.231), scope: __module.model/__module.model.layers.12/__module.model.layers.12.mlp/__module.model.layers.12.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2045 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.25), scope: __module.model/__module.model.layers.12/__module.model.layers.12.mlp/__module.model.layers.12.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.233 : Tensor = prim::GetAttr[name="weight"](%up_proj.25)
  %2047 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2038, %weight.233), scope: __module.model/__module.model.layers.12/__module.model.layers.12.mlp/__module.model.layers.12.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2048 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%2045, %2047), scope: __module.model/__module.model.layers.12/__module.model.layers.12.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.235 : Tensor = prim::GetAttr[name="weight"](%down_proj.25)
  %hidden_states.387 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2048, %weight.235), scope: __module.model/__module.model.layers.12/__module.model.layers.12.mlp/__module.model.layers.12.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.389 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2039, %hidden_states.387, %47), scope: __module.model/__module.model.layers.12 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.27 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_172.MistralMLP = prim::GetAttr[name="mlp"](%_13)
  %post_attention_layernorm.27 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_174.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_13)
  %self_attn.27 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_167.MistralAttention = prim::GetAttr[name="self_attn"](%_13)
  %input_layernorm.27 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_173.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_13)
  %weight.237 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.27)
  %hidden_states.391 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.389, %33, %42, %42, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2058 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.391, %40), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2059 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm
  %variance.53 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2058, %2059, %25, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2061 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.53, %24, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2062 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2061), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.393 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.391, %2062), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.395 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.393, %33, %42, %42, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.397 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.237, %hidden_states.395), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2066 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.397, %hidden_states.391)
  %2067 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2068 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2066)
  %o_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_166.Linear = prim::GetAttr[name="o_proj"](%self_attn.27)
  %v_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_165.Linear = prim::GetAttr[name="v_proj"](%self_attn.27)
  %k_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_164.Linear = prim::GetAttr[name="k_proj"](%self_attn.27)
  %q_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_163.Linear = prim::GetAttr[name="q_proj"](%self_attn.27)
  %2073 : int = aten::size(%2067, %45), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %2074 : int = aten::size(%2067, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.239 : Tensor = prim::GetAttr[name="weight"](%q_proj.27)
  %2076 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2067, %weight.239), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn/__module.model.layers.13.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2077 : int[] = prim::ListConstruct(%2073, %2074, %35, %30), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2078 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%2076, %2077), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.27 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2078, %47, %40), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.241 : Tensor = prim::GetAttr[name="weight"](%k_proj.27)
  %2081 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2067, %weight.241), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn/__module.model.layers.13.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2082 : int[] = prim::ListConstruct(%2073, %2074, %35, %30), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2083 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2081, %2082), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.27 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2083, %47, %40), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.243 : Tensor = prim::GetAttr[name="weight"](%v_proj.27)
  %2086 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2067, %weight.243), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn/__module.model.layers.13.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2087 : int[] = prim::ListConstruct(%2073, %2074, %35, %30), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2088 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2086, %2087), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.403 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2088, %47, %40), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.31 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.31 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %2092 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.27, %cos.31), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2093 : int = aten::size(%q.27, %38), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2094 : Long(device=cpu) = prim::NumToTensor(%2093), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2095 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2094, %29), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2096 : int = aten::Int(%2095), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %x1.53 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.27, %38, %45, %2096, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2098 : int = aten::size(%q.27, %38), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2099 : Long(device=cpu) = prim::NumToTensor(%2098), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2100 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2099, %29), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2101 : int = aten::Int(%2100), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %x2.53 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.27, %38, %2101, %39, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2103 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.53), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2104 : Tensor[] = prim::ListConstruct(%2103, %x1.53), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2105 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2104, %35), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2106 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2105, %sin.31), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.27 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2092, %2106, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2108 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.27, %cos.31), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2109 : int = aten::size(%k.27, %38), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2110 : Long(device=cpu) = prim::NumToTensor(%2109), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2111 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2110, %29), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2112 : int = aten::Int(%2111), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %x1.55 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.27, %38, %45, %2112, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2114 : int = aten::size(%k.27, %38), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2115 : Long(device=cpu) = prim::NumToTensor(%2114), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2116 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2115, %29), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2117 : int = aten::Int(%2116), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %x2.55 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.27, %38, %2117, %39, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2119 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.55), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2120 : Tensor[] = prim::ListConstruct(%2119, %x1.55), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2121 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2120, %35), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2122 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2121, %sin.31), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.399 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2108, %2122, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2124 : int = aten::size(%hidden_states.399, %45), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2125 : int = aten::size(%hidden_states.399, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.53 : Long(device=cpu) = prim::NumToTensor(%2125), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2127 : int = aten::size(%hidden_states.399, %40), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2128 : int = aten::size(%hidden_states.399, %38), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2129 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.399, %45, %45, %39, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2130 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2129, %47, %45, %39, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2131 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2130, %40), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2132 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2131, %38, %45, %39, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2133 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2132, %37, %45, %39, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2134 : int[] = prim::ListConstruct(%2124, %2125, %37, %2127, %2128), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %hidden_states.401 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2133, %2134, %42), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2136 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.53, %28), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2137 : int = aten::Int(%2136), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2138 : int[] = prim::ListConstruct(%2124, %2137, %2127, %2128), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %key.27 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.401, %2138), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2140 : int = aten::size(%hidden_states.403, %45), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2141 : int = aten::size(%hidden_states.403, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.55 : Long(device=cpu) = prim::NumToTensor(%2141), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2143 : int = aten::size(%hidden_states.403, %40), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2144 : int = aten::size(%hidden_states.403, %38), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2145 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.403, %45, %45, %39, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2146 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2145, %47, %45, %39, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2147 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2146, %40), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2148 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2147, %38, %45, %39, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2149 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2148, %37, %45, %39, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2150 : int[] = prim::ListConstruct(%2140, %2141, %37, %2143, %2144), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %hidden_states.405 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2149, %2150, %42), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2152 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.55, %28), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2153 : int = aten::Int(%2152), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2154 : int[] = prim::ListConstruct(%2140, %2153, %2143, %2144), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %value.27 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.405, %2154), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2156 : int = aten::size(%key.27, %40), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2157 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2158 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2157, %47, %45, %39, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2159 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2158, %40, %45, %39, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.33 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2159, %38, %45, %2156, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.53 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.27, %key.27, %value.27, %attention_mask.33, %27, %42, %26, %42), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %2162 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.53, %47, %40), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.55 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2162, %45), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %2164 : int[] = prim::ListConstruct(%2073, %2074, %35), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn
  %2165 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.55, %2164), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %2166 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%2165, %45), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.245 : Tensor = prim::GetAttr[name="weight"](%o_proj.27)
  %hidden_states.407 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2166, %weight.245), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn/__module.model.layers.13.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.409 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2068, %hidden_states.407, %47), scope: __module.model/__module.model.layers.13 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.247 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.27)
  %hidden_states.411 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.409, %33, %42, %42, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2172 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.411, %40), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2173 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm
  %variance.55 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2172, %2173, %25, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2175 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.55, %24, %47), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2176 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2175), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.413 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.411, %2176), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.415 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.413, %33, %42, %42, %44), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2179 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.247, %hidden_states.415), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2180 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%2179, %hidden_states.411)
  %2181 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2182 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2180)
  %down_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_170.Linear = prim::GetAttr[name="down_proj"](%mlp.27)
  %up_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_169.Linear = prim::GetAttr[name="up_proj"](%mlp.27)
  %gate_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_168.Linear = prim::GetAttr[name="gate_proj"](%mlp.27)
  %weight.249 : Tensor = prim::GetAttr[name="weight"](%gate_proj.27)
  %input.27 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2181, %weight.249), scope: __module.model/__module.model.layers.13/__module.model.layers.13.mlp/__module.model.layers.13.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2188 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.27), scope: __module.model/__module.model.layers.13/__module.model.layers.13.mlp/__module.model.layers.13.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.251 : Tensor = prim::GetAttr[name="weight"](%up_proj.27)
  %2190 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2181, %weight.251), scope: __module.model/__module.model.layers.13/__module.model.layers.13.mlp/__module.model.layers.13.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2191 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%2188, %2190), scope: __module.model/__module.model.layers.13/__module.model.layers.13.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.253 : Tensor = prim::GetAttr[name="weight"](%down_proj.27)
  %hidden_states.417 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2191, %weight.253), scope: __module.model/__module.model.layers.13/__module.model.layers.13.mlp/__module.model.layers.13.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.419 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2182, %hidden_states.417, %47), scope: __module.model/__module.model.layers.13 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.29 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_185.MistralMLP = prim::GetAttr[name="mlp"](%_14)
  %post_attention_layernorm.29 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_187.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_14)
  %self_attn.29 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_180.MistralAttention = prim::GetAttr[name="self_attn"](%_14)
  %input_layernorm.29 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_186.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_14)
  %weight.255 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.29)
  %hidden_states.421 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.419, %33, %42, %42, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2201 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.421, %40), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2202 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm
  %variance.57 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2201, %2202, %25, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2204 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.57, %24, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2205 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2204), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.423 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.421, %2205), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.425 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.423, %33, %42, %42, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.427 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.255, %hidden_states.425), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2209 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.427, %hidden_states.421)
  %2210 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2211 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2209)
  %o_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_179.Linear = prim::GetAttr[name="o_proj"](%self_attn.29)
  %v_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_178.Linear = prim::GetAttr[name="v_proj"](%self_attn.29)
  %k_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_177.Linear = prim::GetAttr[name="k_proj"](%self_attn.29)
  %q_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_176.Linear = prim::GetAttr[name="q_proj"](%self_attn.29)
  %2216 : int = aten::size(%2210, %45), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %2217 : int = aten::size(%2210, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.257 : Tensor = prim::GetAttr[name="weight"](%q_proj.29)
  %2219 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2210, %weight.257), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn/__module.model.layers.14.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2220 : int[] = prim::ListConstruct(%2216, %2217, %35, %30), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2221 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%2219, %2220), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.29 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2221, %47, %40), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.259 : Tensor = prim::GetAttr[name="weight"](%k_proj.29)
  %2224 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2210, %weight.259), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn/__module.model.layers.14.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2225 : int[] = prim::ListConstruct(%2216, %2217, %35, %30), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2226 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2224, %2225), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.29 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2226, %47, %40), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.261 : Tensor = prim::GetAttr[name="weight"](%v_proj.29)
  %2229 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2210, %weight.261), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn/__module.model.layers.14.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2230 : int[] = prim::ListConstruct(%2216, %2217, %35, %30), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2231 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2229, %2230), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.433 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2231, %47, %40), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.33 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.33 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %2235 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.29, %cos.33), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2236 : int = aten::size(%q.29, %38), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2237 : Long(device=cpu) = prim::NumToTensor(%2236), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2238 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2237, %29), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2239 : int = aten::Int(%2238), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %x1.57 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.29, %38, %45, %2239, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2241 : int = aten::size(%q.29, %38), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2242 : Long(device=cpu) = prim::NumToTensor(%2241), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2243 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2242, %29), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2244 : int = aten::Int(%2243), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %x2.57 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.29, %38, %2244, %39, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2246 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.57), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2247 : Tensor[] = prim::ListConstruct(%2246, %x1.57), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2248 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2247, %35), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2249 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2248, %sin.33), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.29 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2235, %2249, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2251 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.29, %cos.33), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2252 : int = aten::size(%k.29, %38), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2253 : Long(device=cpu) = prim::NumToTensor(%2252), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2254 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2253, %29), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2255 : int = aten::Int(%2254), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %x1.59 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.29, %38, %45, %2255, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2257 : int = aten::size(%k.29, %38), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2258 : Long(device=cpu) = prim::NumToTensor(%2257), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2259 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2258, %29), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2260 : int = aten::Int(%2259), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %x2.59 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.29, %38, %2260, %39, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2262 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.59), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2263 : Tensor[] = prim::ListConstruct(%2262, %x1.59), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2264 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2263, %35), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2265 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2264, %sin.33), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.429 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2251, %2265, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2267 : int = aten::size(%hidden_states.429, %45), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2268 : int = aten::size(%hidden_states.429, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.57 : Long(device=cpu) = prim::NumToTensor(%2268), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2270 : int = aten::size(%hidden_states.429, %40), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2271 : int = aten::size(%hidden_states.429, %38), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2272 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.429, %45, %45, %39, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2273 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2272, %47, %45, %39, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2274 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2273, %40), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2275 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2274, %38, %45, %39, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2276 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2275, %37, %45, %39, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2277 : int[] = prim::ListConstruct(%2267, %2268, %37, %2270, %2271), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %hidden_states.431 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2276, %2277, %42), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2279 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.57, %28), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2280 : int = aten::Int(%2279), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2281 : int[] = prim::ListConstruct(%2267, %2280, %2270, %2271), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %key.29 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.431, %2281), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2283 : int = aten::size(%hidden_states.433, %45), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2284 : int = aten::size(%hidden_states.433, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.59 : Long(device=cpu) = prim::NumToTensor(%2284), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2286 : int = aten::size(%hidden_states.433, %40), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2287 : int = aten::size(%hidden_states.433, %38), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2288 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.433, %45, %45, %39, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2289 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2288, %47, %45, %39, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2290 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2289, %40), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2291 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2290, %38, %45, %39, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2292 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2291, %37, %45, %39, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2293 : int[] = prim::ListConstruct(%2283, %2284, %37, %2286, %2287), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %hidden_states.435 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2292, %2293, %42), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2295 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.59, %28), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2296 : int = aten::Int(%2295), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2297 : int[] = prim::ListConstruct(%2283, %2296, %2286, %2287), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %value.29 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.435, %2297), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2299 : int = aten::size(%key.29, %40), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2300 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2301 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2300, %47, %45, %39, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2302 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2301, %40, %45, %39, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.35 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2302, %38, %45, %2299, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.57 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.29, %key.29, %value.29, %attention_mask.35, %27, %42, %26, %42), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %2305 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.57, %47, %40), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.59 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2305, %45), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %2307 : int[] = prim::ListConstruct(%2216, %2217, %35), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn
  %2308 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.59, %2307), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %2309 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%2308, %45), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.263 : Tensor = prim::GetAttr[name="weight"](%o_proj.29)
  %hidden_states.437 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2309, %weight.263), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn/__module.model.layers.14.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.439 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2211, %hidden_states.437, %47), scope: __module.model/__module.model.layers.14 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.265 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.29)
  %hidden_states.441 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.439, %33, %42, %42, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2315 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.441, %40), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2316 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm
  %variance.59 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2315, %2316, %25, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2318 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.59, %24, %47), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2319 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2318), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.443 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.441, %2319), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.445 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.443, %33, %42, %42, %44), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2322 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.265, %hidden_states.445), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2323 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%2322, %hidden_states.441)
  %2324 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2325 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2323)
  %down_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_183.Linear = prim::GetAttr[name="down_proj"](%mlp.29)
  %up_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_182.Linear = prim::GetAttr[name="up_proj"](%mlp.29)
  %gate_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_181.Linear = prim::GetAttr[name="gate_proj"](%mlp.29)
  %weight.267 : Tensor = prim::GetAttr[name="weight"](%gate_proj.29)
  %input.29 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2324, %weight.267), scope: __module.model/__module.model.layers.14/__module.model.layers.14.mlp/__module.model.layers.14.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2331 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.29), scope: __module.model/__module.model.layers.14/__module.model.layers.14.mlp/__module.model.layers.14.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.269 : Tensor = prim::GetAttr[name="weight"](%up_proj.29)
  %2333 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2324, %weight.269), scope: __module.model/__module.model.layers.14/__module.model.layers.14.mlp/__module.model.layers.14.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2334 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%2331, %2333), scope: __module.model/__module.model.layers.14/__module.model.layers.14.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.271 : Tensor = prim::GetAttr[name="weight"](%down_proj.29)
  %hidden_states.447 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2334, %weight.271), scope: __module.model/__module.model.layers.14/__module.model.layers.14.mlp/__module.model.layers.14.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.449 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2325, %hidden_states.447, %47), scope: __module.model/__module.model.layers.14 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.31 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_198.MistralMLP = prim::GetAttr[name="mlp"](%_15)
  %post_attention_layernorm.31 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_200.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_15)
  %self_attn.31 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_193.MistralAttention = prim::GetAttr[name="self_attn"](%_15)
  %input_layernorm.31 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_199.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_15)
  %weight.273 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.31)
  %hidden_states.451 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.449, %33, %42, %42, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2344 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.451, %40), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2345 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm
  %variance.61 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2344, %2345, %25, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2347 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.61, %24, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2348 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2347), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.453 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.451, %2348), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.455 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.453, %33, %42, %42, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.457 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.273, %hidden_states.455), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2352 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.457, %hidden_states.451)
  %2353 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2354 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2352)
  %o_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_192.Linear = prim::GetAttr[name="o_proj"](%self_attn.31)
  %v_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_191.Linear = prim::GetAttr[name="v_proj"](%self_attn.31)
  %k_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_190.Linear = prim::GetAttr[name="k_proj"](%self_attn.31)
  %q_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_189.Linear = prim::GetAttr[name="q_proj"](%self_attn.31)
  %2359 : int = aten::size(%2353, %45), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %2360 : int = aten::size(%2353, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.275 : Tensor = prim::GetAttr[name="weight"](%q_proj.31)
  %2362 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2353, %weight.275), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn/__module.model.layers.15.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2363 : int[] = prim::ListConstruct(%2359, %2360, %35, %30), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2364 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%2362, %2363), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.31 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2364, %47, %40), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.277 : Tensor = prim::GetAttr[name="weight"](%k_proj.31)
  %2367 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2353, %weight.277), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn/__module.model.layers.15.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2368 : int[] = prim::ListConstruct(%2359, %2360, %35, %30), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2369 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2367, %2368), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.31 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2369, %47, %40), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.279 : Tensor = prim::GetAttr[name="weight"](%v_proj.31)
  %2372 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2353, %weight.279), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn/__module.model.layers.15.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2373 : int[] = prim::ListConstruct(%2359, %2360, %35, %30), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2374 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2372, %2373), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.463 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2374, %47, %40), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.35 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.35 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %2378 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.31, %cos.35), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2379 : int = aten::size(%q.31, %38), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2380 : Long(device=cpu) = prim::NumToTensor(%2379), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2381 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2380, %29), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2382 : int = aten::Int(%2381), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %x1.61 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.31, %38, %45, %2382, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2384 : int = aten::size(%q.31, %38), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2385 : Long(device=cpu) = prim::NumToTensor(%2384), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2386 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2385, %29), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2387 : int = aten::Int(%2386), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %x2.61 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.31, %38, %2387, %39, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2389 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.61), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2390 : Tensor[] = prim::ListConstruct(%2389, %x1.61), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2391 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2390, %35), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2392 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2391, %sin.35), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.31 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2378, %2392, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2394 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.31, %cos.35), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2395 : int = aten::size(%k.31, %38), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2396 : Long(device=cpu) = prim::NumToTensor(%2395), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2397 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2396, %29), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2398 : int = aten::Int(%2397), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %x1.63 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.31, %38, %45, %2398, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2400 : int = aten::size(%k.31, %38), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2401 : Long(device=cpu) = prim::NumToTensor(%2400), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2402 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2401, %29), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2403 : int = aten::Int(%2402), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %x2.63 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.31, %38, %2403, %39, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2405 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.63), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2406 : Tensor[] = prim::ListConstruct(%2405, %x1.63), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2407 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2406, %35), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2408 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2407, %sin.35), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.459 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2394, %2408, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2410 : int = aten::size(%hidden_states.459, %45), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2411 : int = aten::size(%hidden_states.459, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.61 : Long(device=cpu) = prim::NumToTensor(%2411), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2413 : int = aten::size(%hidden_states.459, %40), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2414 : int = aten::size(%hidden_states.459, %38), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2415 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.459, %45, %45, %39, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2416 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2415, %47, %45, %39, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2417 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2416, %40), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2418 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2417, %38, %45, %39, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2419 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2418, %37, %45, %39, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2420 : int[] = prim::ListConstruct(%2410, %2411, %37, %2413, %2414), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %hidden_states.461 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2419, %2420, %42), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2422 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.61, %28), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2423 : int = aten::Int(%2422), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2424 : int[] = prim::ListConstruct(%2410, %2423, %2413, %2414), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %key.31 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.461, %2424), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2426 : int = aten::size(%hidden_states.463, %45), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2427 : int = aten::size(%hidden_states.463, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.63 : Long(device=cpu) = prim::NumToTensor(%2427), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2429 : int = aten::size(%hidden_states.463, %40), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2430 : int = aten::size(%hidden_states.463, %38), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2431 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.463, %45, %45, %39, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2432 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2431, %47, %45, %39, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2433 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2432, %40), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2434 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2433, %38, %45, %39, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2435 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2434, %37, %45, %39, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2436 : int[] = prim::ListConstruct(%2426, %2427, %37, %2429, %2430), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %hidden_states.465 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2435, %2436, %42), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2438 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.63, %28), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2439 : int = aten::Int(%2438), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2440 : int[] = prim::ListConstruct(%2426, %2439, %2429, %2430), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %value.31 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.465, %2440), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2442 : int = aten::size(%key.31, %40), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2443 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2444 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2443, %47, %45, %39, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2445 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2444, %40, %45, %39, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.37 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2445, %38, %45, %2442, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.61 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.31, %key.31, %value.31, %attention_mask.37, %27, %42, %26, %42), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %2448 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.61, %47, %40), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.63 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2448, %45), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %2450 : int[] = prim::ListConstruct(%2359, %2360, %35), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn
  %2451 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.63, %2450), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %2452 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%2451, %45), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.281 : Tensor = prim::GetAttr[name="weight"](%o_proj.31)
  %hidden_states.467 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2452, %weight.281), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn/__module.model.layers.15.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.469 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2354, %hidden_states.467, %47), scope: __module.model/__module.model.layers.15 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.283 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.31)
  %hidden_states.471 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.469, %33, %42, %42, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2458 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.471, %40), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2459 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm
  %variance.63 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2458, %2459, %25, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2461 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.63, %24, %47), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2462 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2461), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.473 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.471, %2462), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.475 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.473, %33, %42, %42, %44), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2465 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.283, %hidden_states.475), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2466 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%2465, %hidden_states.471)
  %2467 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2468 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2466)
  %down_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_196.Linear = prim::GetAttr[name="down_proj"](%mlp.31)
  %up_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_195.Linear = prim::GetAttr[name="up_proj"](%mlp.31)
  %gate_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_194.Linear = prim::GetAttr[name="gate_proj"](%mlp.31)
  %weight.285 : Tensor = prim::GetAttr[name="weight"](%gate_proj.31)
  %input.31 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2467, %weight.285), scope: __module.model/__module.model.layers.15/__module.model.layers.15.mlp/__module.model.layers.15.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2474 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.31), scope: __module.model/__module.model.layers.15/__module.model.layers.15.mlp/__module.model.layers.15.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.287 : Tensor = prim::GetAttr[name="weight"](%up_proj.31)
  %2476 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2467, %weight.287), scope: __module.model/__module.model.layers.15/__module.model.layers.15.mlp/__module.model.layers.15.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2477 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%2474, %2476), scope: __module.model/__module.model.layers.15/__module.model.layers.15.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.289 : Tensor = prim::GetAttr[name="weight"](%down_proj.31)
  %hidden_states.477 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2477, %weight.289), scope: __module.model/__module.model.layers.15/__module.model.layers.15.mlp/__module.model.layers.15.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.479 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2468, %hidden_states.477, %47), scope: __module.model/__module.model.layers.15 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.33 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_211.MistralMLP = prim::GetAttr[name="mlp"](%_16)
  %post_attention_layernorm.33 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_213.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_16)
  %self_attn.33 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_206.MistralAttention = prim::GetAttr[name="self_attn"](%_16)
  %input_layernorm.33 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_212.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_16)
  %weight.291 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.33)
  %hidden_states.481 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.479, %33, %42, %42, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2487 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.481, %40), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2488 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm
  %variance.65 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2487, %2488, %25, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2490 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.65, %24, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2491 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2490), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.483 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.481, %2491), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.485 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.483, %33, %42, %42, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.487 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.291, %hidden_states.485), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2495 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.487, %hidden_states.481)
  %2496 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2497 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2495)
  %o_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_205.Linear = prim::GetAttr[name="o_proj"](%self_attn.33)
  %v_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_204.Linear = prim::GetAttr[name="v_proj"](%self_attn.33)
  %k_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_203.Linear = prim::GetAttr[name="k_proj"](%self_attn.33)
  %q_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_202.Linear = prim::GetAttr[name="q_proj"](%self_attn.33)
  %2502 : int = aten::size(%2496, %45), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %2503 : int = aten::size(%2496, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.293 : Tensor = prim::GetAttr[name="weight"](%q_proj.33)
  %2505 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2496, %weight.293), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn/__module.model.layers.16.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2506 : int[] = prim::ListConstruct(%2502, %2503, %35, %30), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2507 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%2505, %2506), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.33 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2507, %47, %40), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.295 : Tensor = prim::GetAttr[name="weight"](%k_proj.33)
  %2510 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2496, %weight.295), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn/__module.model.layers.16.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2511 : int[] = prim::ListConstruct(%2502, %2503, %35, %30), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2512 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2510, %2511), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.33 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2512, %47, %40), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.297 : Tensor = prim::GetAttr[name="weight"](%v_proj.33)
  %2515 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2496, %weight.297), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn/__module.model.layers.16.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2516 : int[] = prim::ListConstruct(%2502, %2503, %35, %30), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2517 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2515, %2516), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.493 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2517, %47, %40), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.37 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.37 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %2521 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.33, %cos.37), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2522 : int = aten::size(%q.33, %38), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2523 : Long(device=cpu) = prim::NumToTensor(%2522), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2524 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2523, %29), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2525 : int = aten::Int(%2524), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %x1.65 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.33, %38, %45, %2525, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2527 : int = aten::size(%q.33, %38), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2528 : Long(device=cpu) = prim::NumToTensor(%2527), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2529 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2528, %29), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2530 : int = aten::Int(%2529), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %x2.65 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.33, %38, %2530, %39, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2532 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.65), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2533 : Tensor[] = prim::ListConstruct(%2532, %x1.65), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2534 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2533, %35), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2535 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2534, %sin.37), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.33 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2521, %2535, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2537 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.33, %cos.37), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2538 : int = aten::size(%k.33, %38), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2539 : Long(device=cpu) = prim::NumToTensor(%2538), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2540 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2539, %29), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2541 : int = aten::Int(%2540), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %x1.67 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.33, %38, %45, %2541, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2543 : int = aten::size(%k.33, %38), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2544 : Long(device=cpu) = prim::NumToTensor(%2543), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2545 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2544, %29), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2546 : int = aten::Int(%2545), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %x2.67 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.33, %38, %2546, %39, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2548 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.67), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2549 : Tensor[] = prim::ListConstruct(%2548, %x1.67), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2550 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2549, %35), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2551 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2550, %sin.37), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.489 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2537, %2551, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2553 : int = aten::size(%hidden_states.489, %45), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2554 : int = aten::size(%hidden_states.489, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.65 : Long(device=cpu) = prim::NumToTensor(%2554), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2556 : int = aten::size(%hidden_states.489, %40), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2557 : int = aten::size(%hidden_states.489, %38), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2558 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.489, %45, %45, %39, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2559 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2558, %47, %45, %39, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2560 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2559, %40), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2561 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2560, %38, %45, %39, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2562 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2561, %37, %45, %39, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2563 : int[] = prim::ListConstruct(%2553, %2554, %37, %2556, %2557), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %hidden_states.491 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2562, %2563, %42), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2565 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.65, %28), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2566 : int = aten::Int(%2565), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2567 : int[] = prim::ListConstruct(%2553, %2566, %2556, %2557), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %key.33 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.491, %2567), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2569 : int = aten::size(%hidden_states.493, %45), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2570 : int = aten::size(%hidden_states.493, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.67 : Long(device=cpu) = prim::NumToTensor(%2570), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2572 : int = aten::size(%hidden_states.493, %40), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2573 : int = aten::size(%hidden_states.493, %38), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2574 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.493, %45, %45, %39, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2575 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2574, %47, %45, %39, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2576 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2575, %40), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2577 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2576, %38, %45, %39, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2578 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2577, %37, %45, %39, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2579 : int[] = prim::ListConstruct(%2569, %2570, %37, %2572, %2573), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %hidden_states.495 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2578, %2579, %42), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2581 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.67, %28), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2582 : int = aten::Int(%2581), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2583 : int[] = prim::ListConstruct(%2569, %2582, %2572, %2573), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %value.33 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.495, %2583), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2585 : int = aten::size(%key.33, %40), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2586 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2587 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2586, %47, %45, %39, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2588 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2587, %40, %45, %39, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.39 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2588, %38, %45, %2585, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.65 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.33, %key.33, %value.33, %attention_mask.39, %27, %42, %26, %42), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %2591 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.65, %47, %40), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.67 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2591, %45), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %2593 : int[] = prim::ListConstruct(%2502, %2503, %35), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn
  %2594 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.67, %2593), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %2595 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%2594, %45), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.299 : Tensor = prim::GetAttr[name="weight"](%o_proj.33)
  %hidden_states.497 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2595, %weight.299), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn/__module.model.layers.16.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.499 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2497, %hidden_states.497, %47), scope: __module.model/__module.model.layers.16 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.301 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.33)
  %hidden_states.501 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.499, %33, %42, %42, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2601 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.501, %40), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2602 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm
  %variance.67 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2601, %2602, %25, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2604 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.67, %24, %47), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2605 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2604), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.503 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.501, %2605), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.505 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.503, %33, %42, %42, %44), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2608 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.301, %hidden_states.505), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2609 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%2608, %hidden_states.501)
  %2610 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2611 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2609)
  %down_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_209.Linear = prim::GetAttr[name="down_proj"](%mlp.33)
  %up_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_208.Linear = prim::GetAttr[name="up_proj"](%mlp.33)
  %gate_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_207.Linear = prim::GetAttr[name="gate_proj"](%mlp.33)
  %weight.303 : Tensor = prim::GetAttr[name="weight"](%gate_proj.33)
  %input.33 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2610, %weight.303), scope: __module.model/__module.model.layers.16/__module.model.layers.16.mlp/__module.model.layers.16.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2617 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.33), scope: __module.model/__module.model.layers.16/__module.model.layers.16.mlp/__module.model.layers.16.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.305 : Tensor = prim::GetAttr[name="weight"](%up_proj.33)
  %2619 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2610, %weight.305), scope: __module.model/__module.model.layers.16/__module.model.layers.16.mlp/__module.model.layers.16.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2620 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%2617, %2619), scope: __module.model/__module.model.layers.16/__module.model.layers.16.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.307 : Tensor = prim::GetAttr[name="weight"](%down_proj.33)
  %hidden_states.507 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2620, %weight.307), scope: __module.model/__module.model.layers.16/__module.model.layers.16.mlp/__module.model.layers.16.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.509 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2611, %hidden_states.507, %47), scope: __module.model/__module.model.layers.16 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.35 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_224.MistralMLP = prim::GetAttr[name="mlp"](%_17)
  %post_attention_layernorm.35 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_226.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_17)
  %self_attn.35 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_219.MistralAttention = prim::GetAttr[name="self_attn"](%_17)
  %input_layernorm.35 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_225.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_17)
  %weight.309 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.35)
  %hidden_states.511 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.509, %33, %42, %42, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2630 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.511, %40), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2631 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm
  %variance.69 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2630, %2631, %25, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2633 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.69, %24, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2634 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2633), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.513 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.511, %2634), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.515 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.513, %33, %42, %42, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.517 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.309, %hidden_states.515), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2638 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.517, %hidden_states.511)
  %2639 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2640 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2638)
  %o_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_218.Linear = prim::GetAttr[name="o_proj"](%self_attn.35)
  %v_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_217.Linear = prim::GetAttr[name="v_proj"](%self_attn.35)
  %k_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_216.Linear = prim::GetAttr[name="k_proj"](%self_attn.35)
  %q_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_215.Linear = prim::GetAttr[name="q_proj"](%self_attn.35)
  %2645 : int = aten::size(%2639, %45), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %2646 : int = aten::size(%2639, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.311 : Tensor = prim::GetAttr[name="weight"](%q_proj.35)
  %2648 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2639, %weight.311), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn/__module.model.layers.17.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2649 : int[] = prim::ListConstruct(%2645, %2646, %35, %30), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2650 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%2648, %2649), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.35 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2650, %47, %40), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.313 : Tensor = prim::GetAttr[name="weight"](%k_proj.35)
  %2653 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2639, %weight.313), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn/__module.model.layers.17.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2654 : int[] = prim::ListConstruct(%2645, %2646, %35, %30), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2655 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2653, %2654), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.35 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2655, %47, %40), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.315 : Tensor = prim::GetAttr[name="weight"](%v_proj.35)
  %2658 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2639, %weight.315), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn/__module.model.layers.17.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2659 : int[] = prim::ListConstruct(%2645, %2646, %35, %30), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2660 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2658, %2659), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.523 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2660, %47, %40), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.39 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.39 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %2664 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.35, %cos.39), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2665 : int = aten::size(%q.35, %38), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2666 : Long(device=cpu) = prim::NumToTensor(%2665), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2667 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2666, %29), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2668 : int = aten::Int(%2667), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %x1.69 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.35, %38, %45, %2668, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2670 : int = aten::size(%q.35, %38), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2671 : Long(device=cpu) = prim::NumToTensor(%2670), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2672 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2671, %29), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2673 : int = aten::Int(%2672), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %x2.69 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.35, %38, %2673, %39, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2675 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.69), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2676 : Tensor[] = prim::ListConstruct(%2675, %x1.69), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2677 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2676, %35), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2678 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2677, %sin.39), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.35 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2664, %2678, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2680 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.35, %cos.39), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2681 : int = aten::size(%k.35, %38), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2682 : Long(device=cpu) = prim::NumToTensor(%2681), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2683 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2682, %29), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2684 : int = aten::Int(%2683), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %x1.71 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.35, %38, %45, %2684, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2686 : int = aten::size(%k.35, %38), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2687 : Long(device=cpu) = prim::NumToTensor(%2686), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2688 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2687, %29), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2689 : int = aten::Int(%2688), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %x2.71 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.35, %38, %2689, %39, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2691 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.71), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2692 : Tensor[] = prim::ListConstruct(%2691, %x1.71), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2693 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2692, %35), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2694 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2693, %sin.39), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.519 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2680, %2694, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2696 : int = aten::size(%hidden_states.519, %45), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2697 : int = aten::size(%hidden_states.519, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.69 : Long(device=cpu) = prim::NumToTensor(%2697), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2699 : int = aten::size(%hidden_states.519, %40), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2700 : int = aten::size(%hidden_states.519, %38), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2701 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.519, %45, %45, %39, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2702 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2701, %47, %45, %39, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2703 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2702, %40), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2704 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2703, %38, %45, %39, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2705 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2704, %37, %45, %39, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2706 : int[] = prim::ListConstruct(%2696, %2697, %37, %2699, %2700), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %hidden_states.521 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2705, %2706, %42), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2708 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.69, %28), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2709 : int = aten::Int(%2708), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2710 : int[] = prim::ListConstruct(%2696, %2709, %2699, %2700), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %key.35 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.521, %2710), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2712 : int = aten::size(%hidden_states.523, %45), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2713 : int = aten::size(%hidden_states.523, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.71 : Long(device=cpu) = prim::NumToTensor(%2713), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2715 : int = aten::size(%hidden_states.523, %40), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2716 : int = aten::size(%hidden_states.523, %38), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2717 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.523, %45, %45, %39, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2718 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2717, %47, %45, %39, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2719 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2718, %40), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2720 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2719, %38, %45, %39, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2721 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2720, %37, %45, %39, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2722 : int[] = prim::ListConstruct(%2712, %2713, %37, %2715, %2716), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %hidden_states.525 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2721, %2722, %42), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2724 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.71, %28), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2725 : int = aten::Int(%2724), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2726 : int[] = prim::ListConstruct(%2712, %2725, %2715, %2716), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %value.35 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.525, %2726), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2728 : int = aten::size(%key.35, %40), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2729 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2730 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2729, %47, %45, %39, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2731 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2730, %40, %45, %39, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.41 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2731, %38, %45, %2728, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.69 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.35, %key.35, %value.35, %attention_mask.41, %27, %42, %26, %42), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %2734 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.69, %47, %40), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.71 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2734, %45), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %2736 : int[] = prim::ListConstruct(%2645, %2646, %35), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn
  %2737 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.71, %2736), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %2738 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%2737, %45), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.317 : Tensor = prim::GetAttr[name="weight"](%o_proj.35)
  %hidden_states.527 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2738, %weight.317), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn/__module.model.layers.17.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.529 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2640, %hidden_states.527, %47), scope: __module.model/__module.model.layers.17 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.319 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.35)
  %hidden_states.531 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.529, %33, %42, %42, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2744 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.531, %40), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2745 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm
  %variance.71 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2744, %2745, %25, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2747 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.71, %24, %47), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2748 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2747), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.533 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.531, %2748), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.535 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.533, %33, %42, %42, %44), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2751 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.319, %hidden_states.535), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2752 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%2751, %hidden_states.531)
  %2753 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2754 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2752)
  %down_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_222.Linear = prim::GetAttr[name="down_proj"](%mlp.35)
  %up_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_221.Linear = prim::GetAttr[name="up_proj"](%mlp.35)
  %gate_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_220.Linear = prim::GetAttr[name="gate_proj"](%mlp.35)
  %weight.321 : Tensor = prim::GetAttr[name="weight"](%gate_proj.35)
  %input.35 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2753, %weight.321), scope: __module.model/__module.model.layers.17/__module.model.layers.17.mlp/__module.model.layers.17.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2760 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.35), scope: __module.model/__module.model.layers.17/__module.model.layers.17.mlp/__module.model.layers.17.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.323 : Tensor = prim::GetAttr[name="weight"](%up_proj.35)
  %2762 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2753, %weight.323), scope: __module.model/__module.model.layers.17/__module.model.layers.17.mlp/__module.model.layers.17.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2763 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%2760, %2762), scope: __module.model/__module.model.layers.17/__module.model.layers.17.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.325 : Tensor = prim::GetAttr[name="weight"](%down_proj.35)
  %hidden_states.537 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2763, %weight.325), scope: __module.model/__module.model.layers.17/__module.model.layers.17.mlp/__module.model.layers.17.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.539 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2754, %hidden_states.537, %47), scope: __module.model/__module.model.layers.17 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.37 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_237.MistralMLP = prim::GetAttr[name="mlp"](%_18)
  %post_attention_layernorm.37 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_239.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_18)
  %self_attn.37 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_232.MistralAttention = prim::GetAttr[name="self_attn"](%_18)
  %input_layernorm.37 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_238.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_18)
  %weight.327 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.37)
  %hidden_states.541 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.539, %33, %42, %42, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2773 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.541, %40), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2774 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm
  %variance.73 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2773, %2774, %25, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2776 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.73, %24, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2777 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2776), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.543 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.541, %2777), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.545 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.543, %33, %42, %42, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.547 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.327, %hidden_states.545), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2781 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.547, %hidden_states.541)
  %2782 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2783 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2781)
  %o_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_231.Linear = prim::GetAttr[name="o_proj"](%self_attn.37)
  %v_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_230.Linear = prim::GetAttr[name="v_proj"](%self_attn.37)
  %k_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_229.Linear = prim::GetAttr[name="k_proj"](%self_attn.37)
  %q_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_228.Linear = prim::GetAttr[name="q_proj"](%self_attn.37)
  %2788 : int = aten::size(%2782, %45), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %2789 : int = aten::size(%2782, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.329 : Tensor = prim::GetAttr[name="weight"](%q_proj.37)
  %2791 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2782, %weight.329), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn/__module.model.layers.18.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2792 : int[] = prim::ListConstruct(%2788, %2789, %35, %30), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2793 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%2791, %2792), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.37 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2793, %47, %40), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.331 : Tensor = prim::GetAttr[name="weight"](%k_proj.37)
  %2796 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2782, %weight.331), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn/__module.model.layers.18.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2797 : int[] = prim::ListConstruct(%2788, %2789, %35, %30), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2798 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2796, %2797), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.37 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2798, %47, %40), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.333 : Tensor = prim::GetAttr[name="weight"](%v_proj.37)
  %2801 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2782, %weight.333), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn/__module.model.layers.18.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2802 : int[] = prim::ListConstruct(%2788, %2789, %35, %30), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2803 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2801, %2802), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.553 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2803, %47, %40), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.41 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.41 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %2807 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.37, %cos.41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2808 : int = aten::size(%q.37, %38), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2809 : Long(device=cpu) = prim::NumToTensor(%2808), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2810 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2809, %29), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2811 : int = aten::Int(%2810), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %x1.73 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.37, %38, %45, %2811, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2813 : int = aten::size(%q.37, %38), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2814 : Long(device=cpu) = prim::NumToTensor(%2813), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2815 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2814, %29), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2816 : int = aten::Int(%2815), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %x2.73 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.37, %38, %2816, %39, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2818 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.73), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2819 : Tensor[] = prim::ListConstruct(%2818, %x1.73), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2820 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2819, %35), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2821 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2820, %sin.41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.37 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2807, %2821, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2823 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.37, %cos.41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2824 : int = aten::size(%k.37, %38), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2825 : Long(device=cpu) = prim::NumToTensor(%2824), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2826 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2825, %29), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2827 : int = aten::Int(%2826), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %x1.75 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.37, %38, %45, %2827, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2829 : int = aten::size(%k.37, %38), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2830 : Long(device=cpu) = prim::NumToTensor(%2829), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2831 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2830, %29), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2832 : int = aten::Int(%2831), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %x2.75 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.37, %38, %2832, %39, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2834 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.75), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2835 : Tensor[] = prim::ListConstruct(%2834, %x1.75), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2836 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2835, %35), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2837 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2836, %sin.41), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.549 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2823, %2837, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2839 : int = aten::size(%hidden_states.549, %45), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2840 : int = aten::size(%hidden_states.549, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.73 : Long(device=cpu) = prim::NumToTensor(%2840), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2842 : int = aten::size(%hidden_states.549, %40), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2843 : int = aten::size(%hidden_states.549, %38), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2844 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.549, %45, %45, %39, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2845 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2844, %47, %45, %39, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2846 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2845, %40), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2847 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2846, %38, %45, %39, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2848 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2847, %37, %45, %39, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2849 : int[] = prim::ListConstruct(%2839, %2840, %37, %2842, %2843), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %hidden_states.551 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2848, %2849, %42), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2851 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.73, %28), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2852 : int = aten::Int(%2851), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2853 : int[] = prim::ListConstruct(%2839, %2852, %2842, %2843), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %key.37 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.551, %2853), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2855 : int = aten::size(%hidden_states.553, %45), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2856 : int = aten::size(%hidden_states.553, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.75 : Long(device=cpu) = prim::NumToTensor(%2856), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2858 : int = aten::size(%hidden_states.553, %40), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2859 : int = aten::size(%hidden_states.553, %38), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2860 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.553, %45, %45, %39, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2861 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2860, %47, %45, %39, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2862 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2861, %40), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2863 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2862, %38, %45, %39, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2864 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2863, %37, %45, %39, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2865 : int[] = prim::ListConstruct(%2855, %2856, %37, %2858, %2859), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %hidden_states.555 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2864, %2865, %42), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2867 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.75, %28), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2868 : int = aten::Int(%2867), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2869 : int[] = prim::ListConstruct(%2855, %2868, %2858, %2859), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %value.37 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.555, %2869), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2871 : int = aten::size(%key.37, %40), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2872 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2873 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2872, %47, %45, %39, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %2874 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2873, %40, %45, %39, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.43 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%2874, %38, %45, %2871, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.73 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.37, %key.37, %value.37, %attention_mask.43, %27, %42, %26, %42), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %2877 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.73, %47, %40), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.75 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2877, %45), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %2879 : int[] = prim::ListConstruct(%2788, %2789, %35), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn
  %2880 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.75, %2879), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %2881 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%2880, %45), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.335 : Tensor = prim::GetAttr[name="weight"](%o_proj.37)
  %hidden_states.557 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2881, %weight.335), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn/__module.model.layers.18.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.559 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2783, %hidden_states.557, %47), scope: __module.model/__module.model.layers.18 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.337 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.37)
  %hidden_states.561 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.559, %33, %42, %42, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2887 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.561, %40), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2888 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm
  %variance.75 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2887, %2888, %25, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2890 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.75, %24, %47), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2891 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2890), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.563 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.561, %2891), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.565 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.563, %33, %42, %42, %44), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2894 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.337, %hidden_states.565), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2895 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%2894, %hidden_states.561)
  %2896 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2897 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2895)
  %down_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_235.Linear = prim::GetAttr[name="down_proj"](%mlp.37)
  %up_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_234.Linear = prim::GetAttr[name="up_proj"](%mlp.37)
  %gate_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_233.Linear = prim::GetAttr[name="gate_proj"](%mlp.37)
  %weight.339 : Tensor = prim::GetAttr[name="weight"](%gate_proj.37)
  %input.37 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2896, %weight.339), scope: __module.model/__module.model.layers.18/__module.model.layers.18.mlp/__module.model.layers.18.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2903 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.37), scope: __module.model/__module.model.layers.18/__module.model.layers.18.mlp/__module.model.layers.18.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.341 : Tensor = prim::GetAttr[name="weight"](%up_proj.37)
  %2905 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2896, %weight.341), scope: __module.model/__module.model.layers.18/__module.model.layers.18.mlp/__module.model.layers.18.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2906 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%2903, %2905), scope: __module.model/__module.model.layers.18/__module.model.layers.18.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.343 : Tensor = prim::GetAttr[name="weight"](%down_proj.37)
  %hidden_states.567 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2906, %weight.343), scope: __module.model/__module.model.layers.18/__module.model.layers.18.mlp/__module.model.layers.18.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.569 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2897, %hidden_states.567, %47), scope: __module.model/__module.model.layers.18 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.39 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_250.MistralMLP = prim::GetAttr[name="mlp"](%_19)
  %post_attention_layernorm.39 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_252.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_19)
  %self_attn.39 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_245.MistralAttention = prim::GetAttr[name="self_attn"](%_19)
  %input_layernorm.39 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_251.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_19)
  %weight.345 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.39)
  %hidden_states.571 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.569, %33, %42, %42, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %2916 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.571, %40), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2917 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm
  %variance.77 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2916, %2917, %25, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %2919 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.77, %24, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %2920 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2919), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.573 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.571, %2920), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.575 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.573, %33, %42, %42, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.577 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.345, %hidden_states.575), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %2924 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.577, %hidden_states.571)
  %2925 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2926 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2924)
  %o_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_244.Linear = prim::GetAttr[name="o_proj"](%self_attn.39)
  %v_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_243.Linear = prim::GetAttr[name="v_proj"](%self_attn.39)
  %k_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_242.Linear = prim::GetAttr[name="k_proj"](%self_attn.39)
  %q_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_241.Linear = prim::GetAttr[name="q_proj"](%self_attn.39)
  %2931 : int = aten::size(%2925, %45), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %2932 : int = aten::size(%2925, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.347 : Tensor = prim::GetAttr[name="weight"](%q_proj.39)
  %2934 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2925, %weight.347), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn/__module.model.layers.19.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2935 : int[] = prim::ListConstruct(%2931, %2932, %35, %30), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %2936 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%2934, %2935), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.39 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2936, %47, %40), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.349 : Tensor = prim::GetAttr[name="weight"](%k_proj.39)
  %2939 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2925, %weight.349), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn/__module.model.layers.19.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2940 : int[] = prim::ListConstruct(%2931, %2932, %35, %30), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %2941 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2939, %2940), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.39 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2941, %47, %40), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.351 : Tensor = prim::GetAttr[name="weight"](%v_proj.39)
  %2944 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%2925, %weight.351), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn/__module.model.layers.19.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %2945 : int[] = prim::ListConstruct(%2931, %2932, %35, %30), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %2946 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%2944, %2945), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.583 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2946, %47, %40), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.43 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.43 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %2950 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.39, %cos.43), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2951 : int = aten::size(%q.39, %38), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2952 : Long(device=cpu) = prim::NumToTensor(%2951), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %2953 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2952, %29), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2954 : int = aten::Int(%2953), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %x1.77 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.39, %38, %45, %2954, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2956 : int = aten::size(%q.39, %38), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2957 : Long(device=cpu) = prim::NumToTensor(%2956), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %2958 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2957, %29), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2959 : int = aten::Int(%2958), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %x2.77 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.39, %38, %2959, %39, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2961 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.77), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2962 : Tensor[] = prim::ListConstruct(%2961, %x1.77), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %2963 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2962, %35), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2964 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2963, %sin.43), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.39 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2950, %2964, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %2966 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.39, %cos.43), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2967 : int = aten::size(%k.39, %38), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2968 : Long(device=cpu) = prim::NumToTensor(%2967), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %2969 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2968, %29), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2970 : int = aten::Int(%2969), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %x1.79 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.39, %38, %45, %2970, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %2972 : int = aten::size(%k.39, %38), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2973 : Long(device=cpu) = prim::NumToTensor(%2972), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %2974 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2973, %29), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %2975 : int = aten::Int(%2974), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %x2.79 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.39, %38, %2975, %39, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %2977 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.79), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2978 : Tensor[] = prim::ListConstruct(%2977, %x1.79), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %2979 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2978, %35), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %2980 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2979, %sin.43), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.579 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2966, %2980, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %2982 : int = aten::size(%hidden_states.579, %45), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2983 : int = aten::size(%hidden_states.579, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.77 : Long(device=cpu) = prim::NumToTensor(%2983), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %2985 : int = aten::size(%hidden_states.579, %40), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2986 : int = aten::size(%hidden_states.579, %38), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2987 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.579, %45, %45, %39, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2988 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2987, %47, %45, %39, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2989 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2988, %40), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2990 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2989, %38, %45, %39, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2991 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%2990, %37, %45, %39, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2992 : int[] = prim::ListConstruct(%2982, %2983, %37, %2985, %2986), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %hidden_states.581 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%2991, %2992, %42), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %2994 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.77, %28), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2995 : int = aten::Int(%2994), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %2996 : int[] = prim::ListConstruct(%2982, %2995, %2985, %2986), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %key.39 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.581, %2996), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %2998 : int = aten::size(%hidden_states.583, %45), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %2999 : int = aten::size(%hidden_states.583, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.79 : Long(device=cpu) = prim::NumToTensor(%2999), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3001 : int = aten::size(%hidden_states.583, %40), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3002 : int = aten::size(%hidden_states.583, %38), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3003 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.583, %45, %45, %39, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3004 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3003, %47, %45, %39, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3005 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3004, %40), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3006 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3005, %38, %45, %39, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3007 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3006, %37, %45, %39, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3008 : int[] = prim::ListConstruct(%2998, %2999, %37, %3001, %3002), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %hidden_states.585 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3007, %3008, %42), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3010 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.79, %28), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3011 : int = aten::Int(%3010), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3012 : int[] = prim::ListConstruct(%2998, %3011, %3001, %3002), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %value.39 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.585, %3012), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3014 : int = aten::size(%key.39, %40), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3015 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3016 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3015, %47, %45, %39, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3017 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3016, %40, %45, %39, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.45 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3017, %38, %45, %3014, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.77 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.39, %key.39, %value.39, %attention_mask.45, %27, %42, %26, %42), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %3020 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.77, %47, %40), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.79 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3020, %45), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %3022 : int[] = prim::ListConstruct(%2931, %2932, %35), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn
  %3023 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.79, %3022), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %3024 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%3023, %45), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.353 : Tensor = prim::GetAttr[name="weight"](%o_proj.39)
  %hidden_states.587 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3024, %weight.353), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn/__module.model.layers.19.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.589 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2926, %hidden_states.587, %47), scope: __module.model/__module.model.layers.19 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.355 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.39)
  %hidden_states.591 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.589, %33, %42, %42, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3030 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.591, %40), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3031 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm
  %variance.79 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3030, %3031, %25, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3033 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.79, %24, %47), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3034 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3033), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.593 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.591, %3034), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.595 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.593, %33, %42, %42, %44), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3037 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.355, %hidden_states.595), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3038 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%3037, %hidden_states.591)
  %3039 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3040 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3038)
  %down_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_248.Linear = prim::GetAttr[name="down_proj"](%mlp.39)
  %up_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_247.Linear = prim::GetAttr[name="up_proj"](%mlp.39)
  %gate_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_246.Linear = prim::GetAttr[name="gate_proj"](%mlp.39)
  %weight.357 : Tensor = prim::GetAttr[name="weight"](%gate_proj.39)
  %input.39 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3039, %weight.357), scope: __module.model/__module.model.layers.19/__module.model.layers.19.mlp/__module.model.layers.19.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3046 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.39), scope: __module.model/__module.model.layers.19/__module.model.layers.19.mlp/__module.model.layers.19.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.359 : Tensor = prim::GetAttr[name="weight"](%up_proj.39)
  %3048 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3039, %weight.359), scope: __module.model/__module.model.layers.19/__module.model.layers.19.mlp/__module.model.layers.19.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3049 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%3046, %3048), scope: __module.model/__module.model.layers.19/__module.model.layers.19.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.361 : Tensor = prim::GetAttr[name="weight"](%down_proj.39)
  %hidden_states.597 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3049, %weight.361), scope: __module.model/__module.model.layers.19/__module.model.layers.19.mlp/__module.model.layers.19.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.599 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3040, %hidden_states.597, %47), scope: __module.model/__module.model.layers.19 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.41 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_263.MistralMLP = prim::GetAttr[name="mlp"](%_20)
  %post_attention_layernorm.41 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_265.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_20)
  %self_attn.41 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_258.MistralAttention = prim::GetAttr[name="self_attn"](%_20)
  %input_layernorm.41 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_264.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_20)
  %weight.363 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.41)
  %hidden_states.601 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.599, %33, %42, %42, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3059 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.601, %40), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3060 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm
  %variance.81 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3059, %3060, %25, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3062 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.81, %24, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3063 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3062), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.603 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.601, %3063), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.605 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.603, %33, %42, %42, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.607 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.363, %hidden_states.605), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3067 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.607, %hidden_states.601)
  %3068 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3069 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3067)
  %o_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_257.Linear = prim::GetAttr[name="o_proj"](%self_attn.41)
  %v_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_256.Linear = prim::GetAttr[name="v_proj"](%self_attn.41)
  %k_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_255.Linear = prim::GetAttr[name="k_proj"](%self_attn.41)
  %q_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_254.Linear = prim::GetAttr[name="q_proj"](%self_attn.41)
  %3074 : int = aten::size(%3068, %45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %3075 : int = aten::size(%3068, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.365 : Tensor = prim::GetAttr[name="weight"](%q_proj.41)
  %3077 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3068, %weight.365), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn/__module.model.layers.20.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3078 : int[] = prim::ListConstruct(%3074, %3075, %35, %30), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3079 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%3077, %3078), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.41 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3079, %47, %40), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.367 : Tensor = prim::GetAttr[name="weight"](%k_proj.41)
  %3082 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3068, %weight.367), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn/__module.model.layers.20.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3083 : int[] = prim::ListConstruct(%3074, %3075, %35, %30), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3084 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3082, %3083), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.41 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3084, %47, %40), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.369 : Tensor = prim::GetAttr[name="weight"](%v_proj.41)
  %3087 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3068, %weight.369), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn/__module.model.layers.20.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3088 : int[] = prim::ListConstruct(%3074, %3075, %35, %30), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3089 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3087, %3088), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.613 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3089, %47, %40), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.45 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.45 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %3093 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.41, %cos.45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3094 : int = aten::size(%q.41, %38), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3095 : Long(device=cpu) = prim::NumToTensor(%3094), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3096 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3095, %29), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3097 : int = aten::Int(%3096), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %x1.81 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.41, %38, %45, %3097, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3099 : int = aten::size(%q.41, %38), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3100 : Long(device=cpu) = prim::NumToTensor(%3099), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3101 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3100, %29), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3102 : int = aten::Int(%3101), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %x2.81 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.41, %38, %3102, %39, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3104 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.81), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3105 : Tensor[] = prim::ListConstruct(%3104, %x1.81), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3106 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3105, %35), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3107 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3106, %sin.45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.41 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3093, %3107, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3109 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.41, %cos.45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3110 : int = aten::size(%k.41, %38), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3111 : Long(device=cpu) = prim::NumToTensor(%3110), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3112 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3111, %29), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3113 : int = aten::Int(%3112), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %x1.83 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.41, %38, %45, %3113, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3115 : int = aten::size(%k.41, %38), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3116 : Long(device=cpu) = prim::NumToTensor(%3115), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3117 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3116, %29), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3118 : int = aten::Int(%3117), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %x2.83 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.41, %38, %3118, %39, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3120 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.83), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3121 : Tensor[] = prim::ListConstruct(%3120, %x1.83), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3122 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3121, %35), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3123 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3122, %sin.45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.609 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3109, %3123, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3125 : int = aten::size(%hidden_states.609, %45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3126 : int = aten::size(%hidden_states.609, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.81 : Long(device=cpu) = prim::NumToTensor(%3126), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3128 : int = aten::size(%hidden_states.609, %40), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3129 : int = aten::size(%hidden_states.609, %38), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3130 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.609, %45, %45, %39, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3131 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3130, %47, %45, %39, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3132 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3131, %40), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3133 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3132, %38, %45, %39, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3134 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3133, %37, %45, %39, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3135 : int[] = prim::ListConstruct(%3125, %3126, %37, %3128, %3129), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %hidden_states.611 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3134, %3135, %42), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3137 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.81, %28), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3138 : int = aten::Int(%3137), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3139 : int[] = prim::ListConstruct(%3125, %3138, %3128, %3129), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %key.41 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.611, %3139), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3141 : int = aten::size(%hidden_states.613, %45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3142 : int = aten::size(%hidden_states.613, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.83 : Long(device=cpu) = prim::NumToTensor(%3142), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3144 : int = aten::size(%hidden_states.613, %40), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3145 : int = aten::size(%hidden_states.613, %38), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3146 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.613, %45, %45, %39, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3147 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3146, %47, %45, %39, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3148 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3147, %40), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3149 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3148, %38, %45, %39, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3150 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3149, %37, %45, %39, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3151 : int[] = prim::ListConstruct(%3141, %3142, %37, %3144, %3145), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %hidden_states.615 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3150, %3151, %42), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3153 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.83, %28), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3154 : int = aten::Int(%3153), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3155 : int[] = prim::ListConstruct(%3141, %3154, %3144, %3145), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %value.41 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.615, %3155), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3157 : int = aten::size(%key.41, %40), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3158 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3159 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3158, %47, %45, %39, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3160 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3159, %40, %45, %39, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.47 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3160, %38, %45, %3157, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.81 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.41, %key.41, %value.41, %attention_mask.47, %27, %42, %26, %42), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %3163 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.81, %47, %40), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.83 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3163, %45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %3165 : int[] = prim::ListConstruct(%3074, %3075, %35), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn
  %3166 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.83, %3165), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %3167 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%3166, %45), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.371 : Tensor = prim::GetAttr[name="weight"](%o_proj.41)
  %hidden_states.617 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3167, %weight.371), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn/__module.model.layers.20.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.619 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3069, %hidden_states.617, %47), scope: __module.model/__module.model.layers.20 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.373 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.41)
  %hidden_states.621 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.619, %33, %42, %42, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3173 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.621, %40), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3174 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm
  %variance.83 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3173, %3174, %25, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3176 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.83, %24, %47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3177 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3176), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.623 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.621, %3177), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.625 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.623, %33, %42, %42, %44), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3180 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.373, %hidden_states.625), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3181 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%3180, %hidden_states.621)
  %3182 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3183 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3181)
  %down_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_261.Linear = prim::GetAttr[name="down_proj"](%mlp.41)
  %up_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_260.Linear = prim::GetAttr[name="up_proj"](%mlp.41)
  %gate_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_259.Linear = prim::GetAttr[name="gate_proj"](%mlp.41)
  %weight.375 : Tensor = prim::GetAttr[name="weight"](%gate_proj.41)
  %input.41 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3182, %weight.375), scope: __module.model/__module.model.layers.20/__module.model.layers.20.mlp/__module.model.layers.20.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3189 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.41), scope: __module.model/__module.model.layers.20/__module.model.layers.20.mlp/__module.model.layers.20.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.377 : Tensor = prim::GetAttr[name="weight"](%up_proj.41)
  %3191 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3182, %weight.377), scope: __module.model/__module.model.layers.20/__module.model.layers.20.mlp/__module.model.layers.20.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3192 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%3189, %3191), scope: __module.model/__module.model.layers.20/__module.model.layers.20.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.379 : Tensor = prim::GetAttr[name="weight"](%down_proj.41)
  %hidden_states.627 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3192, %weight.379), scope: __module.model/__module.model.layers.20/__module.model.layers.20.mlp/__module.model.layers.20.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.629 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3183, %hidden_states.627, %47), scope: __module.model/__module.model.layers.20 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.43 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_276.MistralMLP = prim::GetAttr[name="mlp"](%_21)
  %post_attention_layernorm.43 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_278.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_21)
  %self_attn.43 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_271.MistralAttention = prim::GetAttr[name="self_attn"](%_21)
  %input_layernorm.43 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_277.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_21)
  %weight.381 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.43)
  %hidden_states.631 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.629, %33, %42, %42, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3202 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.631, %40), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3203 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm
  %variance.85 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3202, %3203, %25, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3205 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.85, %24, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3206 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3205), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.633 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.631, %3206), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.635 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.633, %33, %42, %42, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.637 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.381, %hidden_states.635), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3210 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.637, %hidden_states.631)
  %3211 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3212 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3210)
  %o_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_270.Linear = prim::GetAttr[name="o_proj"](%self_attn.43)
  %v_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_269.Linear = prim::GetAttr[name="v_proj"](%self_attn.43)
  %k_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_268.Linear = prim::GetAttr[name="k_proj"](%self_attn.43)
  %q_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_267.Linear = prim::GetAttr[name="q_proj"](%self_attn.43)
  %3217 : int = aten::size(%3211, %45), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %3218 : int = aten::size(%3211, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.383 : Tensor = prim::GetAttr[name="weight"](%q_proj.43)
  %3220 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3211, %weight.383), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn/__module.model.layers.21.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3221 : int[] = prim::ListConstruct(%3217, %3218, %35, %30), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3222 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%3220, %3221), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.43 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3222, %47, %40), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.385 : Tensor = prim::GetAttr[name="weight"](%k_proj.43)
  %3225 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3211, %weight.385), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn/__module.model.layers.21.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3226 : int[] = prim::ListConstruct(%3217, %3218, %35, %30), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3227 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3225, %3226), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.43 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3227, %47, %40), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.387 : Tensor = prim::GetAttr[name="weight"](%v_proj.43)
  %3230 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3211, %weight.387), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn/__module.model.layers.21.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3231 : int[] = prim::ListConstruct(%3217, %3218, %35, %30), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3232 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3230, %3231), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.643 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3232, %47, %40), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.47 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.47 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %3236 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.43, %cos.47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3237 : int = aten::size(%q.43, %38), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3238 : Long(device=cpu) = prim::NumToTensor(%3237), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3239 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3238, %29), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3240 : int = aten::Int(%3239), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %x1.85 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.43, %38, %45, %3240, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3242 : int = aten::size(%q.43, %38), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3243 : Long(device=cpu) = prim::NumToTensor(%3242), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3244 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3243, %29), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3245 : int = aten::Int(%3244), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %x2.85 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.43, %38, %3245, %39, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3247 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.85), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3248 : Tensor[] = prim::ListConstruct(%3247, %x1.85), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3249 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3248, %35), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3250 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3249, %sin.47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.43 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3236, %3250, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3252 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.43, %cos.47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3253 : int = aten::size(%k.43, %38), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3254 : Long(device=cpu) = prim::NumToTensor(%3253), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3255 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3254, %29), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3256 : int = aten::Int(%3255), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %x1.87 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.43, %38, %45, %3256, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3258 : int = aten::size(%k.43, %38), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3259 : Long(device=cpu) = prim::NumToTensor(%3258), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3260 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3259, %29), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3261 : int = aten::Int(%3260), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %x2.87 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.43, %38, %3261, %39, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3263 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.87), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3264 : Tensor[] = prim::ListConstruct(%3263, %x1.87), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3265 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3264, %35), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3266 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3265, %sin.47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.639 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3252, %3266, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3268 : int = aten::size(%hidden_states.639, %45), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3269 : int = aten::size(%hidden_states.639, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.85 : Long(device=cpu) = prim::NumToTensor(%3269), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3271 : int = aten::size(%hidden_states.639, %40), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3272 : int = aten::size(%hidden_states.639, %38), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3273 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.639, %45, %45, %39, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3274 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3273, %47, %45, %39, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3275 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3274, %40), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3276 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3275, %38, %45, %39, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3277 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3276, %37, %45, %39, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3278 : int[] = prim::ListConstruct(%3268, %3269, %37, %3271, %3272), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %hidden_states.641 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3277, %3278, %42), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3280 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.85, %28), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3281 : int = aten::Int(%3280), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3282 : int[] = prim::ListConstruct(%3268, %3281, %3271, %3272), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %key.43 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.641, %3282), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3284 : int = aten::size(%hidden_states.643, %45), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3285 : int = aten::size(%hidden_states.643, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.87 : Long(device=cpu) = prim::NumToTensor(%3285), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3287 : int = aten::size(%hidden_states.643, %40), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3288 : int = aten::size(%hidden_states.643, %38), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3289 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.643, %45, %45, %39, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3290 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3289, %47, %45, %39, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3291 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3290, %40), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3292 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3291, %38, %45, %39, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3293 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3292, %37, %45, %39, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3294 : int[] = prim::ListConstruct(%3284, %3285, %37, %3287, %3288), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %hidden_states.645 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3293, %3294, %42), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3296 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.87, %28), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3297 : int = aten::Int(%3296), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3298 : int[] = prim::ListConstruct(%3284, %3297, %3287, %3288), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %value.43 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.645, %3298), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3300 : int = aten::size(%key.43, %40), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3301 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3302 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3301, %47, %45, %39, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3303 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3302, %40, %45, %39, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.49 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3303, %38, %45, %3300, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.85 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.43, %key.43, %value.43, %attention_mask.49, %27, %42, %26, %42), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %3306 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.85, %47, %40), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.87 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3306, %45), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %3308 : int[] = prim::ListConstruct(%3217, %3218, %35), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn
  %3309 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.87, %3308), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %3310 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%3309, %45), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.389 : Tensor = prim::GetAttr[name="weight"](%o_proj.43)
  %hidden_states.647 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3310, %weight.389), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn/__module.model.layers.21.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.649 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3212, %hidden_states.647, %47), scope: __module.model/__module.model.layers.21 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.391 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.43)
  %hidden_states.651 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.649, %33, %42, %42, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3316 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.651, %40), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3317 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm
  %variance.87 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3316, %3317, %25, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3319 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.87, %24, %47), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3320 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3319), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.653 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.651, %3320), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.655 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.653, %33, %42, %42, %44), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3323 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.391, %hidden_states.655), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3324 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%3323, %hidden_states.651)
  %3325 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3326 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3324)
  %down_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_274.Linear = prim::GetAttr[name="down_proj"](%mlp.43)
  %up_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_273.Linear = prim::GetAttr[name="up_proj"](%mlp.43)
  %gate_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_272.Linear = prim::GetAttr[name="gate_proj"](%mlp.43)
  %weight.393 : Tensor = prim::GetAttr[name="weight"](%gate_proj.43)
  %input.43 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3325, %weight.393), scope: __module.model/__module.model.layers.21/__module.model.layers.21.mlp/__module.model.layers.21.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3332 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.43), scope: __module.model/__module.model.layers.21/__module.model.layers.21.mlp/__module.model.layers.21.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.395 : Tensor = prim::GetAttr[name="weight"](%up_proj.43)
  %3334 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3325, %weight.395), scope: __module.model/__module.model.layers.21/__module.model.layers.21.mlp/__module.model.layers.21.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3335 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%3332, %3334), scope: __module.model/__module.model.layers.21/__module.model.layers.21.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.397 : Tensor = prim::GetAttr[name="weight"](%down_proj.43)
  %hidden_states.657 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3335, %weight.397), scope: __module.model/__module.model.layers.21/__module.model.layers.21.mlp/__module.model.layers.21.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.659 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3326, %hidden_states.657, %47), scope: __module.model/__module.model.layers.21 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.45 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_289.MistralMLP = prim::GetAttr[name="mlp"](%_22)
  %post_attention_layernorm.45 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_291.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_22)
  %self_attn.45 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_284.MistralAttention = prim::GetAttr[name="self_attn"](%_22)
  %input_layernorm.45 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_290.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_22)
  %weight.399 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.45)
  %hidden_states.661 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.659, %33, %42, %42, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3345 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.661, %40), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3346 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm
  %variance.89 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3345, %3346, %25, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3348 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.89, %24, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3349 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3348), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.663 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.661, %3349), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.665 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.663, %33, %42, %42, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.667 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.399, %hidden_states.665), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3353 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.667, %hidden_states.661)
  %3354 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3355 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3353)
  %o_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_283.Linear = prim::GetAttr[name="o_proj"](%self_attn.45)
  %v_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_282.Linear = prim::GetAttr[name="v_proj"](%self_attn.45)
  %k_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_281.Linear = prim::GetAttr[name="k_proj"](%self_attn.45)
  %q_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_280.Linear = prim::GetAttr[name="q_proj"](%self_attn.45)
  %3360 : int = aten::size(%3354, %45), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %3361 : int = aten::size(%3354, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.401 : Tensor = prim::GetAttr[name="weight"](%q_proj.45)
  %3363 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3354, %weight.401), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn/__module.model.layers.22.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3364 : int[] = prim::ListConstruct(%3360, %3361, %35, %30), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3365 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%3363, %3364), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.45 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3365, %47, %40), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.403 : Tensor = prim::GetAttr[name="weight"](%k_proj.45)
  %3368 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3354, %weight.403), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn/__module.model.layers.22.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3369 : int[] = prim::ListConstruct(%3360, %3361, %35, %30), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3370 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3368, %3369), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.45 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3370, %47, %40), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.405 : Tensor = prim::GetAttr[name="weight"](%v_proj.45)
  %3373 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3354, %weight.405), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn/__module.model.layers.22.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3374 : int[] = prim::ListConstruct(%3360, %3361, %35, %30), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3375 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3373, %3374), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.673 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3375, %47, %40), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.49 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.49 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %3379 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.45, %cos.49), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3380 : int = aten::size(%q.45, %38), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3381 : Long(device=cpu) = prim::NumToTensor(%3380), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3382 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3381, %29), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3383 : int = aten::Int(%3382), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %x1.89 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.45, %38, %45, %3383, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3385 : int = aten::size(%q.45, %38), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3386 : Long(device=cpu) = prim::NumToTensor(%3385), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3387 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3386, %29), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3388 : int = aten::Int(%3387), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %x2.89 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.45, %38, %3388, %39, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3390 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.89), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3391 : Tensor[] = prim::ListConstruct(%3390, %x1.89), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3392 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3391, %35), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3393 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3392, %sin.49), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.45 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3379, %3393, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3395 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.45, %cos.49), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3396 : int = aten::size(%k.45, %38), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3397 : Long(device=cpu) = prim::NumToTensor(%3396), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3398 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3397, %29), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3399 : int = aten::Int(%3398), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %x1.91 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.45, %38, %45, %3399, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3401 : int = aten::size(%k.45, %38), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3402 : Long(device=cpu) = prim::NumToTensor(%3401), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3403 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3402, %29), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3404 : int = aten::Int(%3403), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %x2.91 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.45, %38, %3404, %39, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3406 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.91), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3407 : Tensor[] = prim::ListConstruct(%3406, %x1.91), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3408 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3407, %35), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3409 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3408, %sin.49), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.669 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3395, %3409, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3411 : int = aten::size(%hidden_states.669, %45), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3412 : int = aten::size(%hidden_states.669, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.89 : Long(device=cpu) = prim::NumToTensor(%3412), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3414 : int = aten::size(%hidden_states.669, %40), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3415 : int = aten::size(%hidden_states.669, %38), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3416 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.669, %45, %45, %39, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3417 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3416, %47, %45, %39, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3418 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3417, %40), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3419 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3418, %38, %45, %39, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3420 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3419, %37, %45, %39, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3421 : int[] = prim::ListConstruct(%3411, %3412, %37, %3414, %3415), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %hidden_states.671 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3420, %3421, %42), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3423 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.89, %28), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3424 : int = aten::Int(%3423), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3425 : int[] = prim::ListConstruct(%3411, %3424, %3414, %3415), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %key.45 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.671, %3425), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3427 : int = aten::size(%hidden_states.673, %45), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3428 : int = aten::size(%hidden_states.673, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.91 : Long(device=cpu) = prim::NumToTensor(%3428), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3430 : int = aten::size(%hidden_states.673, %40), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3431 : int = aten::size(%hidden_states.673, %38), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3432 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.673, %45, %45, %39, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3433 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3432, %47, %45, %39, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3434 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3433, %40), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3435 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3434, %38, %45, %39, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3436 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3435, %37, %45, %39, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3437 : int[] = prim::ListConstruct(%3427, %3428, %37, %3430, %3431), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %hidden_states.675 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3436, %3437, %42), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3439 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.91, %28), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3440 : int = aten::Int(%3439), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3441 : int[] = prim::ListConstruct(%3427, %3440, %3430, %3431), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %value.45 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.675, %3441), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3443 : int = aten::size(%key.45, %40), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3444 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3445 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3444, %47, %45, %39, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3446 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3445, %40, %45, %39, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.51 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3446, %38, %45, %3443, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.89 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.45, %key.45, %value.45, %attention_mask.51, %27, %42, %26, %42), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %3449 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.89, %47, %40), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.91 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3449, %45), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %3451 : int[] = prim::ListConstruct(%3360, %3361, %35), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn
  %3452 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.91, %3451), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %3453 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%3452, %45), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.407 : Tensor = prim::GetAttr[name="weight"](%o_proj.45)
  %hidden_states.677 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3453, %weight.407), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn/__module.model.layers.22.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.679 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3355, %hidden_states.677, %47), scope: __module.model/__module.model.layers.22 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.409 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.45)
  %hidden_states.681 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.679, %33, %42, %42, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3459 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.681, %40), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3460 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm
  %variance.91 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3459, %3460, %25, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3462 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.91, %24, %47), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3463 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3462), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.683 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.681, %3463), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.685 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.683, %33, %42, %42, %44), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3466 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.409, %hidden_states.685), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3467 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%3466, %hidden_states.681)
  %3468 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3469 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3467)
  %down_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_287.Linear = prim::GetAttr[name="down_proj"](%mlp.45)
  %up_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_286.Linear = prim::GetAttr[name="up_proj"](%mlp.45)
  %gate_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_285.Linear = prim::GetAttr[name="gate_proj"](%mlp.45)
  %weight.411 : Tensor = prim::GetAttr[name="weight"](%gate_proj.45)
  %input.45 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3468, %weight.411), scope: __module.model/__module.model.layers.22/__module.model.layers.22.mlp/__module.model.layers.22.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3475 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.45), scope: __module.model/__module.model.layers.22/__module.model.layers.22.mlp/__module.model.layers.22.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.413 : Tensor = prim::GetAttr[name="weight"](%up_proj.45)
  %3477 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3468, %weight.413), scope: __module.model/__module.model.layers.22/__module.model.layers.22.mlp/__module.model.layers.22.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3478 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%3475, %3477), scope: __module.model/__module.model.layers.22/__module.model.layers.22.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.415 : Tensor = prim::GetAttr[name="weight"](%down_proj.45)
  %hidden_states.687 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3478, %weight.415), scope: __module.model/__module.model.layers.22/__module.model.layers.22.mlp/__module.model.layers.22.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.689 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3469, %hidden_states.687, %47), scope: __module.model/__module.model.layers.22 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.47 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_302.MistralMLP = prim::GetAttr[name="mlp"](%_23)
  %post_attention_layernorm.47 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_304.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_23)
  %self_attn.47 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_297.MistralAttention = prim::GetAttr[name="self_attn"](%_23)
  %input_layernorm.47 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_303.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_23)
  %weight.417 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.47)
  %hidden_states.691 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.689, %33, %42, %42, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3488 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.691, %40), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3489 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm
  %variance.93 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3488, %3489, %25, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3491 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.93, %24, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3492 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3491), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.693 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.691, %3492), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.695 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.693, %33, %42, %42, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.697 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.417, %hidden_states.695), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3496 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.697, %hidden_states.691)
  %3497 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3498 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3496)
  %o_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_296.Linear = prim::GetAttr[name="o_proj"](%self_attn.47)
  %v_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_295.Linear = prim::GetAttr[name="v_proj"](%self_attn.47)
  %k_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_294.Linear = prim::GetAttr[name="k_proj"](%self_attn.47)
  %q_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_293.Linear = prim::GetAttr[name="q_proj"](%self_attn.47)
  %3503 : int = aten::size(%3497, %45), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %3504 : int = aten::size(%3497, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.419 : Tensor = prim::GetAttr[name="weight"](%q_proj.47)
  %3506 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3497, %weight.419), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn/__module.model.layers.23.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3507 : int[] = prim::ListConstruct(%3503, %3504, %35, %30), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3508 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%3506, %3507), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.47 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3508, %47, %40), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.421 : Tensor = prim::GetAttr[name="weight"](%k_proj.47)
  %3511 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3497, %weight.421), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn/__module.model.layers.23.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3512 : int[] = prim::ListConstruct(%3503, %3504, %35, %30), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3513 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3511, %3512), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.47 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3513, %47, %40), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.423 : Tensor = prim::GetAttr[name="weight"](%v_proj.47)
  %3516 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3497, %weight.423), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn/__module.model.layers.23.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3517 : int[] = prim::ListConstruct(%3503, %3504, %35, %30), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3518 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3516, %3517), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.703 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3518, %47, %40), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.51 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.51 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %3522 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.47, %cos.51), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3523 : int = aten::size(%q.47, %38), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3524 : Long(device=cpu) = prim::NumToTensor(%3523), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3525 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3524, %29), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3526 : int = aten::Int(%3525), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %x1.93 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.47, %38, %45, %3526, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3528 : int = aten::size(%q.47, %38), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3529 : Long(device=cpu) = prim::NumToTensor(%3528), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3530 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3529, %29), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3531 : int = aten::Int(%3530), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %x2.93 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.47, %38, %3531, %39, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3533 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.93), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3534 : Tensor[] = prim::ListConstruct(%3533, %x1.93), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3535 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3534, %35), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3536 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3535, %sin.51), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.47 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3522, %3536, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3538 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.47, %cos.51), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3539 : int = aten::size(%k.47, %38), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3540 : Long(device=cpu) = prim::NumToTensor(%3539), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3541 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3540, %29), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3542 : int = aten::Int(%3541), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %x1.95 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.47, %38, %45, %3542, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3544 : int = aten::size(%k.47, %38), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3545 : Long(device=cpu) = prim::NumToTensor(%3544), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3546 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3545, %29), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3547 : int = aten::Int(%3546), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %x2.95 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.47, %38, %3547, %39, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3549 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.95), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3550 : Tensor[] = prim::ListConstruct(%3549, %x1.95), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3551 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3550, %35), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3552 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3551, %sin.51), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.699 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3538, %3552, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3554 : int = aten::size(%hidden_states.699, %45), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3555 : int = aten::size(%hidden_states.699, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.93 : Long(device=cpu) = prim::NumToTensor(%3555), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3557 : int = aten::size(%hidden_states.699, %40), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3558 : int = aten::size(%hidden_states.699, %38), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3559 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.699, %45, %45, %39, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3560 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3559, %47, %45, %39, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3561 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3560, %40), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3562 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3561, %38, %45, %39, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3563 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3562, %37, %45, %39, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3564 : int[] = prim::ListConstruct(%3554, %3555, %37, %3557, %3558), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %hidden_states.701 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3563, %3564, %42), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3566 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.93, %28), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3567 : int = aten::Int(%3566), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3568 : int[] = prim::ListConstruct(%3554, %3567, %3557, %3558), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %key.47 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.701, %3568), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3570 : int = aten::size(%hidden_states.703, %45), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3571 : int = aten::size(%hidden_states.703, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.95 : Long(device=cpu) = prim::NumToTensor(%3571), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3573 : int = aten::size(%hidden_states.703, %40), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3574 : int = aten::size(%hidden_states.703, %38), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3575 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.703, %45, %45, %39, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3576 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3575, %47, %45, %39, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3577 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3576, %40), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3578 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3577, %38, %45, %39, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3579 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3578, %37, %45, %39, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3580 : int[] = prim::ListConstruct(%3570, %3571, %37, %3573, %3574), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %hidden_states.705 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3579, %3580, %42), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3582 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.95, %28), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3583 : int = aten::Int(%3582), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3584 : int[] = prim::ListConstruct(%3570, %3583, %3573, %3574), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %value.47 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.705, %3584), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3586 : int = aten::size(%key.47, %40), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3587 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3588 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3587, %47, %45, %39, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3589 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3588, %40, %45, %39, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.53 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3589, %38, %45, %3586, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.93 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.47, %key.47, %value.47, %attention_mask.53, %27, %42, %26, %42), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %3592 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.93, %47, %40), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.95 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3592, %45), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %3594 : int[] = prim::ListConstruct(%3503, %3504, %35), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn
  %3595 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.95, %3594), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %3596 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%3595, %45), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.425 : Tensor = prim::GetAttr[name="weight"](%o_proj.47)
  %hidden_states.707 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3596, %weight.425), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn/__module.model.layers.23.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.709 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3498, %hidden_states.707, %47), scope: __module.model/__module.model.layers.23 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.427 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.47)
  %hidden_states.711 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.709, %33, %42, %42, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3602 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.711, %40), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3603 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm
  %variance.95 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3602, %3603, %25, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3605 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.95, %24, %47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3606 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3605), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.713 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.711, %3606), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.715 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.713, %33, %42, %42, %44), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3609 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.427, %hidden_states.715), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3610 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%3609, %hidden_states.711)
  %3611 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3612 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3610)
  %down_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_300.Linear = prim::GetAttr[name="down_proj"](%mlp.47)
  %up_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_299.Linear = prim::GetAttr[name="up_proj"](%mlp.47)
  %gate_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_298.Linear = prim::GetAttr[name="gate_proj"](%mlp.47)
  %weight.429 : Tensor = prim::GetAttr[name="weight"](%gate_proj.47)
  %input.47 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3611, %weight.429), scope: __module.model/__module.model.layers.23/__module.model.layers.23.mlp/__module.model.layers.23.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3618 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.mlp/__module.model.layers.23.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.431 : Tensor = prim::GetAttr[name="weight"](%up_proj.47)
  %3620 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3611, %weight.431), scope: __module.model/__module.model.layers.23/__module.model.layers.23.mlp/__module.model.layers.23.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3621 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%3618, %3620), scope: __module.model/__module.model.layers.23/__module.model.layers.23.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.433 : Tensor = prim::GetAttr[name="weight"](%down_proj.47)
  %hidden_states.717 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3621, %weight.433), scope: __module.model/__module.model.layers.23/__module.model.layers.23.mlp/__module.model.layers.23.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.719 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3612, %hidden_states.717, %47), scope: __module.model/__module.model.layers.23 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.49 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_315.MistralMLP = prim::GetAttr[name="mlp"](%_24)
  %post_attention_layernorm.49 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_317.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_24)
  %self_attn.49 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_310.MistralAttention = prim::GetAttr[name="self_attn"](%_24)
  %input_layernorm.49 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_316.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_24)
  %weight.435 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.49)
  %hidden_states.721 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.719, %33, %42, %42, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3631 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.721, %40), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3632 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm
  %variance.97 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3631, %3632, %25, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3634 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.97, %24, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3635 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3634), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.723 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.721, %3635), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.725 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.723, %33, %42, %42, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.727 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.435, %hidden_states.725), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3639 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.727, %hidden_states.721)
  %3640 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3641 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3639)
  %o_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_309.Linear = prim::GetAttr[name="o_proj"](%self_attn.49)
  %v_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_308.Linear = prim::GetAttr[name="v_proj"](%self_attn.49)
  %k_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_307.Linear = prim::GetAttr[name="k_proj"](%self_attn.49)
  %q_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_306.Linear = prim::GetAttr[name="q_proj"](%self_attn.49)
  %3646 : int = aten::size(%3640, %45), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %3647 : int = aten::size(%3640, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.437 : Tensor = prim::GetAttr[name="weight"](%q_proj.49)
  %3649 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3640, %weight.437), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn/__module.model.layers.24.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3650 : int[] = prim::ListConstruct(%3646, %3647, %35, %30), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3651 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%3649, %3650), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.49 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3651, %47, %40), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.439 : Tensor = prim::GetAttr[name="weight"](%k_proj.49)
  %3654 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3640, %weight.439), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn/__module.model.layers.24.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3655 : int[] = prim::ListConstruct(%3646, %3647, %35, %30), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3656 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3654, %3655), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.49 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3656, %47, %40), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.441 : Tensor = prim::GetAttr[name="weight"](%v_proj.49)
  %3659 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3640, %weight.441), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn/__module.model.layers.24.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3660 : int[] = prim::ListConstruct(%3646, %3647, %35, %30), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3661 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3659, %3660), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.733 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3661, %47, %40), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.53 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.53 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %3665 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.49, %cos.53), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3666 : int = aten::size(%q.49, %38), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3667 : Long(device=cpu) = prim::NumToTensor(%3666), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3668 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3667, %29), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3669 : int = aten::Int(%3668), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %x1.97 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.49, %38, %45, %3669, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3671 : int = aten::size(%q.49, %38), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3672 : Long(device=cpu) = prim::NumToTensor(%3671), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3673 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3672, %29), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3674 : int = aten::Int(%3673), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %x2.97 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.49, %38, %3674, %39, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3676 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.97), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3677 : Tensor[] = prim::ListConstruct(%3676, %x1.97), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3678 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3677, %35), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3679 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3678, %sin.53), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.49 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3665, %3679, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3681 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.49, %cos.53), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3682 : int = aten::size(%k.49, %38), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3683 : Long(device=cpu) = prim::NumToTensor(%3682), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3684 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3683, %29), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3685 : int = aten::Int(%3684), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %x1.99 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.49, %38, %45, %3685, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3687 : int = aten::size(%k.49, %38), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3688 : Long(device=cpu) = prim::NumToTensor(%3687), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3689 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3688, %29), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3690 : int = aten::Int(%3689), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %x2.99 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.49, %38, %3690, %39, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3692 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.99), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3693 : Tensor[] = prim::ListConstruct(%3692, %x1.99), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3694 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3693, %35), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3695 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3694, %sin.53), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.729 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3681, %3695, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3697 : int = aten::size(%hidden_states.729, %45), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3698 : int = aten::size(%hidden_states.729, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.97 : Long(device=cpu) = prim::NumToTensor(%3698), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3700 : int = aten::size(%hidden_states.729, %40), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3701 : int = aten::size(%hidden_states.729, %38), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3702 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.729, %45, %45, %39, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3703 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3702, %47, %45, %39, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3704 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3703, %40), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3705 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3704, %38, %45, %39, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3706 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3705, %37, %45, %39, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3707 : int[] = prim::ListConstruct(%3697, %3698, %37, %3700, %3701), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %hidden_states.731 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3706, %3707, %42), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3709 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.97, %28), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3710 : int = aten::Int(%3709), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3711 : int[] = prim::ListConstruct(%3697, %3710, %3700, %3701), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %key.49 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.731, %3711), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3713 : int = aten::size(%hidden_states.733, %45), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3714 : int = aten::size(%hidden_states.733, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.99 : Long(device=cpu) = prim::NumToTensor(%3714), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3716 : int = aten::size(%hidden_states.733, %40), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3717 : int = aten::size(%hidden_states.733, %38), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3718 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.733, %45, %45, %39, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3719 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3718, %47, %45, %39, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3720 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3719, %40), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3721 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3720, %38, %45, %39, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3722 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3721, %37, %45, %39, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3723 : int[] = prim::ListConstruct(%3713, %3714, %37, %3716, %3717), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %hidden_states.735 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3722, %3723, %42), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3725 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.99, %28), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3726 : int = aten::Int(%3725), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3727 : int[] = prim::ListConstruct(%3713, %3726, %3716, %3717), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %value.49 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.735, %3727), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3729 : int = aten::size(%key.49, %40), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3730 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3731 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3730, %47, %45, %39, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3732 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3731, %40, %45, %39, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.55 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3732, %38, %45, %3729, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.97 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.49, %key.49, %value.49, %attention_mask.55, %27, %42, %26, %42), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %3735 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.97, %47, %40), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.99 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3735, %45), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %3737 : int[] = prim::ListConstruct(%3646, %3647, %35), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn
  %3738 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.99, %3737), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %3739 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%3738, %45), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.443 : Tensor = prim::GetAttr[name="weight"](%o_proj.49)
  %hidden_states.737 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3739, %weight.443), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn/__module.model.layers.24.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.739 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3641, %hidden_states.737, %47), scope: __module.model/__module.model.layers.24 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.445 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.49)
  %hidden_states.741 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.739, %33, %42, %42, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3745 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.741, %40), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3746 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm
  %variance.99 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3745, %3746, %25, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3748 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.99, %24, %47), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3749 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3748), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.743 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.741, %3749), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.745 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.743, %33, %42, %42, %44), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3752 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.445, %hidden_states.745), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3753 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%3752, %hidden_states.741)
  %3754 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3755 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3753)
  %down_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_313.Linear = prim::GetAttr[name="down_proj"](%mlp.49)
  %up_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_312.Linear = prim::GetAttr[name="up_proj"](%mlp.49)
  %gate_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_311.Linear = prim::GetAttr[name="gate_proj"](%mlp.49)
  %weight.447 : Tensor = prim::GetAttr[name="weight"](%gate_proj.49)
  %input.49 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3754, %weight.447), scope: __module.model/__module.model.layers.24/__module.model.layers.24.mlp/__module.model.layers.24.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3761 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.49), scope: __module.model/__module.model.layers.24/__module.model.layers.24.mlp/__module.model.layers.24.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.449 : Tensor = prim::GetAttr[name="weight"](%up_proj.49)
  %3763 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3754, %weight.449), scope: __module.model/__module.model.layers.24/__module.model.layers.24.mlp/__module.model.layers.24.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3764 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%3761, %3763), scope: __module.model/__module.model.layers.24/__module.model.layers.24.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.451 : Tensor = prim::GetAttr[name="weight"](%down_proj.49)
  %hidden_states.747 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3764, %weight.451), scope: __module.model/__module.model.layers.24/__module.model.layers.24.mlp/__module.model.layers.24.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.749 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3755, %hidden_states.747, %47), scope: __module.model/__module.model.layers.24 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.51 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_328.MistralMLP = prim::GetAttr[name="mlp"](%_25)
  %post_attention_layernorm.51 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_330.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_25)
  %self_attn.51 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_323.MistralAttention = prim::GetAttr[name="self_attn"](%_25)
  %input_layernorm.51 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_329.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_25)
  %weight.453 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.51)
  %hidden_states.751 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.749, %33, %42, %42, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3774 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.751, %40), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3775 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm
  %variance.101 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3774, %3775, %25, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3777 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.101, %24, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3778 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3777), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.753 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.751, %3778), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.755 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.753, %33, %42, %42, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.757 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.453, %hidden_states.755), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3782 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.757, %hidden_states.751)
  %3783 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3784 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3782)
  %o_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_322.Linear = prim::GetAttr[name="o_proj"](%self_attn.51)
  %v_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_321.Linear = prim::GetAttr[name="v_proj"](%self_attn.51)
  %k_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_320.Linear = prim::GetAttr[name="k_proj"](%self_attn.51)
  %q_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_319.Linear = prim::GetAttr[name="q_proj"](%self_attn.51)
  %3789 : int = aten::size(%3783, %45), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %3790 : int = aten::size(%3783, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.455 : Tensor = prim::GetAttr[name="weight"](%q_proj.51)
  %3792 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3783, %weight.455), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn/__module.model.layers.25.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3793 : int[] = prim::ListConstruct(%3789, %3790, %35, %30), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3794 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%3792, %3793), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.51 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3794, %47, %40), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.457 : Tensor = prim::GetAttr[name="weight"](%k_proj.51)
  %3797 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3783, %weight.457), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn/__module.model.layers.25.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3798 : int[] = prim::ListConstruct(%3789, %3790, %35, %30), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3799 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3797, %3798), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.51 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3799, %47, %40), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.459 : Tensor = prim::GetAttr[name="weight"](%v_proj.51)
  %3802 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3783, %weight.459), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn/__module.model.layers.25.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3803 : int[] = prim::ListConstruct(%3789, %3790, %35, %30), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3804 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3802, %3803), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.763 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3804, %47, %40), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.55 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.55 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %3808 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.51, %cos.55), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3809 : int = aten::size(%q.51, %38), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3810 : Long(device=cpu) = prim::NumToTensor(%3809), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3811 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3810, %29), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3812 : int = aten::Int(%3811), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %x1.101 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.51, %38, %45, %3812, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3814 : int = aten::size(%q.51, %38), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3815 : Long(device=cpu) = prim::NumToTensor(%3814), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3816 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3815, %29), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3817 : int = aten::Int(%3816), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %x2.101 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.51, %38, %3817, %39, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3819 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.101), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3820 : Tensor[] = prim::ListConstruct(%3819, %x1.101), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3821 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3820, %35), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3822 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3821, %sin.55), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.51 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3808, %3822, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3824 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.51, %cos.55), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3825 : int = aten::size(%k.51, %38), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3826 : Long(device=cpu) = prim::NumToTensor(%3825), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3827 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3826, %29), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3828 : int = aten::Int(%3827), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %x1.103 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.51, %38, %45, %3828, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3830 : int = aten::size(%k.51, %38), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3831 : Long(device=cpu) = prim::NumToTensor(%3830), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3832 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3831, %29), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3833 : int = aten::Int(%3832), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %x2.103 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.51, %38, %3833, %39, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3835 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.103), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3836 : Tensor[] = prim::ListConstruct(%3835, %x1.103), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3837 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3836, %35), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3838 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3837, %sin.55), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.759 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3824, %3838, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3840 : int = aten::size(%hidden_states.759, %45), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3841 : int = aten::size(%hidden_states.759, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.101 : Long(device=cpu) = prim::NumToTensor(%3841), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3843 : int = aten::size(%hidden_states.759, %40), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3844 : int = aten::size(%hidden_states.759, %38), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3845 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.759, %45, %45, %39, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3846 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3845, %47, %45, %39, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3847 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3846, %40), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3848 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3847, %38, %45, %39, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3849 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3848, %37, %45, %39, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3850 : int[] = prim::ListConstruct(%3840, %3841, %37, %3843, %3844), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %hidden_states.761 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3849, %3850, %42), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3852 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.101, %28), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3853 : int = aten::Int(%3852), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3854 : int[] = prim::ListConstruct(%3840, %3853, %3843, %3844), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %key.51 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.761, %3854), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3856 : int = aten::size(%hidden_states.763, %45), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3857 : int = aten::size(%hidden_states.763, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.103 : Long(device=cpu) = prim::NumToTensor(%3857), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3859 : int = aten::size(%hidden_states.763, %40), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3860 : int = aten::size(%hidden_states.763, %38), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3861 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.763, %45, %45, %39, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3862 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3861, %47, %45, %39, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3863 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3862, %40), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3864 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3863, %38, %45, %39, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3865 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3864, %37, %45, %39, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3866 : int[] = prim::ListConstruct(%3856, %3857, %37, %3859, %3860), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %hidden_states.765 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3865, %3866, %42), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3868 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.103, %28), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3869 : int = aten::Int(%3868), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3870 : int[] = prim::ListConstruct(%3856, %3869, %3859, %3860), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %value.51 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.765, %3870), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3872 : int = aten::size(%key.51, %40), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3873 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3874 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3873, %47, %45, %39, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %3875 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3874, %40, %45, %39, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.57 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%3875, %38, %45, %3872, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.101 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.51, %key.51, %value.51, %attention_mask.57, %27, %42, %26, %42), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %3878 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.101, %47, %40), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.103 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3878, %45), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %3880 : int[] = prim::ListConstruct(%3789, %3790, %35), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn
  %3881 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.103, %3880), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %3882 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%3881, %45), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.461 : Tensor = prim::GetAttr[name="weight"](%o_proj.51)
  %hidden_states.767 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3882, %weight.461), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn/__module.model.layers.25.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.769 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3784, %hidden_states.767, %47), scope: __module.model/__module.model.layers.25 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.463 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.51)
  %hidden_states.771 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.769, %33, %42, %42, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3888 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.771, %40), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3889 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm
  %variance.103 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3888, %3889, %25, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3891 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.103, %24, %47), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3892 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3891), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.773 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.771, %3892), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.775 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.773, %33, %42, %42, %44), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3895 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.463, %hidden_states.775), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3896 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%3895, %hidden_states.771)
  %3897 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3898 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3896)
  %down_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_326.Linear = prim::GetAttr[name="down_proj"](%mlp.51)
  %up_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_325.Linear = prim::GetAttr[name="up_proj"](%mlp.51)
  %gate_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_324.Linear = prim::GetAttr[name="gate_proj"](%mlp.51)
  %weight.465 : Tensor = prim::GetAttr[name="weight"](%gate_proj.51)
  %input.51 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3897, %weight.465), scope: __module.model/__module.model.layers.25/__module.model.layers.25.mlp/__module.model.layers.25.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3904 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.51), scope: __module.model/__module.model.layers.25/__module.model.layers.25.mlp/__module.model.layers.25.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.467 : Tensor = prim::GetAttr[name="weight"](%up_proj.51)
  %3906 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3897, %weight.467), scope: __module.model/__module.model.layers.25/__module.model.layers.25.mlp/__module.model.layers.25.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3907 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%3904, %3906), scope: __module.model/__module.model.layers.25/__module.model.layers.25.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.469 : Tensor = prim::GetAttr[name="weight"](%down_proj.51)
  %hidden_states.777 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3907, %weight.469), scope: __module.model/__module.model.layers.25/__module.model.layers.25.mlp/__module.model.layers.25.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.779 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3898, %hidden_states.777, %47), scope: __module.model/__module.model.layers.25 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.53 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_341.MistralMLP = prim::GetAttr[name="mlp"](%_26)
  %post_attention_layernorm.53 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_343.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_26)
  %self_attn.53 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_336.MistralAttention = prim::GetAttr[name="self_attn"](%_26)
  %input_layernorm.53 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_342.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_26)
  %weight.471 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.53)
  %hidden_states.781 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.779, %33, %42, %42, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %3917 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.781, %40), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3918 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm
  %variance.105 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3917, %3918, %25, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %3920 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.105, %24, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %3921 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3920), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.783 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.781, %3921), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.785 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.783, %33, %42, %42, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.787 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.471, %hidden_states.785), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %3925 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.787, %hidden_states.781)
  %3926 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3927 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3925)
  %o_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_335.Linear = prim::GetAttr[name="o_proj"](%self_attn.53)
  %v_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_334.Linear = prim::GetAttr[name="v_proj"](%self_attn.53)
  %k_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_333.Linear = prim::GetAttr[name="k_proj"](%self_attn.53)
  %q_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_332.Linear = prim::GetAttr[name="q_proj"](%self_attn.53)
  %3932 : int = aten::size(%3926, %45), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %3933 : int = aten::size(%3926, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.473 : Tensor = prim::GetAttr[name="weight"](%q_proj.53)
  %3935 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3926, %weight.473), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn/__module.model.layers.26.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3936 : int[] = prim::ListConstruct(%3932, %3933, %35, %30), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %3937 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%3935, %3936), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.53 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3937, %47, %40), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.475 : Tensor = prim::GetAttr[name="weight"](%k_proj.53)
  %3940 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3926, %weight.475), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn/__module.model.layers.26.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3941 : int[] = prim::ListConstruct(%3932, %3933, %35, %30), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %3942 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3940, %3941), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.53 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3942, %47, %40), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.477 : Tensor = prim::GetAttr[name="weight"](%v_proj.53)
  %3945 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%3926, %weight.477), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn/__module.model.layers.26.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %3946 : int[] = prim::ListConstruct(%3932, %3933, %35, %30), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %3947 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%3945, %3946), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.793 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3947, %47, %40), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.57 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.57 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %3951 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.53, %cos.57), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3952 : int = aten::size(%q.53, %38), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3953 : Long(device=cpu) = prim::NumToTensor(%3952), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %3954 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3953, %29), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3955 : int = aten::Int(%3954), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %x1.105 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.53, %38, %45, %3955, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3957 : int = aten::size(%q.53, %38), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3958 : Long(device=cpu) = prim::NumToTensor(%3957), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %3959 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3958, %29), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3960 : int = aten::Int(%3959), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %x2.105 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.53, %38, %3960, %39, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3962 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.105), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3963 : Tensor[] = prim::ListConstruct(%3962, %x1.105), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %3964 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3963, %35), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3965 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3964, %sin.57), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.53 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3951, %3965, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %3967 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.53, %cos.57), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3968 : int = aten::size(%k.53, %38), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3969 : Long(device=cpu) = prim::NumToTensor(%3968), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %3970 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3969, %29), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3971 : int = aten::Int(%3970), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %x1.107 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.53, %38, %45, %3971, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %3973 : int = aten::size(%k.53, %38), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3974 : Long(device=cpu) = prim::NumToTensor(%3973), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %3975 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3974, %29), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %3976 : int = aten::Int(%3975), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %x2.107 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.53, %38, %3976, %39, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %3978 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.107), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3979 : Tensor[] = prim::ListConstruct(%3978, %x1.107), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %3980 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3979, %35), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %3981 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3980, %sin.57), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.789 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3967, %3981, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %3983 : int = aten::size(%hidden_states.789, %45), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3984 : int = aten::size(%hidden_states.789, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.105 : Long(device=cpu) = prim::NumToTensor(%3984), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %3986 : int = aten::size(%hidden_states.789, %40), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3987 : int = aten::size(%hidden_states.789, %38), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %3988 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.789, %45, %45, %39, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3989 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3988, %47, %45, %39, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3990 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3989, %40), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3991 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3990, %38, %45, %39, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3992 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%3991, %37, %45, %39, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3993 : int[] = prim::ListConstruct(%3983, %3984, %37, %3986, %3987), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %hidden_states.791 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%3992, %3993, %42), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %3995 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.105, %28), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3996 : int = aten::Int(%3995), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %3997 : int[] = prim::ListConstruct(%3983, %3996, %3986, %3987), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %key.53 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.791, %3997), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %3999 : int = aten::size(%hidden_states.793, %45), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4000 : int = aten::size(%hidden_states.793, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.107 : Long(device=cpu) = prim::NumToTensor(%4000), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4002 : int = aten::size(%hidden_states.793, %40), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4003 : int = aten::size(%hidden_states.793, %38), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4004 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.793, %45, %45, %39, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4005 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4004, %47, %45, %39, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4006 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4005, %40), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4007 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4006, %38, %45, %39, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4008 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4007, %37, %45, %39, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4009 : int[] = prim::ListConstruct(%3999, %4000, %37, %4002, %4003), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %hidden_states.795 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4008, %4009, %42), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4011 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.107, %28), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4012 : int = aten::Int(%4011), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4013 : int[] = prim::ListConstruct(%3999, %4012, %4002, %4003), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %value.53 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.795, %4013), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4015 : int = aten::size(%key.53, %40), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4016 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4017 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4016, %47, %45, %39, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4018 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4017, %40, %45, %39, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.59 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4018, %38, %45, %4015, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.105 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.53, %key.53, %value.53, %attention_mask.59, %27, %42, %26, %42), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %4021 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.105, %47, %40), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.107 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4021, %45), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %4023 : int[] = prim::ListConstruct(%3932, %3933, %35), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn
  %4024 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.107, %4023), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %4025 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%4024, %45), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.479 : Tensor = prim::GetAttr[name="weight"](%o_proj.53)
  %hidden_states.797 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4025, %weight.479), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn/__module.model.layers.26.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.799 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3927, %hidden_states.797, %47), scope: __module.model/__module.model.layers.26 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.481 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.53)
  %hidden_states.801 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.799, %33, %42, %42, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4031 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.801, %40), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4032 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm
  %variance.107 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4031, %4032, %25, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4034 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.107, %24, %47), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4035 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4034), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.803 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.801, %4035), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.805 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.803, %33, %42, %42, %44), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4038 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.481, %hidden_states.805), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4039 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%4038, %hidden_states.801)
  %4040 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4041 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4039)
  %down_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_339.Linear = prim::GetAttr[name="down_proj"](%mlp.53)
  %up_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_338.Linear = prim::GetAttr[name="up_proj"](%mlp.53)
  %gate_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_337.Linear = prim::GetAttr[name="gate_proj"](%mlp.53)
  %weight.483 : Tensor = prim::GetAttr[name="weight"](%gate_proj.53)
  %input.53 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4040, %weight.483), scope: __module.model/__module.model.layers.26/__module.model.layers.26.mlp/__module.model.layers.26.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4047 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.53), scope: __module.model/__module.model.layers.26/__module.model.layers.26.mlp/__module.model.layers.26.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.485 : Tensor = prim::GetAttr[name="weight"](%up_proj.53)
  %4049 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4040, %weight.485), scope: __module.model/__module.model.layers.26/__module.model.layers.26.mlp/__module.model.layers.26.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4050 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%4047, %4049), scope: __module.model/__module.model.layers.26/__module.model.layers.26.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.487 : Tensor = prim::GetAttr[name="weight"](%down_proj.53)
  %hidden_states.807 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4050, %weight.487), scope: __module.model/__module.model.layers.26/__module.model.layers.26.mlp/__module.model.layers.26.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.809 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4041, %hidden_states.807, %47), scope: __module.model/__module.model.layers.26 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.55 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_354.MistralMLP = prim::GetAttr[name="mlp"](%_27)
  %post_attention_layernorm.55 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_356.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_27)
  %self_attn.55 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_349.MistralAttention = prim::GetAttr[name="self_attn"](%_27)
  %input_layernorm.55 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_355.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_27)
  %weight.489 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.55)
  %hidden_states.811 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.809, %33, %42, %42, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4060 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.811, %40), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4061 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm
  %variance.109 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4060, %4061, %25, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4063 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.109, %24, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4064 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4063), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.813 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.811, %4064), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.815 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.813, %33, %42, %42, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.817 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.489, %hidden_states.815), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4068 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.817, %hidden_states.811)
  %4069 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4070 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4068)
  %o_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_348.Linear = prim::GetAttr[name="o_proj"](%self_attn.55)
  %v_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_347.Linear = prim::GetAttr[name="v_proj"](%self_attn.55)
  %k_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_346.Linear = prim::GetAttr[name="k_proj"](%self_attn.55)
  %q_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_345.Linear = prim::GetAttr[name="q_proj"](%self_attn.55)
  %4075 : int = aten::size(%4069, %45), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %4076 : int = aten::size(%4069, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.491 : Tensor = prim::GetAttr[name="weight"](%q_proj.55)
  %4078 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4069, %weight.491), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn/__module.model.layers.27.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4079 : int[] = prim::ListConstruct(%4075, %4076, %35, %30), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4080 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%4078, %4079), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.55 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4080, %47, %40), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.493 : Tensor = prim::GetAttr[name="weight"](%k_proj.55)
  %4083 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4069, %weight.493), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn/__module.model.layers.27.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4084 : int[] = prim::ListConstruct(%4075, %4076, %35, %30), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4085 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4083, %4084), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.55 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4085, %47, %40), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.495 : Tensor = prim::GetAttr[name="weight"](%v_proj.55)
  %4088 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4069, %weight.495), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn/__module.model.layers.27.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4089 : int[] = prim::ListConstruct(%4075, %4076, %35, %30), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4090 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4088, %4089), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.823 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4090, %47, %40), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.59 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.59 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %4094 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.55, %cos.59), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4095 : int = aten::size(%q.55, %38), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4096 : Long(device=cpu) = prim::NumToTensor(%4095), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4097 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4096, %29), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4098 : int = aten::Int(%4097), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %x1.109 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.55, %38, %45, %4098, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4100 : int = aten::size(%q.55, %38), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4101 : Long(device=cpu) = prim::NumToTensor(%4100), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4102 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4101, %29), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4103 : int = aten::Int(%4102), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %x2.109 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.55, %38, %4103, %39, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4105 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.109), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4106 : Tensor[] = prim::ListConstruct(%4105, %x1.109), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4107 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4106, %35), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4108 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4107, %sin.59), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.55 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4094, %4108, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4110 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.55, %cos.59), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4111 : int = aten::size(%k.55, %38), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4112 : Long(device=cpu) = prim::NumToTensor(%4111), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4113 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4112, %29), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4114 : int = aten::Int(%4113), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %x1.111 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.55, %38, %45, %4114, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4116 : int = aten::size(%k.55, %38), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4117 : Long(device=cpu) = prim::NumToTensor(%4116), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4118 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4117, %29), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4119 : int = aten::Int(%4118), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %x2.111 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.55, %38, %4119, %39, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4121 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.111), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4122 : Tensor[] = prim::ListConstruct(%4121, %x1.111), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4123 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4122, %35), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4124 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4123, %sin.59), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.819 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4110, %4124, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4126 : int = aten::size(%hidden_states.819, %45), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4127 : int = aten::size(%hidden_states.819, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.109 : Long(device=cpu) = prim::NumToTensor(%4127), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4129 : int = aten::size(%hidden_states.819, %40), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4130 : int = aten::size(%hidden_states.819, %38), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4131 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.819, %45, %45, %39, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4132 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4131, %47, %45, %39, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4133 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4132, %40), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4134 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4133, %38, %45, %39, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4135 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4134, %37, %45, %39, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4136 : int[] = prim::ListConstruct(%4126, %4127, %37, %4129, %4130), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %hidden_states.821 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4135, %4136, %42), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4138 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.109, %28), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4139 : int = aten::Int(%4138), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4140 : int[] = prim::ListConstruct(%4126, %4139, %4129, %4130), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %key.55 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.821, %4140), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4142 : int = aten::size(%hidden_states.823, %45), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4143 : int = aten::size(%hidden_states.823, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.111 : Long(device=cpu) = prim::NumToTensor(%4143), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4145 : int = aten::size(%hidden_states.823, %40), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4146 : int = aten::size(%hidden_states.823, %38), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4147 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.823, %45, %45, %39, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4148 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4147, %47, %45, %39, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4149 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4148, %40), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4150 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4149, %38, %45, %39, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4151 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4150, %37, %45, %39, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4152 : int[] = prim::ListConstruct(%4142, %4143, %37, %4145, %4146), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %hidden_states.825 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4151, %4152, %42), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4154 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.111, %28), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4155 : int = aten::Int(%4154), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4156 : int[] = prim::ListConstruct(%4142, %4155, %4145, %4146), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %value.55 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.825, %4156), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4158 : int = aten::size(%key.55, %40), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4159 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4160 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4159, %47, %45, %39, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4161 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4160, %40, %45, %39, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.61 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4161, %38, %45, %4158, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.109 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.55, %key.55, %value.55, %attention_mask.61, %27, %42, %26, %42), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %4164 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.109, %47, %40), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.111 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4164, %45), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %4166 : int[] = prim::ListConstruct(%4075, %4076, %35), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn
  %4167 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.111, %4166), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %4168 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%4167, %45), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.497 : Tensor = prim::GetAttr[name="weight"](%o_proj.55)
  %hidden_states.827 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4168, %weight.497), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn/__module.model.layers.27.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.829 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4070, %hidden_states.827, %47), scope: __module.model/__module.model.layers.27 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.499 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.55)
  %hidden_states.831 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.829, %33, %42, %42, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4174 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.831, %40), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4175 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm
  %variance.111 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4174, %4175, %25, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4177 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.111, %24, %47), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4178 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4177), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.833 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.831, %4178), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.835 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.833, %33, %42, %42, %44), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4181 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.499, %hidden_states.835), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4182 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%4181, %hidden_states.831)
  %4183 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4184 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4182)
  %down_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_352.Linear = prim::GetAttr[name="down_proj"](%mlp.55)
  %up_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_351.Linear = prim::GetAttr[name="up_proj"](%mlp.55)
  %gate_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_350.Linear = prim::GetAttr[name="gate_proj"](%mlp.55)
  %weight.501 : Tensor = prim::GetAttr[name="weight"](%gate_proj.55)
  %input.55 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4183, %weight.501), scope: __module.model/__module.model.layers.27/__module.model.layers.27.mlp/__module.model.layers.27.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4190 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.55), scope: __module.model/__module.model.layers.27/__module.model.layers.27.mlp/__module.model.layers.27.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.503 : Tensor = prim::GetAttr[name="weight"](%up_proj.55)
  %4192 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4183, %weight.503), scope: __module.model/__module.model.layers.27/__module.model.layers.27.mlp/__module.model.layers.27.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4193 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%4190, %4192), scope: __module.model/__module.model.layers.27/__module.model.layers.27.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.505 : Tensor = prim::GetAttr[name="weight"](%down_proj.55)
  %hidden_states.837 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4193, %weight.505), scope: __module.model/__module.model.layers.27/__module.model.layers.27.mlp/__module.model.layers.27.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.839 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4184, %hidden_states.837, %47), scope: __module.model/__module.model.layers.27 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.57 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_367.MistralMLP = prim::GetAttr[name="mlp"](%_28)
  %post_attention_layernorm.57 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_369.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_28)
  %self_attn.57 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_362.MistralAttention = prim::GetAttr[name="self_attn"](%_28)
  %input_layernorm.57 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_368.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_28)
  %weight.507 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.57)
  %hidden_states.841 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.839, %33, %42, %42, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4203 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.841, %40), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4204 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm
  %variance.113 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4203, %4204, %25, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4206 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.113, %24, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4207 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4206), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.843 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.841, %4207), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.845 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.843, %33, %42, %42, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.847 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.507, %hidden_states.845), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4211 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.847, %hidden_states.841)
  %4212 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4213 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4211)
  %o_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_361.Linear = prim::GetAttr[name="o_proj"](%self_attn.57)
  %v_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_360.Linear = prim::GetAttr[name="v_proj"](%self_attn.57)
  %k_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_359.Linear = prim::GetAttr[name="k_proj"](%self_attn.57)
  %q_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_358.Linear = prim::GetAttr[name="q_proj"](%self_attn.57)
  %4218 : int = aten::size(%4212, %45), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %4219 : int = aten::size(%4212, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.509 : Tensor = prim::GetAttr[name="weight"](%q_proj.57)
  %4221 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4212, %weight.509), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn/__module.model.layers.28.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4222 : int[] = prim::ListConstruct(%4218, %4219, %35, %30), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4223 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%4221, %4222), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.57 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4223, %47, %40), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.511 : Tensor = prim::GetAttr[name="weight"](%k_proj.57)
  %4226 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4212, %weight.511), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn/__module.model.layers.28.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4227 : int[] = prim::ListConstruct(%4218, %4219, %35, %30), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4228 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4226, %4227), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.57 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4228, %47, %40), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.513 : Tensor = prim::GetAttr[name="weight"](%v_proj.57)
  %4231 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4212, %weight.513), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn/__module.model.layers.28.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4232 : int[] = prim::ListConstruct(%4218, %4219, %35, %30), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4233 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4231, %4232), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.853 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4233, %47, %40), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.61 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.61 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %4237 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.57, %cos.61), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4238 : int = aten::size(%q.57, %38), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4239 : Long(device=cpu) = prim::NumToTensor(%4238), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4240 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4239, %29), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4241 : int = aten::Int(%4240), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %x1.113 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.57, %38, %45, %4241, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4243 : int = aten::size(%q.57, %38), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4244 : Long(device=cpu) = prim::NumToTensor(%4243), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4245 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4244, %29), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4246 : int = aten::Int(%4245), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %x2.113 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.57, %38, %4246, %39, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4248 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.113), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4249 : Tensor[] = prim::ListConstruct(%4248, %x1.113), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4250 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4249, %35), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4251 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4250, %sin.61), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.57 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4237, %4251, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4253 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.57, %cos.61), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4254 : int = aten::size(%k.57, %38), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4255 : Long(device=cpu) = prim::NumToTensor(%4254), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4256 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4255, %29), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4257 : int = aten::Int(%4256), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %x1.115 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.57, %38, %45, %4257, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4259 : int = aten::size(%k.57, %38), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4260 : Long(device=cpu) = prim::NumToTensor(%4259), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4261 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4260, %29), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4262 : int = aten::Int(%4261), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %x2.115 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.57, %38, %4262, %39, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4264 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.115), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4265 : Tensor[] = prim::ListConstruct(%4264, %x1.115), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4266 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4265, %35), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4267 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4266, %sin.61), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.849 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4253, %4267, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4269 : int = aten::size(%hidden_states.849, %45), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4270 : int = aten::size(%hidden_states.849, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.113 : Long(device=cpu) = prim::NumToTensor(%4270), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4272 : int = aten::size(%hidden_states.849, %40), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4273 : int = aten::size(%hidden_states.849, %38), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4274 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.849, %45, %45, %39, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4275 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4274, %47, %45, %39, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4276 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4275, %40), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4277 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4276, %38, %45, %39, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4278 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4277, %37, %45, %39, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4279 : int[] = prim::ListConstruct(%4269, %4270, %37, %4272, %4273), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %hidden_states.851 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4278, %4279, %42), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4281 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.113, %28), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4282 : int = aten::Int(%4281), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4283 : int[] = prim::ListConstruct(%4269, %4282, %4272, %4273), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %key.57 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.851, %4283), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4285 : int = aten::size(%hidden_states.853, %45), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4286 : int = aten::size(%hidden_states.853, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.115 : Long(device=cpu) = prim::NumToTensor(%4286), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4288 : int = aten::size(%hidden_states.853, %40), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4289 : int = aten::size(%hidden_states.853, %38), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4290 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.853, %45, %45, %39, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4291 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4290, %47, %45, %39, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4292 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4291, %40), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4293 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4292, %38, %45, %39, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4294 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4293, %37, %45, %39, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4295 : int[] = prim::ListConstruct(%4285, %4286, %37, %4288, %4289), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %hidden_states.855 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4294, %4295, %42), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4297 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.115, %28), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4298 : int = aten::Int(%4297), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4299 : int[] = prim::ListConstruct(%4285, %4298, %4288, %4289), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %value.57 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.855, %4299), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4301 : int = aten::size(%key.57, %40), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4302 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4303 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4302, %47, %45, %39, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4304 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4303, %40, %45, %39, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.63 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4304, %38, %45, %4301, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.113 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.57, %key.57, %value.57, %attention_mask.63, %27, %42, %26, %42), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %4307 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.113, %47, %40), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.115 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4307, %45), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %4309 : int[] = prim::ListConstruct(%4218, %4219, %35), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn
  %4310 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.115, %4309), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %4311 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%4310, %45), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.515 : Tensor = prim::GetAttr[name="weight"](%o_proj.57)
  %hidden_states.857 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4311, %weight.515), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn/__module.model.layers.28.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.859 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4213, %hidden_states.857, %47), scope: __module.model/__module.model.layers.28 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.517 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.57)
  %hidden_states.861 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.859, %33, %42, %42, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4317 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.861, %40), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4318 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm
  %variance.115 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4317, %4318, %25, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4320 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.115, %24, %47), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4321 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4320), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.863 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.861, %4321), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.865 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.863, %33, %42, %42, %44), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4324 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.517, %hidden_states.865), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4325 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%4324, %hidden_states.861)
  %4326 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4327 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4325)
  %down_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_365.Linear = prim::GetAttr[name="down_proj"](%mlp.57)
  %up_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_364.Linear = prim::GetAttr[name="up_proj"](%mlp.57)
  %gate_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_363.Linear = prim::GetAttr[name="gate_proj"](%mlp.57)
  %weight.519 : Tensor = prim::GetAttr[name="weight"](%gate_proj.57)
  %input.57 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4326, %weight.519), scope: __module.model/__module.model.layers.28/__module.model.layers.28.mlp/__module.model.layers.28.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4333 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.57), scope: __module.model/__module.model.layers.28/__module.model.layers.28.mlp/__module.model.layers.28.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.521 : Tensor = prim::GetAttr[name="weight"](%up_proj.57)
  %4335 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4326, %weight.521), scope: __module.model/__module.model.layers.28/__module.model.layers.28.mlp/__module.model.layers.28.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4336 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%4333, %4335), scope: __module.model/__module.model.layers.28/__module.model.layers.28.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.523 : Tensor = prim::GetAttr[name="weight"](%down_proj.57)
  %hidden_states.867 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4336, %weight.523), scope: __module.model/__module.model.layers.28/__module.model.layers.28.mlp/__module.model.layers.28.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.869 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4327, %hidden_states.867, %47), scope: __module.model/__module.model.layers.28 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.59 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_380.MistralMLP = prim::GetAttr[name="mlp"](%_29)
  %post_attention_layernorm.59 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_382.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_29)
  %self_attn.59 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_375.MistralAttention = prim::GetAttr[name="self_attn"](%_29)
  %input_layernorm.59 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_381.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_29)
  %weight.525 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.59)
  %hidden_states.871 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.869, %33, %42, %42, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4346 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.871, %40), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4347 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm
  %variance.117 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4346, %4347, %25, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4349 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.117, %24, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4350 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4349), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.873 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.871, %4350), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.875 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.873, %33, %42, %42, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.877 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.525, %hidden_states.875), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4354 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.877, %hidden_states.871)
  %4355 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4356 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4354)
  %o_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_374.Linear = prim::GetAttr[name="o_proj"](%self_attn.59)
  %v_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_373.Linear = prim::GetAttr[name="v_proj"](%self_attn.59)
  %k_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_372.Linear = prim::GetAttr[name="k_proj"](%self_attn.59)
  %q_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_371.Linear = prim::GetAttr[name="q_proj"](%self_attn.59)
  %4361 : int = aten::size(%4355, %45), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %4362 : int = aten::size(%4355, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.527 : Tensor = prim::GetAttr[name="weight"](%q_proj.59)
  %4364 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4355, %weight.527), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn/__module.model.layers.29.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4365 : int[] = prim::ListConstruct(%4361, %4362, %35, %30), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4366 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%4364, %4365), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.59 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4366, %47, %40), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.529 : Tensor = prim::GetAttr[name="weight"](%k_proj.59)
  %4369 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4355, %weight.529), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn/__module.model.layers.29.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4370 : int[] = prim::ListConstruct(%4361, %4362, %35, %30), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4371 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4369, %4370), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.59 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4371, %47, %40), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.531 : Tensor = prim::GetAttr[name="weight"](%v_proj.59)
  %4374 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4355, %weight.531), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn/__module.model.layers.29.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4375 : int[] = prim::ListConstruct(%4361, %4362, %35, %30), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4376 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4374, %4375), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.883 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4376, %47, %40), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.63 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.63 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %4380 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.59, %cos.63), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4381 : int = aten::size(%q.59, %38), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4382 : Long(device=cpu) = prim::NumToTensor(%4381), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4383 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4382, %29), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4384 : int = aten::Int(%4383), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %x1.117 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.59, %38, %45, %4384, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4386 : int = aten::size(%q.59, %38), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4387 : Long(device=cpu) = prim::NumToTensor(%4386), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4388 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4387, %29), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4389 : int = aten::Int(%4388), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %x2.117 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.59, %38, %4389, %39, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4391 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.117), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4392 : Tensor[] = prim::ListConstruct(%4391, %x1.117), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4393 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4392, %35), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4394 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4393, %sin.63), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.59 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4380, %4394, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4396 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.59, %cos.63), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4397 : int = aten::size(%k.59, %38), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4398 : Long(device=cpu) = prim::NumToTensor(%4397), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4399 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4398, %29), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4400 : int = aten::Int(%4399), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %x1.119 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.59, %38, %45, %4400, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4402 : int = aten::size(%k.59, %38), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4403 : Long(device=cpu) = prim::NumToTensor(%4402), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4404 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4403, %29), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4405 : int = aten::Int(%4404), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %x2.119 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.59, %38, %4405, %39, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4407 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.119), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4408 : Tensor[] = prim::ListConstruct(%4407, %x1.119), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4409 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4408, %35), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4410 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4409, %sin.63), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.879 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4396, %4410, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4412 : int = aten::size(%hidden_states.879, %45), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4413 : int = aten::size(%hidden_states.879, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.117 : Long(device=cpu) = prim::NumToTensor(%4413), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4415 : int = aten::size(%hidden_states.879, %40), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4416 : int = aten::size(%hidden_states.879, %38), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4417 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.879, %45, %45, %39, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4418 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4417, %47, %45, %39, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4419 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4418, %40), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4420 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4419, %38, %45, %39, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4421 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4420, %37, %45, %39, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4422 : int[] = prim::ListConstruct(%4412, %4413, %37, %4415, %4416), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %hidden_states.881 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4421, %4422, %42), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4424 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.117, %28), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4425 : int = aten::Int(%4424), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4426 : int[] = prim::ListConstruct(%4412, %4425, %4415, %4416), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %key.59 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.881, %4426), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4428 : int = aten::size(%hidden_states.883, %45), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4429 : int = aten::size(%hidden_states.883, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.119 : Long(device=cpu) = prim::NumToTensor(%4429), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4431 : int = aten::size(%hidden_states.883, %40), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4432 : int = aten::size(%hidden_states.883, %38), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4433 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.883, %45, %45, %39, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4434 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4433, %47, %45, %39, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4435 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4434, %40), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4436 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4435, %38, %45, %39, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4437 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4436, %37, %45, %39, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4438 : int[] = prim::ListConstruct(%4428, %4429, %37, %4431, %4432), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %hidden_states.885 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4437, %4438, %42), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4440 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.119, %28), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4441 : int = aten::Int(%4440), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4442 : int[] = prim::ListConstruct(%4428, %4441, %4431, %4432), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %value.59 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.885, %4442), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4444 : int = aten::size(%key.59, %40), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4445 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4446 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4445, %47, %45, %39, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4447 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4446, %40, %45, %39, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.65 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4447, %38, %45, %4444, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.117 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.59, %key.59, %value.59, %attention_mask.65, %27, %42, %26, %42), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %4450 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.117, %47, %40), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.119 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4450, %45), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %4452 : int[] = prim::ListConstruct(%4361, %4362, %35), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn
  %4453 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.119, %4452), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %4454 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%4453, %45), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.533 : Tensor = prim::GetAttr[name="weight"](%o_proj.59)
  %hidden_states.887 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4454, %weight.533), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn/__module.model.layers.29.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.889 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4356, %hidden_states.887, %47), scope: __module.model/__module.model.layers.29 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.535 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.59)
  %hidden_states.891 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.889, %33, %42, %42, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4460 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.891, %40), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4461 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm
  %variance.119 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4460, %4461, %25, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4463 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.119, %24, %47), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4464 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4463), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.893 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.891, %4464), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.895 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.893, %33, %42, %42, %44), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4467 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.535, %hidden_states.895), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4468 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%4467, %hidden_states.891)
  %4469 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4470 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4468)
  %down_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_378.Linear = prim::GetAttr[name="down_proj"](%mlp.59)
  %up_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_377.Linear = prim::GetAttr[name="up_proj"](%mlp.59)
  %gate_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_376.Linear = prim::GetAttr[name="gate_proj"](%mlp.59)
  %weight.537 : Tensor = prim::GetAttr[name="weight"](%gate_proj.59)
  %input.59 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4469, %weight.537), scope: __module.model/__module.model.layers.29/__module.model.layers.29.mlp/__module.model.layers.29.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4476 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.59), scope: __module.model/__module.model.layers.29/__module.model.layers.29.mlp/__module.model.layers.29.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.539 : Tensor = prim::GetAttr[name="weight"](%up_proj.59)
  %4478 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4469, %weight.539), scope: __module.model/__module.model.layers.29/__module.model.layers.29.mlp/__module.model.layers.29.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4479 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%4476, %4478), scope: __module.model/__module.model.layers.29/__module.model.layers.29.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.541 : Tensor = prim::GetAttr[name="weight"](%down_proj.59)
  %hidden_states.897 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4479, %weight.541), scope: __module.model/__module.model.layers.29/__module.model.layers.29.mlp/__module.model.layers.29.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.899 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4470, %hidden_states.897, %47), scope: __module.model/__module.model.layers.29 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp.61 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_393.MistralMLP = prim::GetAttr[name="mlp"](%_30)
  %post_attention_layernorm.61 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_395.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_30)
  %self_attn.61 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_388.MistralAttention = prim::GetAttr[name="self_attn"](%_30)
  %input_layernorm.61 : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_394.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_30)
  %weight.543 : Tensor = prim::GetAttr[name="weight"](%input_layernorm.61)
  %hidden_states.901 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.899, %33, %42, %42, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4489 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.901, %40), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4490 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm
  %variance.121 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4489, %4490, %25, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4492 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.121, %24, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4493 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4492), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.903 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.901, %4493), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.905 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.903, %33, %42, %42, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.907 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.543, %hidden_states.905), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4497 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.907, %hidden_states.901)
  %4498 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4499 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4497)
  %o_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_387.Linear = prim::GetAttr[name="o_proj"](%self_attn.61)
  %v_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_386.Linear = prim::GetAttr[name="v_proj"](%self_attn.61)
  %k_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_385.Linear = prim::GetAttr[name="k_proj"](%self_attn.61)
  %q_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_384.Linear = prim::GetAttr[name="q_proj"](%self_attn.61)
  %4504 : int = aten::size(%4498, %45), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %4505 : int = aten::size(%4498, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.545 : Tensor = prim::GetAttr[name="weight"](%q_proj.61)
  %4507 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4498, %weight.545), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn/__module.model.layers.30.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4508 : int[] = prim::ListConstruct(%4504, %4505, %35, %30), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4509 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%4507, %4508), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q.61 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4509, %47, %40), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.547 : Tensor = prim::GetAttr[name="weight"](%k_proj.61)
  %4512 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4498, %weight.547), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn/__module.model.layers.30.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4513 : int[] = prim::ListConstruct(%4504, %4505, %35, %30), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4514 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4512, %4513), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k.61 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4514, %47, %40), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.549 : Tensor = prim::GetAttr[name="weight"](%v_proj.61)
  %4517 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4498, %weight.549), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn/__module.model.layers.30.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4518 : int[] = prim::ListConstruct(%4504, %4505, %35, %30), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4519 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4517, %4518), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.913 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4519, %47, %40), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos.65 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin.65 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %4523 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.61, %cos.65), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4524 : int = aten::size(%q.61, %38), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4525 : Long(device=cpu) = prim::NumToTensor(%4524), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4526 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4525, %29), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4527 : int = aten::Int(%4526), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %x1.121 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.61, %38, %45, %4527, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4529 : int = aten::size(%q.61, %38), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4530 : Long(device=cpu) = prim::NumToTensor(%4529), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4531 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4530, %29), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4532 : int = aten::Int(%4531), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %x2.121 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.61, %38, %4532, %39, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4534 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.121), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4535 : Tensor[] = prim::ListConstruct(%4534, %x1.121), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4536 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4535, %35), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4537 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4536, %sin.65), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query.61 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4523, %4537, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4539 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.61, %cos.65), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4540 : int = aten::size(%k.61, %38), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4541 : Long(device=cpu) = prim::NumToTensor(%4540), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4542 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4541, %29), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4543 : int = aten::Int(%4542), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %x1.123 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.61, %38, %45, %4543, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4545 : int = aten::size(%k.61, %38), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4546 : Long(device=cpu) = prim::NumToTensor(%4545), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4547 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4546, %29), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4548 : int = aten::Int(%4547), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %x2.123 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.61, %38, %4548, %39, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4550 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.123), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4551 : Tensor[] = prim::ListConstruct(%4550, %x1.123), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4552 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4551, %35), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4553 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4552, %sin.65), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.909 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4539, %4553, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4555 : int = aten::size(%hidden_states.909, %45), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4556 : int = aten::size(%hidden_states.909, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.121 : Long(device=cpu) = prim::NumToTensor(%4556), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4558 : int = aten::size(%hidden_states.909, %40), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4559 : int = aten::size(%hidden_states.909, %38), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4560 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.909, %45, %45, %39, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4561 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4560, %47, %45, %39, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4562 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4561, %40), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4563 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4562, %38, %45, %39, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4564 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4563, %37, %45, %39, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4565 : int[] = prim::ListConstruct(%4555, %4556, %37, %4558, %4559), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %hidden_states.911 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4564, %4565, %42), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4567 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.121, %28), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4568 : int = aten::Int(%4567), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4569 : int[] = prim::ListConstruct(%4555, %4568, %4558, %4559), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %key.61 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.911, %4569), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4571 : int = aten::size(%hidden_states.913, %45), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4572 : int = aten::size(%hidden_states.913, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.123 : Long(device=cpu) = prim::NumToTensor(%4572), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4574 : int = aten::size(%hidden_states.913, %40), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4575 : int = aten::size(%hidden_states.913, %38), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4576 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.913, %45, %45, %39, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4577 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4576, %47, %45, %39, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4578 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4577, %40), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4579 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4578, %38, %45, %39, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4580 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4579, %37, %45, %39, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4581 : int[] = prim::ListConstruct(%4571, %4572, %37, %4574, %4575), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %hidden_states.915 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4580, %4581, %42), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4583 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.123, %28), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4584 : int = aten::Int(%4583), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4585 : int[] = prim::ListConstruct(%4571, %4584, %4574, %4575), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %value.61 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.915, %4585), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4587 : int = aten::size(%key.61, %40), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4588 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4589 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4588, %47, %45, %39, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4590 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4589, %40, %45, %39, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask.67 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4590, %38, %45, %4587, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.121 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query.61, %key.61, %value.61, %attention_mask.67, %27, %42, %26, %42), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %4593 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.121, %47, %40), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output.123 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4593, %45), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %4595 : int[] = prim::ListConstruct(%4504, %4505, %35), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn
  %4596 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output.123, %4595), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %4597 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%4596, %45), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.551 : Tensor = prim::GetAttr[name="weight"](%o_proj.61)
  %hidden_states.917 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4597, %weight.551), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn/__module.model.layers.30.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.919 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4499, %hidden_states.917, %47), scope: __module.model/__module.model.layers.30 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.553 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm.61)
  %hidden_states.921 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.919, %33, %42, %42, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4603 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.921, %40), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4604 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm
  %variance.123 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4603, %4604, %25, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4606 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.123, %24, %47), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4607 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4606), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.923 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.921, %4607), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.925 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.923, %33, %42, %42, %44), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4610 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.553, %hidden_states.925), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4611 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%4610, %hidden_states.921)
  %4612 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4613 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4611)
  %down_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_391.Linear = prim::GetAttr[name="down_proj"](%mlp.61)
  %up_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_390.Linear = prim::GetAttr[name="up_proj"](%mlp.61)
  %gate_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_389.Linear = prim::GetAttr[name="gate_proj"](%mlp.61)
  %weight.555 : Tensor = prim::GetAttr[name="weight"](%gate_proj.61)
  %input.61 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4612, %weight.555), scope: __module.model/__module.model.layers.30/__module.model.layers.30.mlp/__module.model.layers.30.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4619 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.61), scope: __module.model/__module.model.layers.30/__module.model.layers.30.mlp/__module.model.layers.30.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.557 : Tensor = prim::GetAttr[name="weight"](%up_proj.61)
  %4621 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4612, %weight.557), scope: __module.model/__module.model.layers.30/__module.model.layers.30.mlp/__module.model.layers.30.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4622 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%4619, %4621), scope: __module.model/__module.model.layers.30/__module.model.layers.30.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.559 : Tensor = prim::GetAttr[name="weight"](%down_proj.61)
  %hidden_states.927 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4622, %weight.559), scope: __module.model/__module.model.layers.30/__module.model.layers.30.mlp/__module.model.layers.30.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.929 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4613, %hidden_states.927, %47), scope: __module.model/__module.model.layers.30 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %mlp : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_406.MistralMLP = prim::GetAttr[name="mlp"](%_31)
  %post_attention_layernorm : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_408.MistralRMSNorm = prim::GetAttr[name="post_attention_layernorm"](%_31)
  %self_attn : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_401.MistralAttention = prim::GetAttr[name="self_attn"](%_31)
  %input_layernorm : __torch__.transformers.models.mistral.modeling_mistral.___torch_mangle_407.MistralRMSNorm = prim::GetAttr[name="input_layernorm"](%_31)
  %weight.561 : Tensor = prim::GetAttr[name="weight"](%input_layernorm)
  %hidden_states.931 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.929, %33, %42, %42, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4632 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.931, %40), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4633 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm
  %variance.125 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4632, %4633, %25, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4635 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.125, %24, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4636 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4635), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.933 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.931, %4636), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.935 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.933, %33, %42, %42, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states.937 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.561, %hidden_states.935), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4640 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.937, %hidden_states.931)
  %4641 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4642 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4640)
  %o_proj : __torch__.torch.nn.modules.linear.___torch_mangle_400.Linear = prim::GetAttr[name="o_proj"](%self_attn)
  %v_proj : __torch__.torch.nn.modules.linear.___torch_mangle_399.Linear = prim::GetAttr[name="v_proj"](%self_attn)
  %k_proj : __torch__.torch.nn.modules.linear.___torch_mangle_398.Linear = prim::GetAttr[name="k_proj"](%self_attn)
  %q_proj : __torch__.torch.nn.modules.linear.___torch_mangle_397.Linear = prim::GetAttr[name="q_proj"](%self_attn)
  %4647 : int = aten::size(%4641, %45), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %4648 : int = aten::size(%4641, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:150:0
  %weight.563 : Tensor = prim::GetAttr[name="weight"](%q_proj)
  %4650 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4641, %weight.563), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn/__module.model.layers.31.self_attn.q_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4651 : int[] = prim::ListConstruct(%4647, %4648, %35, %30), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4652 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%4650, %4651), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %q : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4652, %47, %40), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153:0
  %weight.565 : Tensor = prim::GetAttr[name="weight"](%k_proj)
  %4655 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4641, %weight.565), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn/__module.model.layers.31.self_attn.k_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4656 : int[] = prim::ListConstruct(%4647, %4648, %35, %30), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4657 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4655, %4656), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %k : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4657, %47, %40), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:154:0
  %weight.567 : Tensor = prim::GetAttr[name="weight"](%v_proj)
  %4660 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4641, %weight.567), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn/__module.model.layers.31.self_attn.v_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4661 : int[] = prim::ListConstruct(%4647, %4648, %35, %30), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4662 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%4660, %4661), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %hidden_states.943 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4662, %47, %40), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:155:0
  %cos : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%191, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78:0
  %sin : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%192, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79:0
  %4666 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q, %cos), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4667 : int = aten::size(%q, %38), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4668 : Long(device=cpu) = prim::NumToTensor(%4667), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4669 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4668, %29), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4670 : int = aten::Int(%4669), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %x1.125 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q, %38, %45, %4670, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4672 : int = aten::size(%q, %38), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4673 : Long(device=cpu) = prim::NumToTensor(%4672), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4674 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4673, %29), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4675 : int = aten::Int(%4674), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %x2.125 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q, %38, %4675, %39, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4677 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.125), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4678 : Tensor[] = prim::ListConstruct(%4677, %x1.125), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4679 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4678, %35), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4680 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4679, %sin), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %query : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4666, %4680, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80:0
  %4682 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k, %cos), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4683 : int = aten::size(%k, %38), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4684 : Long(device=cpu) = prim::NumToTensor(%4683), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4685 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4684, %29), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4686 : int = aten::Int(%4685), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %x1 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k, %38, %45, %4686, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53:0
  %4688 : int = aten::size(%k, %38), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4689 : Long(device=cpu) = prim::NumToTensor(%4688), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4690 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4689, %29), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/_tensor.py:1119:0
  %4691 : int = aten::Int(%4690), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %x2 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k, %38, %4691, %39, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54:0
  %4693 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4694 : Tensor[] = prim::ListConstruct(%4693, %x1), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4695 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4694, %35), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:55:0
  %4696 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4695, %sin), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %hidden_states.939 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4682, %4696, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:81:0
  %4698 : int = aten::size(%hidden_states.939, %45), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4699 : int = aten::size(%hidden_states.939, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads.125 : Long(device=cpu) = prim::NumToTensor(%4699), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4701 : int = aten::size(%hidden_states.939, %40), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4702 : int = aten::size(%hidden_states.939, %38), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4703 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.939, %45, %45, %39, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4704 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4703, %47, %45, %39, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4705 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4704, %40), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4706 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4705, %38, %45, %39, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4707 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4706, %37, %45, %39, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4708 : int[] = prim::ListConstruct(%4698, %4699, %37, %4701, %4702), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %hidden_states.941 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4707, %4708, %42), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4710 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.125, %28), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4711 : int = aten::Int(%4710), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4712 : int[] = prim::ListConstruct(%4698, %4711, %4701, %4702), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %key : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.941, %4712), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4714 : int = aten::size(%hidden_states.943, %45), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4715 : int = aten::size(%hidden_states.943, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %num_key_value_heads : Long(device=cpu) = prim::NumToTensor(%4715), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4717 : int = aten::size(%hidden_states.943, %40), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4718 : int = aten::size(%hidden_states.943, %38), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:23:0
  %4719 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.943, %45, %45, %39, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4720 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4719, %47, %45, %39, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4721 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4720, %40), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4722 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4721, %38, %45, %39, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4723 : Float(2, 8, 1, 16, 128, strides=[16384, 128, 16384, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%4722, %37, %45, %39, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4724 : int[] = prim::ListConstruct(%4714, %4715, %37, %4717, %4718), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %hidden_states.945 : Float(2, 8, 4, 16, 128, strides=[16384, 128, 0, 1024, 1], requires_grad=0, device=cpu) = aten::expand(%4723, %4724, %42), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26:0
  %4726 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads, %28), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4727 : int = aten::Int(%4726), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4728 : int[] = prim::ListConstruct(%4714, %4727, %4717, %4718), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %value : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.945, %4728), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:27:0
  %4730 : int = aten::size(%key, %40), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4731 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask.5, %45, %45, %39, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4732 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4731, %47, %45, %39, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %4733 : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4732, %40, %45, %39, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attention_mask : Float(2, 1, 16, 16, strides=[256, 256, 16, 1], requires_grad=0, device=cpu) = aten::slice(%4733, %38, %45, %4730, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73:0
  %attn_output.125 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%query, %key, %value, %attention_mask, %27, %42, %26, %42), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96:0
  %4736 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.125, %47, %40), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %attn_output : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4736, %45), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:106:0
  %4738 : int[] = prim::ListConstruct(%4647, %4648, %35), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn
  %4739 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::reshape(%attn_output, %4738), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %4740 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::contiguous(%4739, %45), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:181:0
  %weight.569 : Tensor = prim::GetAttr[name="weight"](%o_proj)
  %hidden_states.947 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4740, %weight.569), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn/__module.model.layers.31.self_attn.o_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.949 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4642, %hidden_states.947, %47), scope: __module.model/__module.model.layers.31 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:241:0
  %weight.571 : Tensor = prim::GetAttr[name="weight"](%post_attention_layernorm)
  %hidden_states.951 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.949, %33, %42, %42, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4746 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.951, %40), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4747 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm
  %variance.127 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4746, %4747, %25, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4749 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.127, %24, %47), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4750 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4749), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.953 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.951, %4750), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.955 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.953, %33, %42, %42, %44), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4753 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.571, %hidden_states.955), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %4754 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%4753, %hidden_states.951)
  %4755 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4756 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4754)
  %down_proj : __torch__.torch.nn.modules.linear.___torch_mangle_404.Linear = prim::GetAttr[name="down_proj"](%mlp)
  %up_proj : __torch__.torch.nn.modules.linear.___torch_mangle_403.Linear = prim::GetAttr[name="up_proj"](%mlp)
  %gate_proj : __torch__.torch.nn.modules.linear.___torch_mangle_402.Linear = prim::GetAttr[name="gate_proj"](%mlp)
  %weight.573 : Tensor = prim::GetAttr[name="weight"](%gate_proj)
  %input : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4755, %weight.573), scope: __module.model/__module.model.layers.31/__module.model.layers.31.mlp/__module.model.layers.31.mlp.gate_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4762 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input), scope: __module.model/__module.model.layers.31/__module.model.layers.31.mlp/__module.model.layers.31.mlp.act_fn # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/nn/functional.py:2371:0
  %weight.575 : Tensor = prim::GetAttr[name="weight"](%up_proj)
  %4764 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4755, %weight.575), scope: __module.model/__module.model.layers.31/__module.model.layers.31.mlp/__module.model.layers.31.mlp.up_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %4765 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%4762, %4764), scope: __module.model/__module.model.layers.31/__module.model.layers.31.mlp # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:47:0
  %weight.577 : Tensor = prim::GetAttr[name="weight"](%down_proj)
  %hidden_states.957 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%4765, %weight.577), scope: __module.model/__module.model.layers.31/__module.model.layers.31.mlp/__module.model.layers.31.mlp.down_proj # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %hidden_states.959 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4756, %hidden_states.957, %47), scope: __module.model/__module.model.layers.31 # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:247:0
  %weight.579 : Tensor = prim::GetAttr[name="weight"](%norm)
  %hidden_states.961 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.959, %33, %42, %42, %44), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198:0
  %4771 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.961, %40), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4772 : int[] = prim::ListConstruct(%35), scope: __module.model/__module.model.norm
  %variance : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4771, %4772, %25, %44), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199:0
  %4774 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance, %24, %47), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %4775 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4774), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.963 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.961, %4775), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:200:0
  %hidden_states.965 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.963, %33, %42, %42, %44), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %hidden_states : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.579, %hidden_states.965), scope: __module.model/__module.model.norm # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:201:0
  %7 : int = prim::Constant[value=0]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %8 : int = prim::Constant[value=0]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %9 : int = prim::Constant[value=9223372036854775807]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %10 : int = prim::Constant[value=1]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %11 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states, %7, %8, %9, %10) # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %12 : int = prim::Constant[value=1]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %13 : int = prim::Constant[value=0]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %14 : int = prim::Constant[value=9223372036854775807]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %15 : int = prim::Constant[value=1]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %16 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%11, %12, %13, %14, %15) # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %17 : int = prim::Constant[value=2]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %18 : int = prim::Constant[value=0]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %19 : int = prim::Constant[value=9223372036854775807]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %20 : int = prim::Constant[value=1]() # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %21 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%16, %17, %18, %19, %20) # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:447:0
  %weight : Tensor = prim::GetAttr[name="weight"](%lm_head)
  %4780 : Float(2, 16, 32000, strides=[512000, 32000, 1], requires_grad=0, device=cpu) = ^Trampoline[inplace=0, module="openvino.frontend.pytorch.patch_model", Subgraph=<Graph>](None)(%21, %weight), scope: __module.lm_head # /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/torch/autograd/function.py:581:0
  %23 : (Float(2, 16, 32000, strides=[512000, 32000, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%4780)
  return (%23)

2026-01-14 19:28:24,503 - DEBUG - matplotlib data path: /home/phil2/Super-Intel-NPU/current/Super-Intel-NPU/venv_offering/lib/python3.12/site-packages/matplotlib/mpl-data
2026-01-14 19:28:24,516 - DEBUG - CONFIGDIR=/home/phil2/.config/matplotlib
2026-01-14 19:28:24,518 - DEBUG - interactive is False
2026-01-14 19:28:24,519 - DEBUG - platform is linux
2026-01-14 19:28:24,624 - DEBUG - CACHEDIR=/home/phil2/.cache/matplotlib
2026-01-14 19:28:24,633 - DEBUG - Using fontManager instance from /home/phil2/.cache/matplotlib/fontlist-v390.json
2026-01-14 19:28:25,173 - DEBUG - {'conversion_parameters': {'framework': 'pytorch', 'is_python_object': True}}
2026-01-14 19:28:25,173 - DEBUG - Model Conversion API started
2026-01-14 19:28:25,173 - DEBUG - Placeholder shapes : [<PartialShape: [?,?]>, <PartialShape: [?,?]>, <PartialShape: [?,?]>]
2026-01-14 19:28:51,872 - INFO - >>> [Bake] Saving Intermediate (Dynamic) IR to ./models/neuralchat_int4/intermediate_dynamic...
2026-01-14 19:28:53,536 - INFO - >>> [Bake] Stage 2: Loading Intermediate for Static Reshaping...
2026-01-14 19:28:53,601 - INFO - >>> [Bake] Remediation: Enforcing STRICT STATIC shapes [1, 128]...
2026-01-14 19:28:53,602 - INFO -     > Found Input: input_ids, Shape: [?,?]
2026-01-14 19:28:53,602 - INFO -       -> Locking 2D Input input_ids to [1, 128]
2026-01-14 19:28:53,602 - INFO -     > Found Input: attention_mask, Shape: [?,?]
2026-01-14 19:28:53,602 - INFO -       -> Locking 2D Input attention_mask to [1, 128]
2026-01-14 19:28:53,602 - INFO -     > Found Input: position_ids, Shape: [?,?]
2026-01-14 19:28:53,602 - INFO -       -> Locking 2D Input position_ids to [1, 128]
2026-01-14 19:28:53,602 - INFO - Applying reshape to graph...
2026-01-14 19:28:53,649 - INFO - >>> [Bake] Serializing Final Static IR to ./models/neuralchat_int4...
2026-01-14 19:28:54,954 - INFO - >>> [Bake] Removing intermediate files...
2026-01-14 19:28:54,955 - INFO - >>> [Bake] Final Verification...
2026-01-14 19:28:55,013 - INFO -     > [Verify Input] input_ids: [1,128]
2026-01-14 19:28:55,014 - INFO -     > [Verify Input] attention_mask: [1,128]
2026-01-14 19:28:55,014 - INFO -     > [Verify Input] position_ids: [1,128]
2026-01-14 19:28:55,014 - INFO - >>> [Bake] Success! Optimized NPU-ready model is at ./models/neuralchat_int4
