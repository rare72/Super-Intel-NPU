Task: Fix Phi-4 NPU Crash (NullNode Error) and Increase Context Window

Summary of Changes:
1. Modified `src/python/bake_phi4.py`:
   - Changed target variant from `gpu/*` to `cpu_and_mobile/*` (specifically targeting `cpu-int4-rtn-block-32-acc-level-4`).
   - The previous GPU variant contained DirectML-specific custom operators (likely related to GenAI optimizations) that caused `NullNode` errors when loading on the OpenVINO NPU driver.
   - The CPU variant utilizes standard ONNX operators which are fully supported by the NPU driver, resolving the crash.

2. Modified `src/python/supervisor.py`:
   - Increased `STATIC_SEQ_LEN` from `128` to `1024`.
   - This expands the static inference loop and memory allocation to support larger prompts and longer generation, as requested.

Technical Notes:
- **INT8 Conversion**: The user requested "convert to INT8". However, the mandated source repository (`microsoft/phi-4-onnx`) only provides INT4 quantized models. Switching to the unquantized PyTorch model (`microsoft/phi-4`) to perform custom INT8 quantization was explicitly forbidden ("Do not switch the source model"). Therefore, the solution retains the INT4 quantization from the source but fixes the runtime crash.
- **Context Window**: The NPU memory bandwidth and capacity (NPU 3720/4090) are sufficient to handle a 1024 token window for this model (approx 14B params quantized to INT4). The static allocation in `supervisor.py` will now pad inputs to [1, 1024].

Usage Instructions:
1. Run `python src/python/bake_phi4.py` to download and convert the CPU-optimized model.
   - Ensure the output directory is clean if problems persist.
2. Run `python src/python/supervisor.py --prompt "Your prompt here" --chat_style phi4` (or via `windows_launch.ps1`).

Validation:
- The `NullNode` error should be eliminated.
- Input prompts > 128 tokens will no longer be truncated immediately (up to 1024).
